{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Buffer, collect_one_episode, normalize_obs, copy_params, avg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResLinear(nn.Module):\n",
    "    def __init__(self, n_in, n_out, act=nn.ReLU()):\n",
    "        super(ResLinear, self).__init__()\n",
    "        self.act = act\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "        self.bn = nn.BatchNorm1d(n_out)\n",
    "        \n",
    "        assert(n_in == n_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.act(self.bn(self.linear(x)))\n",
    "        return h + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(nn.Module):\n",
    "    def __init__(self, n_in=128, n_hid=100, n_out=6):\n",
    "        super(Player, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid),\n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, n_out))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, obs, normalized=False):\n",
    "        if normalized:\n",
    "            return self.softmax(self.layers(obs))\n",
    "        else:\n",
    "            return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(nn.Module):\n",
    "    def __init__(self, n_in=128, n_hid=100):\n",
    "        super(Value, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid),\n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, 1))\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-ram-v0')\n",
    "# env = gym.make('Assault-ram-v0')\n",
    "\n",
    "n_frames = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a policy\n",
    "player = Player(n_in=128 * n_frames, n_hid=32, n_out=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a value estimator\n",
    "value = Value(n_in=128 * n_frames, n_hid=32).to(device)\n",
    "value_old = Value(n_in=128 * n_frames, n_hid=32).to(device)\n",
    "copy_params(value, value_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimizers\n",
    "opt_player = Adam(player.parameters(), lr=0.0001)\n",
    "opt_value = Adam(value.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize replay buffer\n",
    "replay_buffer = Buffer(max_items=50000, n_frames=n_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid run -20.0 -20.0\n",
      "# plays 1 return -16.0 value_loss 0.034986674785614014 entropy inf\n",
      "Valid run -15.0 -19.5\n",
      "# plays 2 return -16.4 value_loss 0.033986848592758176 entropy 1.7395795265060316\n",
      "Valid run -17.0 -19.25\n",
      "# plays 3 return -16.36 value_loss 0.03231568776071071 entropy 1.7140545098208968\n",
      "Valid run -19.0 -19.224999999999998\n",
      "# plays 4 return -16.224 value_loss 0.031508751928806304 entropy 1.6747530044528558\n",
      "Valid run -20.0 -19.3025\n",
      "# plays 5 return -16.401600000000002 value_loss 0.03031411570608616 entropy 1.6363754703657845\n",
      "Valid run -20.0 -19.372249999999998\n",
      "# plays 6 return -16.06144 value_loss 0.028339462077319622 entropy 1.6009768642112558\n",
      "Valid run -20.0 -19.435025\n",
      "# plays 7 return -16.055296000000002 value_loss 0.02660307598474622 entropy 1.5770189365367744\n",
      "Valid run -20.0 -19.4915225\n",
      "# plays 8 return -16.1497664 value_loss 0.024847178066787125 entropy 1.5299457573978035\n",
      "Valid run -18.0 -19.34237025\n",
      "# plays 9 return -16.43478976 value_loss 0.022969855385661427 entropy 1.477239374099086\n",
      "Valid run -14.0 -18.808133225\n",
      "# plays 10 return -16.391310784 value_loss 0.021155598691603814 entropy 1.4238067369474234\n",
      "Valid run -7.0 -17.6273199025\n",
      "# plays 11 return -16.2521797056 value_loss 0.019573454292715734 entropy 1.3736399324548256\n",
      "Valid run -18.0 -17.66458791225\n",
      "# plays 12 return -16.42696173504 value_loss 0.018061198445797993 entropy 1.3336884812094194\n",
      "Valid run -18.0 -17.698129121024998\n",
      "# plays 13 return -15.584265561536 value_loss 0.016739092112086146 entropy 1.3048257984406118\n",
      "Valid run -20.0 -17.928316208922496\n",
      "# plays 14 return -15.9258390053824 value_loss 0.015616938021855292 entropy 1.2591216602952506\n",
      "Valid run -20.0 -18.13548458803025\n",
      "# plays 15 return -16.23325510484416 value_loss 0.01470144765003926 entropy 1.222115840722982\n",
      "Valid run -20.0 -18.321936129227225\n",
      "# plays 16 return -16.409929594359745 value_loss 0.013744595501268563 entropy 1.1913171640261275\n",
      "Valid run -18.0 -18.289742516304504\n",
      "# plays 17 return -16.56893663492377 value_loss 0.01264765322143852 entropy 1.1649971938316404\n",
      "Valid run -16.0 -18.060768264674056\n",
      "# plays 18 return -16.212042971431394 value_loss 0.011495118590887808 entropy 1.1318319220572843\n",
      "Valid run -11.0 -17.354691438206654\n",
      "# plays 19 return -14.990838674288256 value_loss 0.010626949269872687 entropy 1.1119064163018595\n",
      "Valid run -16.0 -17.21922229438599\n",
      "# plays 20 return -15.09175480685943 value_loss 0.010061960251377627 entropy 1.097355555656272\n",
      "Valid run -19.0 -17.39730006494739\n",
      "# plays 21 return -15.082579326173487 value_loss 0.009391570555043166 entropy 1.0796708622017634\n",
      "Valid run -15.0 -17.157570058452652\n",
      "# plays 22 return -15.374321393556139 value_loss 0.008691709407788765 entropy 1.0781526426424666\n",
      "Valid run -10.0 -16.441813052607387\n",
      "# plays 23 return -14.936889254200524 value_loss 0.008237759370242584 entropy 1.0798715800622427\n",
      "Valid run -5.0 -15.297631747346648\n",
      "# plays 24 return -14.843200328780473 value_loss 0.007469653082955281 entropy 1.0692075020178196\n",
      "Valid run -15.0 -15.267868572611983\n",
      "# plays 25 return -14.658880295902426 value_loss 0.00698219575765139 entropy 1.070485607947868\n",
      "Valid run -11.0 -14.841081715350784\n",
      "# plays 26 return -13.492992266312184 value_loss 0.006413564339756126 entropy 1.0748582533035365\n",
      "Valid run -13.0 -14.656973543815706\n",
      "# plays 27 return -13.543693039680965 value_loss 0.00603329473664983 entropy 1.0477535097028332\n",
      "Valid run -8.0 -13.991276189434137\n",
      "# plays 28 return -12.889323735712868 value_loss 0.005779278982182955 entropy 0.9856273412981265\n",
      "Valid run -11.0 -13.692148570490723\n",
      "# plays 29 return -12.70039136214158 value_loss 0.0054218200502550145 entropy 0.9432630112579463\n",
      "Valid run -18.0 -14.122933713441652\n",
      "# plays 30 return -12.630352225927425 value_loss 0.005220925370365876 entropy 0.8786814882804682\n",
      "Valid run -15.0 -14.210640342097488\n",
      "# plays 31 return -12.267317003334682 value_loss 0.004919858032027333 entropy 0.8513600269156778\n",
      "Valid run -8.0 -13.58957630788774\n",
      "# plays 32 return -11.740585303001213 value_loss 0.004815151871570452 entropy 0.8280000514912882\n",
      "Valid run -9.0 -13.130618677098965\n",
      "# plays 33 return -11.466526772701092 value_loss 0.004629246665235016 entropy 0.7909401787971536\n",
      "Valid run -8.0 -12.61755680938907\n",
      "# plays 34 return -11.219874095430983 value_loss 0.004548601323988128 entropy 0.7600858574491547\n",
      "Valid run -13.0 -12.655801128450165\n",
      "# plays 35 return -11.597886685887884 value_loss 0.004310825848058839 entropy 0.7731610707858163\n",
      "Valid run -13.0 -12.690221015605148\n",
      "# plays 36 return -10.938098017299096 value_loss 0.004515636254229527 entropy 0.7687400768868808\n",
      "Valid run -6.0 -12.021198914044634\n",
      "# plays 37 return -10.244288215569187 value_loss 0.004586915399248942 entropy 0.7367794376178355\n",
      "Valid run -7.0 -11.51907902264017\n",
      "# plays 38 return -9.719859394012268 value_loss 0.004236355392812679 entropy 0.7123968589251828\n",
      "Valid run -4.0 -10.767171120376155\n",
      "# plays 39 return -8.84787345461104 value_loss 0.004216746631946256 entropy 0.6900962627687532\n",
      "Valid run -12.0 -10.89045400833854\n",
      "# plays 40 return -8.763086109149937 value_loss 0.004224610792415819 entropy 0.6774397668090375\n",
      "Valid run -6.0 -10.401408607504687\n",
      "# plays 41 return -8.186777498234944 value_loss 0.003972030820459856 entropy 0.6477410080656246\n",
      "Valid run -6.0 -9.961267746754217\n",
      "# plays 42 return -8.06809974841145 value_loss 0.0038665961300305128 entropy 0.6296291529349857\n",
      "Valid run -3.0 -9.265140972078797\n",
      "# plays 43 return -8.361289773570306 value_loss 0.0036071486614619136 entropy 0.6035493131027566\n",
      "Valid run -6.0 -8.938626874870916\n",
      "# plays 44 return -8.225160796213276 value_loss 0.0035663721802435314 entropy 0.5963811744610388\n",
      "Valid run -3.0 -8.344764187383825\n",
      "# plays 45 return -7.602644716591948 value_loss 0.0035763325150029654 entropy 0.5882593754361646\n",
      "Valid run -5.0 -8.010287768645444\n",
      "# plays 46 return -7.042380244932754 value_loss 0.003558379498404276 entropy 0.5738963978313155\n",
      "Valid run -7.0 -7.9092589917809\n",
      "# plays 47 return -7.038142220439479 value_loss 0.003445083302250813 entropy 0.5714479632581385\n",
      "Valid run -6.0 -7.71833309260281\n"
     ]
    }
   ],
   "source": [
    "n_iter = 1000\n",
    "init_collect = 1\n",
    "n_collect = 1\n",
    "n_value = 150\n",
    "n_policy = 150\n",
    "disp_iter = 1\n",
    "val_iter = 1\n",
    "\n",
    "max_len = 1000\n",
    "batch_size = 1000\n",
    "\n",
    "ent_coeff = 0. #0.001\n",
    "discount_factor = .95\n",
    "\n",
    "value_loss = -numpy.Inf\n",
    "ret = -numpy.Inf\n",
    "entropy = -numpy.Inf\n",
    "valid_ret = -numpy.Inf\n",
    "\n",
    "return_history = []\n",
    "\n",
    "for ni in range(n_iter):\n",
    "    player.eval()\n",
    "\n",
    "    if numpy.mod(ni, val_iter) == 0:\n",
    "        _, _, _, _, _, ret_ = collect_one_episode(env, player, max_len=max_len, deterministic=True, n_frames=n_frames)\n",
    "        return_history.append(ret_)\n",
    "        if valid_ret == -numpy.Inf:\n",
    "            valid_ret = ret_\n",
    "        else:\n",
    "            valid_ret = 0.9 * valid_ret + 0.1 * ret_\n",
    "        print('Valid run', ret_, valid_ret)\n",
    "\n",
    "    # collect some episodes using the current policy\n",
    "    # and push (obs,a,r,p(a)) tuples to the replay buffer.\n",
    "    nc = n_collect\n",
    "    if ni == 0:\n",
    "        nc = init_collect\n",
    "    for ci in range(nc):\n",
    "        o_, r_, c_, a_, ap_, ret_ = collect_one_episode(env, player, max_len=max_len, discount_factor=discount_factor, n_frames=n_frames)\n",
    "        replay_buffer.add(o_, r_, c_, a_, ap_)\n",
    "        if ret == -numpy.Inf:\n",
    "            ret = ret_\n",
    "        else:\n",
    "            ret = 0.9 * ret + 0.1 * ret_\n",
    "    \n",
    "    # fit a value function\n",
    "    # TD(1)\n",
    "    value.train()\n",
    "    for vi in range(n_value):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "\n",
    "        batch_x = torch.from_numpy(numpy.stack([ex['current']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_r = torch.from_numpy(numpy.stack([ex['current']['rew'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_xn = torch.from_numpy(numpy.stack([ex['next']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        pred_y = value(batch_x).squeeze()\n",
    "        pred_next = value_old(batch_xn).squeeze().clone().detach()\n",
    "        loss_ = ((batch_r + discount_factor * pred_next - pred_y) ** 2)\n",
    "        \n",
    "        batch_a = torch.from_numpy(numpy.stack([ex['current']['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        batch_pi = player(batch_x, normalized=True)\n",
    "        batch_q = torch.from_numpy(numpy.stack([ex['current']['prob'] for ex in batch]).astype('float32')).to(device)\n",
    "        logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "\n",
    "        # (clipped) importance weight: \n",
    "        # because the policy may have changed since the tuple was collected.\n",
    "        iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "    \n",
    "        loss = iw * loss_\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_value.step()\n",
    "    \n",
    "    copy_params(value, value_old)\n",
    "        \n",
    "    if value_loss < 0.:\n",
    "        value_loss = loss_.mean().item()\n",
    "    else:\n",
    "        value_loss = 0.9 * value_loss + 0.1 * loss_.mean().item()\n",
    "    \n",
    "    if numpy.mod(ni, disp_iter) == 0:\n",
    "        print('# plays', (ni+1) * n_collect, 'return', ret, 'value_loss', value_loss, 'entropy', -entropy)\n",
    "    \n",
    "    # fit a policy\n",
    "    value.eval()\n",
    "    player.train()\n",
    "    for pi in range(n_policy):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        batch_x = torch.from_numpy(numpy.stack([ex['current']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_xn = torch.from_numpy(numpy.stack([ex['next']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_r = torch.from_numpy(numpy.stack([ex['current']['rew'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        \n",
    "        batch_v = value(batch_x).clone().detach()\n",
    "        batch_vn = value(batch_xn).clone().detach()\n",
    "        \n",
    "        batch_a = torch.from_numpy(numpy.stack([ex['current']['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        batch_q = torch.from_numpy(numpy.stack([ex['current']['prob'] for ex in batch]).astype('float32')).to(device)\n",
    "\n",
    "        batch_pi = player(batch_x, normalized=True)\n",
    "        \n",
    "        logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "        \n",
    "        # advantage: r(s,a) + \\gamma * V(s') - V(s)\n",
    "        adv = batch_r + discount_factor * batch_vn - batch_v\n",
    "        adv = adv / adv.abs().max().clamp(min=1.)\n",
    "        \n",
    "        loss = -(adv * logp)\n",
    "        \n",
    "        # (clipped) importance weight: \n",
    "        # because the policy may have changed since the tuple was collected.\n",
    "        iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "    \n",
    "        loss = iw * loss\n",
    "        \n",
    "        # entropy regularization: though, it doesn't look necessary in this specific case.\n",
    "        ent = (batch_pi * torch.log(batch_pi)).sum(1)\n",
    "        \n",
    "        if entropy == -numpy.Inf:\n",
    "            entropy = ent.mean().item()\n",
    "        else:\n",
    "            entropy = 0.9 * entropy + 0.1 * ent.mean().item()\n",
    "        \n",
    "        loss = (loss + ent_coeff * ent).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_player.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.figure()\n",
    "\n",
    "plot.plot(return_history)\n",
    "plot.grid(True)\n",
    "plot.xlabel('# of plays x {}'.format(n_collect))\n",
    "plot.ylabel('Return over the episode of length {}'.format(max_len))\n",
    "\n",
    "plot.show()\n",
    "plot.savefig('return_log.pdf', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let the final policy play the pong longer\n",
    "player.eval()\n",
    "_, _, _, _, _, ret_ = collect_one_episode(env, player, max_len=1000000, deterministic=True, rendering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
