{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Buffer, collect_one_episode, normalize_obs, copy_params, avg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResLinear(nn.Module):\n",
    "    def __init__(self, n_in, n_out, act=nn.ReLU()):\n",
    "        super(ResLinear, self).__init__()\n",
    "        self.act = act\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "        self.bn = nn.BatchNorm1d(n_out)\n",
    "        \n",
    "        assert(n_in == n_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.act(self.bn(self.linear(x)))\n",
    "        return h + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(nn.Module):\n",
    "    def __init__(self, n_in=128, n_hid=100, n_out=6):\n",
    "        super(Player, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid),\n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, n_out))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, obs, normalized=False):\n",
    "        if normalized:\n",
    "            return self.softmax(self.layers(obs))\n",
    "        else:\n",
    "            return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(nn.Module):\n",
    "    def __init__(self, n_in=128, n_hid=100):\n",
    "        super(Value, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid),\n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, 1))\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-ram-v0')\n",
    "# env = gym.make('Assault-ram-v0')\n",
    "\n",
    "n_frames = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a policy\n",
    "player = Player(n_in=128 * n_frames, n_hid=32, n_out=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a value estimator\n",
    "value = Value(n_in=128 * n_frames, n_hid=32).to(device)\n",
    "value_old = Value(n_in=128 * n_frames, n_hid=32).to(device)\n",
    "copy_params(value, value_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimizers\n",
    "opt_player = Adam(player.parameters(), lr=0.0001)\n",
    "opt_value = Adam(value.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize replay buffer\n",
    "replay_buffer = Buffer(max_items=50000, n_frames=n_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid run -20.0 -20.0\n",
      "# plays 1 return -14.0 value_loss 0.033731766045093536 entropy inf\n",
      "Valid run -16.0 -19.6\n",
      "# plays 2 return -14.4 value_loss 0.033103793673217294 entropy 1.7172912003572065\n",
      "Valid run -20.0 -19.64\n",
      "# plays 3 return -14.760000000000002 value_loss 0.032623151149600745 entropy 1.6468200642714312\n",
      "Valid run -17.0 -19.376\n",
      "# plays 4 return -14.884000000000002 value_loss 0.031084919570013882 entropy 1.578775800818984\n",
      "Valid run -17.0 -19.1384\n",
      "# plays 5 return -14.795600000000002 value_loss 0.029353572348318993 entropy 1.5888493484133865\n",
      "Valid run -18.0 -19.02456\n",
      "# plays 6 return -15.216040000000003 value_loss 0.027622609476353973 entropy 1.5832249633379019\n",
      "Valid run -8.0 -17.922104\n",
      "# plays 7 return -15.194436000000003 value_loss 0.025817276979332792 entropy 1.5500605946583879\n",
      "Valid run -18.0 -17.929893600000003\n",
      "# plays 8 return -14.574992400000003 value_loss 0.02400024111108091 entropy 1.5256270774380376\n",
      "Valid run -20.0 -18.136904240000003\n",
      "# plays 9 return -14.617493160000002 value_loss 0.022352632508004663 entropy 1.503208660083536\n",
      "Valid run -17.0 -18.023213816000002\n",
      "# plays 10 return -15.055743844000002 value_loss 0.020643490841902756 entropy 1.4849566622757109\n",
      "Valid run -18.0 -18.020892434400004\n",
      "# plays 11 return -15.150169459600002 value_loss 0.019013835172430393 entropy 1.4903708059745802\n",
      "Valid run -20.0 -18.218803190960003\n",
      "# plays 12 return -15.235152513640003 value_loss 0.01755196540928696 entropy 1.4484038245575837\n",
      "Valid run -20.0 -18.396922871864003\n",
      "# plays 13 return -15.411637262276003 value_loss 0.016075214690545035 entropy 1.423391907874834\n",
      "Valid run -20.0 -18.557230584677605\n",
      "# plays 14 return -15.170473536048403 value_loss 0.01489831000446871 entropy 1.3848580542935738\n",
      "Valid run -17.0 -18.401507526209844\n",
      "# plays 15 return -15.253426182443564 value_loss 0.013828376171033935 entropy 1.3559860470441534\n",
      "Valid run -17.0 -18.261356773588858\n",
      "# plays 16 return -15.528083564199209 value_loss 0.01276108826748999 entropy 1.3103973897047456\n",
      "Valid run -17.0 -18.135221096229973\n",
      "# plays 17 return -15.17527520777929 value_loss 0.011976414568073798 entropy 1.308596536347186\n",
      "Valid run -16.0 -17.921698986606977\n",
      "# plays 18 return -15.657747687001361 value_loss 0.011059393944193942 entropy 1.247403171864374\n",
      "Valid run -17.0 -17.82952908794628\n",
      "# plays 19 return -16.091972918301224 value_loss 0.010412289568573258 entropy 1.1996525717127426\n",
      "Valid run -19.0 -17.946576179151652\n",
      "# plays 20 return -16.182775626471102 value_loss 0.00983439475674035 entropy 1.1880373975284801\n",
      "Valid run -19.0 -18.051918561236487\n",
      "# plays 21 return -16.464498063823992 value_loss 0.00927152351255955 entropy 1.1497753524657752\n",
      "Valid run -20.0 -18.246726705112838\n",
      "# plays 22 return -16.518048257441592 value_loss 0.008764707772872523 entropy 1.1375343455696114\n",
      "Valid run -17.0 -18.122054034601554\n",
      "# plays 23 return -16.66624343169743 value_loss 0.008073478534150877 entropy 1.103520942407342\n",
      "Valid run -17.0 -18.0098486311414\n",
      "# plays 24 return -16.499619088527687 value_loss 0.007388127116072557 entropy 1.1292326443838836\n",
      "Valid run -19.0 -18.10886376802726\n",
      "# plays 25 return -16.54965717967492 value_loss 0.006960633608512624 entropy 1.11347846014455\n",
      "Valid run -16.0 -17.897977391224536\n",
      "# plays 26 return -16.394691461707428 value_loss 0.006942695923429022 entropy 1.1105954763780908\n",
      "Valid run -16.0 -17.708179652102086\n",
      "# plays 27 return -16.755222315536685 value_loss 0.006433283419692924 entropy 1.1313527738434461\n",
      "Valid run -19.0 -17.837361686891878\n",
      "# plays 28 return -16.779700083983016 value_loss 0.006056481926732729 entropy 1.1350575926943365\n",
      "Valid run -17.0 -17.75362551820269\n",
      "# plays 29 return -16.801730075584715 value_loss 0.005777207511411729 entropy 1.1297550465242232\n",
      "Valid run -19.0 -17.87826296638242\n",
      "# plays 30 return -16.821557068026245 value_loss 0.005373736340866199 entropy 1.0755716652898717\n",
      "Valid run -17.0 -17.790436669744178\n",
      "# plays 31 return -17.13940136122362 value_loss 0.0049908771196408965 entropy 1.0746837445242297\n",
      "Valid run -19.0 -17.91139300276976\n",
      "# plays 32 return -17.225461225101256 value_loss 0.004717507075167051 entropy 1.029851677891332\n",
      "Valid run -17.0 -17.820253702492785\n",
      "# plays 33 return -17.40291510259113 value_loss 0.004455763298515693 entropy 1.0102178463296916\n",
      "Valid run -17.0 -17.738228332243505\n",
      "# plays 34 return -17.562623592332017 value_loss 0.004081240024244829 entropy 0.9822276486143622\n",
      "Valid run -19.0 -17.864405499019153\n"
     ]
    }
   ],
   "source": [
    "n_iter = 1000\n",
    "init_collect = 1\n",
    "n_collect = 1\n",
    "n_value = 150\n",
    "n_policy = 150\n",
    "disp_iter = 1\n",
    "val_iter = 1\n",
    "\n",
    "max_len = 1000\n",
    "batch_size = 1000\n",
    "\n",
    "ent_coeff = 0. #0.001\n",
    "discount_factor = .95\n",
    "\n",
    "value_loss = -numpy.Inf\n",
    "ret = -numpy.Inf\n",
    "entropy = -numpy.Inf\n",
    "valid_ret = -numpy.Inf\n",
    "\n",
    "return_history = []\n",
    "\n",
    "for ni in range(n_iter):\n",
    "    player.eval()\n",
    "\n",
    "    if numpy.mod(ni, val_iter) == 0:\n",
    "        _, _, _, _, _, ret_ = collect_one_episode(env, player, max_len=max_len, deterministic=True, n_frames=n_frames)\n",
    "        return_history.append(ret_)\n",
    "        if valid_ret == -numpy.Inf:\n",
    "            valid_ret = ret_\n",
    "        else:\n",
    "            valid_ret = 0.9 * valid_ret + 0.1 * ret_\n",
    "        print('Valid run', ret_, valid_ret)\n",
    "\n",
    "    # collect some episodes using the current policy\n",
    "    # and push (obs,a,r,p(a)) tuples to the replay buffer.\n",
    "    nc = n_collect\n",
    "    if ni == 0:\n",
    "        nc = init_collect\n",
    "    for ci in range(nc):\n",
    "        o_, r_, c_, a_, ap_, ret_ = collect_one_episode(env, player, max_len=max_len, discount_factor=discount_factor, n_frames=n_frames)\n",
    "        replay_buffer.add(o_, r_, c_, a_, ap_)\n",
    "        if ret == -numpy.Inf:\n",
    "            ret = ret_\n",
    "        else:\n",
    "            ret = 0.9 * ret + 0.1 * ret_\n",
    "    \n",
    "    # fit a value function\n",
    "    # TD(1)\n",
    "    value.train()\n",
    "    for vi in range(n_value):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "\n",
    "        batch_x = torch.from_numpy(numpy.stack([ex['current']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_r = torch.from_numpy(numpy.stack([ex['current']['rew'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_xn = torch.from_numpy(numpy.stack([ex['next']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        pred_y = value(batch_x).squeeze()\n",
    "        pred_next = value_old(batch_xn).squeeze().clone().detach()\n",
    "        loss_ = ((batch_r + discount_factor * pred_next - pred_y) ** 2)\n",
    "        \n",
    "        batch_a = torch.from_numpy(numpy.stack([ex['current']['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        batch_pi = player(batch_x, normalized=True)\n",
    "        batch_q = torch.from_numpy(numpy.stack([ex['current']['prob'] for ex in batch]).astype('float32')).to(device)\n",
    "        logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "\n",
    "        # (clipped) importance weight: \n",
    "        # because the policy may have changed since the tuple was collected.\n",
    "        iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "    \n",
    "        loss = iw * loss_\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_value.step()\n",
    "    \n",
    "    copy_params(value, value_old)\n",
    "        \n",
    "    if value_loss < 0.:\n",
    "        value_loss = loss_.mean().item()\n",
    "    else:\n",
    "        value_loss = 0.9 * value_loss + 0.1 * loss_.mean().item()\n",
    "    \n",
    "    if numpy.mod(ni, disp_iter) == 0:\n",
    "        print('# plays', (ni+1) * n_collect, 'return', ret, 'value_loss', value_loss, 'entropy', -entropy)\n",
    "    \n",
    "    # fit a policy\n",
    "    value.eval()\n",
    "    player.train()\n",
    "    for pi in range(n_policy):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        batch_x = torch.from_numpy(numpy.stack([ex['current']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_xn = torch.from_numpy(numpy.stack([ex['next']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_r = torch.from_numpy(numpy.stack([ex['current']['rew'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        \n",
    "        batch_v = value(batch_x).clone().detach()\n",
    "        batch_vn = value(batch_xn).clone().detach()\n",
    "        \n",
    "        batch_a = torch.from_numpy(numpy.stack([ex['current']['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        batch_q = torch.from_numpy(numpy.stack([ex['current']['prob'] for ex in batch]).astype('float32')).to(device)\n",
    "\n",
    "        batch_pi = player(batch_x, normalized=True)\n",
    "        \n",
    "        logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "        \n",
    "        # advantage: r(s,a) + \\gamma * V(s') - V(s)\n",
    "        adv = batch_r + discount_factor * batch_vn - batch_v\n",
    "        adv = adv / adv.abs().max().clamp(min=1.)\n",
    "        \n",
    "        loss = -(adv * logp)\n",
    "        \n",
    "        # (clipped) importance weight: \n",
    "        # because the policy may have changed since the tuple was collected.\n",
    "        iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "    \n",
    "        loss = iw * loss\n",
    "        \n",
    "        # entropy regularization: though, it doesn't look necessary in this specific case.\n",
    "        ent = (batch_pi * torch.log(batch_pi)).sum(1)\n",
    "        \n",
    "        if entropy == -numpy.Inf:\n",
    "            entropy = ent.mean().item()\n",
    "        else:\n",
    "            entropy = 0.9 * entropy + 0.1 * ent.mean().item()\n",
    "        \n",
    "        loss = (loss + ent_coeff * ent).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_player.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08476cfc52a44f10a6f89ff1623beb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot.figure()\n",
    "\n",
    "plot.plot(return_history)\n",
    "plot.grid(True)\n",
    "plot.xlabel('# of plays x {}'.format(n_collect))\n",
    "plot.ylabel('Return over the episode of length {}'.format(max_len))\n",
    "\n",
    "plot.show()\n",
    "plot.savefig('return_log.pdf', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let the final policy play the pong longer\n",
    "player.eval()\n",
    "_, _, _, _, _, ret_ = collect_one_episode(env, player, max_len=1000000, deterministic=True, rendering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
