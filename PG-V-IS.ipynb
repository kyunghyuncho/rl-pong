{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResLinear(nn.Module):\n",
    "    def __init__(self, n_in, n_out, act=nn.ReLU()):\n",
    "        super(ResLinear, self).__init__()\n",
    "        self.act = act\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "        self.bn = nn.BatchNorm1d(n_out)\n",
    "        \n",
    "        assert(n_in == n_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.act(self.bn(self.linear(x)))\n",
    "        return h + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(nn.Module):\n",
    "    def __init__(self, n_in=128, n_hid=100, n_out=6):\n",
    "        super(Player, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid),\n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, n_out))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, obs, normalized=False):\n",
    "        if normalized:\n",
    "            return self.softmax(self.layers(obs))\n",
    "        else:\n",
    "            return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(nn.Module):\n",
    "    def __init__(self, n_in=128, n_hid=100):\n",
    "        super(Value, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid),\n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, 1))\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_params(from_, to_):\n",
    "    for f_, t_ in zip(from_.parameters(), to_.parameters()):\n",
    "        t_.data.copy_(f_.data)\n",
    "        \n",
    "def avg_params(from_, to_, coeff=0.95):\n",
    "    for f_, t_ in zip(from_.parameters(), to_.parameters()):\n",
    "        t_.data.copy_(coeff * t_.data + (1.-coeff) * f_.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_obs(obs):\n",
    "    return obs.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data\n",
    "def collect_one_episode(env, player, max_len=50, discount_factor=0.9, deterministic=False, rendering=False, verbose=False):\n",
    "    episode = []\n",
    "\n",
    "    observations = []\n",
    "\n",
    "    rewards = []\n",
    "    crewards = []\n",
    "\n",
    "    actions = []\n",
    "    action_probs = []\n",
    "\n",
    "    obs = env.reset()\n",
    "    \n",
    "    for ml in range(max_len):\n",
    "        if rendering:\n",
    "            env.render()\n",
    "            sleep(0.05)\n",
    "            \n",
    "        obs = normalize_obs(obs)\n",
    "\n",
    "        out_probs = player(torch.from_numpy(obs[None,:]).to(device), normalized=True).squeeze()\n",
    "        \n",
    "        if deterministic:\n",
    "            action = numpy.argmax(out_probs.to('cpu').data.numpy())\n",
    "            if verbose:\n",
    "                print(out_probs, action)\n",
    "        else:\n",
    "            act_dist = Categorical(out_probs)\n",
    "            action = act_dist.sample().item()\n",
    "        action_prob = out_probs[action].item()\n",
    "\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        action_probs.append(action_prob)\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if deterministic and verbose:\n",
    "            print(reward, done)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "\n",
    "    rewards = numpy.array(rewards)\n",
    "\n",
    "    # it's probably not the best idea to compute the discounted cumulative returns here, but well..\n",
    "    for ri in range(len(rewards)):\n",
    "        factors = (discount_factor ** numpy.arange(len(rewards)-ri))\n",
    "        crewards.append(numpy.sum(rewards[ri:] * factors))\n",
    "        \n",
    "    # discard the final 10%, because it really doesn't give me a good signal due to the unbounded horizon\n",
    "    # this is only for training, not for computing the total return of the episode of the given length\n",
    "    discard = max_len // 10\n",
    "        \n",
    "    return observations[:-discard], crewards[:-discard], actions[:-discard], action_probs[:-discard], rewards.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple implementation of FIFO-based replay buffer\n",
    "class Buffer:\n",
    "    def __init__(self, max_items=10000):\n",
    "        self.max_items = max_items\n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, observations, crewards, actions, action_probs):\n",
    "        new_n = len(observations)\n",
    "        old_n = len(self.buffer)\n",
    "        if new_n + old_n > self.max_items:\n",
    "            del self.buffer[:new_n]\n",
    "        for o, c, a, p in zip(observations, crewards, actions, action_probs):\n",
    "            self.buffer.append({'obs': o, \n",
    "                                'crew': c, \n",
    "                                'act': a, \n",
    "                                'prob': p})\n",
    "            \n",
    "    def sample(self, n=100):\n",
    "        idxs = numpy.random.choice(len(self.buffer),n)\n",
    "        return [self.buffer[ii] for ii in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a policy\n",
    "player = Player(n_in=128, n_hid=128, n_out=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a value estimator\n",
    "value = Value(n_in=128, n_hid=128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimizers\n",
    "opt_player = Adam(player.parameters(), lr=0.0001)\n",
    "opt_value = Adam(value.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize replay buffer\n",
    "replay_buffer = Buffer(max_items=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid run -20.0 -20.0\n",
      "# plays 1 return -16.0 value_loss 0.023137284442782402 entropy inf\n",
      "Valid run -20.0 -20.0\n",
      "# plays 2 return -16.1 value_loss 0.022702215053141118 entropy 1.6558149141417464\n",
      "Valid run -16.0 -19.6\n",
      "# plays 3 return -16.39 value_loss 0.02156923498958349 entropy 1.6210344867858724\n",
      "Valid run -20.0 -19.64\n",
      "# plays 4 return -15.951 value_loss 0.02149133826419711 entropy 1.5494103916183428\n",
      "Valid run -20.0 -19.676000000000002\n",
      "# plays 5 return -16.0559 value_loss 0.021122910895571112 entropy 1.4447876454991573\n",
      "Valid run -13.0 -19.0084\n",
      "# plays 6 return -16.15031 value_loss 0.020639202916882933 entropy 1.4266170005999066\n",
      "Valid run -14.0 -18.50756\n",
      "# plays 7 return -15.935279000000001 value_loss 0.02000217349016294 entropy 1.3818747999403453\n",
      "Valid run -10.0 -17.656804\n",
      "# plays 8 return -15.841751100000002 value_loss 0.01964448990538977 entropy 1.2917913688943405\n",
      "Valid run -10.0 -16.8911236\n",
      "# plays 9 return -15.657575990000002 value_loss 0.019433153216081224 entropy 1.2315484224503135\n",
      "Valid run -6.0 -15.80201124\n",
      "# plays 10 return -15.291818391000003 value_loss 0.01936652627054217 entropy 1.1826187969939128\n",
      "Valid run -15.0 -15.721810116\n",
      "# plays 11 return -14.962636551900005 value_loss 0.019717331154316925 entropy 1.1304508525648744\n",
      "Valid run -9.0 -15.049629104400001\n",
      "# plays 12 return -14.566372896710003 value_loss 0.02004195135668242 entropy 1.0782516842673264\n",
      "Valid run -11.0 -14.644666193960001\n",
      "# plays 13 return -14.009735607039003 value_loss 0.02028423404844395 entropy 1.0552363247626442\n",
      "Valid run -14.0 -14.580199574564002\n",
      "# plays 14 return -13.908762046335104 value_loss 0.020471680927143618 entropy 1.0124398086827833\n",
      "Valid run -13.0 -14.422179617107602\n",
      "# plays 15 return -13.917885841701594 value_loss 0.020809794419586174 entropy 0.9729955498908701\n",
      "Valid run -15.0 -14.479961655396842\n",
      "# plays 16 return -13.926097257531435 value_loss 0.02106878194532947 entropy 0.9490892211327813\n",
      "Valid run -9.0 -13.93196548985716\n",
      "# plays 17 return -13.733487531778291 value_loss 0.021402038000424114 entropy 0.9321346604320245\n",
      "Valid run -10.0 -13.538768940871444\n",
      "# plays 18 return -13.360138778600462 value_loss 0.021695175941943593 entropy 0.9049735814681255\n",
      "Valid run -8.0 -12.984892046784301\n",
      "# plays 19 return -13.124124900740416 value_loss 0.02202182059093666 entropy 0.8843681060525898\n",
      "Valid run -12.0 -12.88640284210587\n",
      "# plays 20 return -12.211712410666374 value_loss 0.022305311102608017 entropy 0.8790566546904742\n",
      "Valid run -10.0 -12.597762557895283\n",
      "# plays 21 return -11.690541169599737 value_loss 0.022609010790830112 entropy 0.8367052590228539\n",
      "Valid run -6.0 -11.937986302105754\n",
      "# plays 22 return -11.821487052639764 value_loss 0.023217412863712244 entropy 0.8210677854371693\n",
      "Valid run -7.0 -11.444187671895179\n",
      "# plays 23 return -11.839338347375786 value_loss 0.023519986414498747 entropy 0.7885583964564413\n",
      "Valid run -6.0 -10.899768904705661\n",
      "# plays 24 return -11.255404512638208 value_loss 0.023837883585917947 entropy 0.7639721514401989\n",
      "Valid run -4.0 -10.209792014235095\n",
      "# plays 25 return -10.929864061374388 value_loss 0.023876355906630755 entropy 0.749882471096969\n",
      "Valid run -8.0 -9.988812812811586\n",
      "# plays 26 return -10.136877655236951 value_loss 0.024277475613628505 entropy 0.7234023184519572\n",
      "Valid run -9.0 -9.889931531530427\n",
      "# plays 27 return -9.323189889713255 value_loss 0.024491532196052088 entropy 0.6883601581712933\n",
      "Valid run -6.0 -9.500938378377384\n",
      "# plays 28 return -8.89087090074193 value_loss 0.024526763528091206 entropy 0.6776727178672609\n",
      "Valid run -9.0 -9.450844540539647\n",
      "# plays 29 return -8.801783810667738 value_loss 0.024902598584743585 entropy 0.6622099931200387\n",
      "Valid run -11.0 -9.605760086485683\n",
      "# plays 30 return -8.921605429600964 value_loss 0.025001536928163988 entropy 0.6388842790522696\n",
      "Valid run -9.0 -9.545184077837115\n",
      "# plays 31 return -7.9294448866408676 value_loss 0.025037695726049265 entropy 0.6291361893879732\n",
      "Valid run -2.0 -8.790665670053404\n",
      "# plays 32 return -7.736500397976782 value_loss 0.025277052232884456 entropy 0.6064993414941349\n",
      "Valid run -8.0 -8.711599103048064\n",
      "# plays 33 return -7.062850358179103 value_loss 0.025189080790516516 entropy 0.5939953954673897\n",
      "Valid run -4.0 -8.240439192743258\n",
      "# plays 34 return -7.056565322361194 value_loss 0.025200522875043853 entropy 0.5908938223281588\n",
      "Valid run -7.0 -8.116395273468932\n",
      "# plays 35 return -6.450908790125074 value_loss 0.02516859570317248 entropy 0.5752975074004129\n",
      "Valid run -5.0 -7.804755746122039\n",
      "# plays 36 return -6.605817911112567 value_loss 0.02501110888277989 entropy 0.5602978780909778\n",
      "Valid run -3.0 -7.324280171509835\n",
      "# plays 37 return -6.645236120001311 value_loss 0.025215045460362746 entropy 0.5571596897466121\n",
      "Valid run -9.0 -7.491852154358852\n",
      "# plays 38 return -6.380712508001181 value_loss 0.0256208984315093 entropy 0.5382981613858308\n",
      "Valid run -5.0 -7.242666938922967\n",
      "# plays 39 return -6.442641257201063 value_loss 0.025823099345835654 entropy 0.5405491631241578\n",
      "Valid run -3.0 -6.81840024503067\n",
      "# plays 40 return -6.598377131480957 value_loss 0.025877663505857535 entropy 0.5317566451996406\n",
      "Valid run -8.0 -6.936560220527603\n",
      "# plays 41 return -6.038539418332861 value_loss 0.025988121379421962 entropy 0.5087955094099706\n",
      "Valid run -1.0 -6.342904198474843\n",
      "# plays 42 return -5.734685476499575 value_loss 0.026415482132738963 entropy 0.49610686805512355\n",
      "Valid run -2.0 -5.908613778627359\n",
      "# plays 43 return -5.561216928849618 value_loss 0.026650391863977434 entropy 0.493467523060352\n",
      "Valid run -4.0 -5.717752400764623\n",
      "# plays 44 return -6.005095235964656 value_loss 0.02665792287635403 entropy 0.4943510117842967\n",
      "Valid run -2.0 -5.345977160688161\n",
      "# plays 45 return -5.8045857123681905 value_loss 0.026830927710958696 entropy 0.4844422459665567\n",
      "Valid run -9.0 -5.711379444619346\n",
      "# plays 46 return -5.5241271411313715 value_loss 0.02681408890893612 entropy 0.4688685171929815\n",
      "Valid run -6.0 -5.740241500157412\n",
      "# plays 47 return -5.871714427018235 value_loss 0.026756946426426356 entropy 0.46879514530694005\n",
      "Valid run -1.0 -5.2662173501416705\n",
      "# plays 48 return -5.684542984316412 value_loss 0.026498725837583826 entropy 0.46238630403114134\n",
      "Valid run -4.0 -5.1395956151275035\n",
      "# plays 49 return -5.616088685884771 value_loss 0.026379553967221515 entropy 0.45322459563744477\n",
      "Valid run -3.0 -4.925636053614753\n",
      "# plays 50 return -5.354479817296293 value_loss 0.02675021621749157 entropy 0.44450807887719046\n",
      "Valid run -2.0 -4.633072448253278\n",
      "# plays 51 return -5.019031835566664 value_loss 0.02715446721612234 entropy 0.43917031278612406\n",
      "Valid run -9.0 -5.069765203427951\n",
      "# plays 52 return -5.117128652009997 value_loss 0.027017263176430634 entropy 0.43628133439370553\n",
      "Valid run -3.0 -4.862788683085156\n",
      "# plays 53 return -5.005415786808998 value_loss 0.027244058156659638 entropy 0.4357322119343593\n",
      "Valid run -7.0 -5.0765098147766405\n",
      "# plays 54 return -5.1048742081280984 value_loss 0.02710272076396683 entropy 0.42370108659452965\n",
      "Valid run -6.0 -5.168858833298977\n",
      "# plays 55 return -5.194386787315288 value_loss 0.02689884951861387 entropy 0.41665570686768666\n",
      "Valid run -5.0 -5.151972949969079\n",
      "# plays 56 return -5.474948108583759 value_loss 0.027058014619216492 entropy 0.41960327116138574\n",
      "Valid run -8.0 -5.4367756549721715\n",
      "# plays 57 return -5.527453297725383 value_loss 0.02707433400112257 entropy 0.4157130456246043\n",
      "Valid run -7.0 -5.593098089474955\n",
      "# plays 58 return -5.174707967952846 value_loss 0.027487862572591296 entropy 0.4137266962501571\n",
      "Valid run -8.0 -5.833788280527459\n",
      "# plays 59 return -5.257237171157561 value_loss 0.02763012324171137 entropy 0.40128348931460645\n",
      "Valid run -3.0 -5.550409452474713\n",
      "# plays 60 return -5.131513454041805 value_loss 0.027507415631981533 entropy 0.3990119108500171\n",
      "Valid run -7.0 -5.695368507227242\n",
      "# plays 61 return -5.1183621086376245 value_loss 0.027550108144794562 entropy 0.39240465991706525\n",
      "Valid run -6.0 -5.725831656504518\n",
      "# plays 62 return -5.506525897773862 value_loss 0.027899773450298305 entropy 0.3940609736474966\n",
      "Valid run -3.0 -5.453248490854066\n",
      "# plays 63 return -5.355873307996476 value_loss 0.02778274423474359 entropy 0.3921042328418284\n",
      "Valid run -10.0 -5.90792364176866\n",
      "# plays 64 return -5.3202859771968285 value_loss 0.02763745749677081 entropy 0.3839913674111315\n",
      "Valid run -7.0 -6.017131277591794\n",
      "# plays 65 return -5.188257379477146 value_loss 0.02747012155221277 entropy 0.3888728434509191\n",
      "Valid run -10.0 -6.415418149832615\n",
      "# plays 66 return -5.169431641529432 value_loss 0.026945392954048873 entropy 0.38209637476720903\n",
      "Valid run 0.0 -5.773876334849353\n",
      "# plays 67 return -4.752488477376488 value_loss 0.02706679649743202 entropy 0.3701407519619933\n",
      "Valid run -5.0 -5.696488701364418\n",
      "# plays 68 return -4.87723962963884 value_loss 0.027252491095001365 entropy 0.35658617502633944\n",
      "Valid run -2.0 -5.326839831227976\n",
      "# plays 69 return -4.589515666674957 value_loss 0.027243312026561675 entropy 0.38760569875082385\n",
      "Valid run 1.0 -4.694155848105178\n",
      "# plays 70 return -4.330564100007461 value_loss 0.0270689446409059 entropy 0.3857387396883434\n",
      "Valid run -1.0 -4.32474026329466\n",
      "# plays 71 return -4.797507690006715 value_loss 0.027194941596345223 entropy 0.37621573749535114\n",
      "Valid run -5.0 -4.392266236965194\n",
      "# plays 72 return -5.117756921006044 value_loss 0.026859789876183378 entropy 0.37245719549309986\n",
      "Valid run -7.0 -4.653039613268675\n",
      "# plays 73 return -5.20598122890544 value_loss 0.02661918289635227 entropy 0.3659734495575538\n",
      "Valid run -6.0 -4.787735651941807\n",
      "# plays 74 return -4.985383106014896 value_loss 0.026809656058314257 entropy 0.3663155192510274\n",
      "Valid run -9.0 -5.2089620867476265\n",
      "# plays 75 return -4.586844795413406 value_loss 0.02682472830375685 entropy 0.38455402209724926\n",
      "Valid run -7.0 -5.388065878072864\n",
      "# plays 76 return -4.328160315872066 value_loss 0.02657246126556981 entropy 0.3803958551305058\n",
      "Valid run -8.0 -5.649259290265578\n",
      "# plays 77 return -4.0953442842848595 value_loss 0.026399200525801674 entropy 0.3675302553521322\n",
      "Valid run -1.0 -5.18433336123902\n",
      "# plays 78 return -4.385809855856373 value_loss 0.02634840193413612 entropy 0.3746828083258199\n",
      "Valid run -3.0 -4.9659000251151175\n",
      "# plays 79 return -3.8472288702707362 value_loss 0.02638940013667569 entropy 0.37095405544700794\n",
      "Valid run -10.0 -5.469310022603606\n",
      "# plays 80 return -4.2625059832436625 value_loss 0.02634763473022543 entropy 0.3716294992691585\n",
      "Valid run 0.0 -4.922379020343246\n",
      "# plays 81 return -4.136255384919297 value_loss 0.026182300072150493 entropy 0.37357065544244566\n",
      "Valid run -7.0 -5.130141118308921\n",
      "# plays 82 return -4.222629846427367 value_loss 0.026387546372029544 entropy 0.3731141299252098\n",
      "Valid run -3.0 -4.917127006478029\n",
      "# plays 83 return -3.70036686178463 value_loss 0.026112302350950266 entropy 0.368143811114941\n",
      "Valid run -6.0 -5.0254143058302265\n",
      "# plays 84 return -3.8303301756061674 value_loss 0.026292426176028276 entropy 0.3647647602271162\n",
      "Valid run -1.0 -4.622872875247204\n",
      "# plays 85 return -3.4472971580455507 value_loss 0.025854902784964585 entropy 0.36463244459503885\n",
      "Valid run -5.0 -4.660585587722483\n",
      "# plays 86 return -3.502567442240996 value_loss 0.025671726677663157 entropy 0.3672821979600785\n",
      "Valid run -2.0 -4.394527028950235\n",
      "# plays 87 return -3.6523106980168962 value_loss 0.025668531623941986 entropy 0.35841577202419306\n",
      "Valid run -4.0 -4.355074326055212\n",
      "# plays 88 return -3.387079628215207 value_loss 0.025967066705938274 entropy 0.3666658706664653\n",
      "Valid run -11.0 -5.019566893449691\n",
      "# plays 89 return -3.2483716653936865 value_loss 0.025803210597380485 entropy 0.36803751746608043\n",
      "Valid run -5.0 -5.017610204104722\n",
      "# plays 90 return -3.823534498854318 value_loss 0.025549639547894437 entropy 0.3716830618738448\n",
      "Valid run 0.0 -4.51584918369425\n",
      "# plays 91 return -3.6411810489688863 value_loss 0.025308461593118347 entropy 0.356244247685677\n",
      "Valid run -7.0 -4.764264265324826\n",
      "# plays 92 return -3.777062944071998 value_loss 0.02526813646989498 entropy 0.35214036916024244\n",
      "Valid run -5.0 -4.787837838792344\n",
      "# plays 93 return -3.6993566496647983 value_loss 0.025104117810762827 entropy 0.34845296866014147\n",
      "Valid run -2.0 -4.5090540549131095\n",
      "# plays 94 return -3.8294209846983187 value_loss 0.025286206904086686 entropy 0.35423075516836\n",
      "Valid run -4.0 -4.4581486494217994\n",
      "# plays 95 return -3.646478886228487 value_loss 0.024844588833778995 entropy 0.3461645640272193\n",
      "Valid run -7.0 -4.71233378447962\n",
      "# plays 96 return -3.581830997605638 value_loss 0.02477778058296135 entropy 0.35668305795142663\n",
      "Valid run -5.0 -4.741100406031658\n",
      "# plays 97 return -3.9236478978450746 value_loss 0.0248062806073936 entropy 0.35341407754448917\n",
      "Valid run -3.0 -4.5669903654284925\n",
      "# plays 98 return -3.9312831080605672 value_loss 0.02504262243158628 entropy 0.34483928481756937\n",
      "Valid run -4.0 -4.510291328885644\n",
      "# plays 99 return -4.1381547972545105 value_loss 0.02545328324538776 entropy 0.3468698837976313\n",
      "Valid run -5.0 -4.559262195997079\n",
      "# plays 100 return -4.72433931752906 value_loss 0.025207283069214007 entropy 0.3497490322598777\n",
      "Valid run -13.0 -5.403335976397371\n",
      "# plays 101 return -4.951905385776154 value_loss 0.025005119937229852 entropy 0.3441408056312355\n",
      "Valid run -5.0 -5.363002378757634\n",
      "# plays 102 return -4.656714847198539 value_loss 0.02495399227521005 entropy 0.33982961389418137\n",
      "Valid run -6.0 -5.42670214088187\n",
      "# plays 103 return -4.391043362478685 value_loss 0.02495399011624917 entropy 0.3401695118979489\n",
      "Valid run -8.0 -5.684031926793684\n",
      "# plays 104 return -4.351939026230816 value_loss 0.024576966629502754 entropy 0.3383329857260901\n",
      "Valid run -6.0 -5.715628734114315\n",
      "# plays 105 return -4.716745123607735 value_loss 0.024271613433430417 entropy 0.3482830283868681\n",
      "Valid run -7.0 -5.844065860702884\n",
      "# plays 106 return -4.845070611246962 value_loss 0.024131371110203706 entropy 0.343445720673403\n",
      "Valid run -7.0 -5.959659274632596\n",
      "# plays 107 return -4.460563550122266 value_loss 0.023835439969172638 entropy 0.33655739981497945\n",
      "Valid run -4.0 -5.763693347169337\n",
      "# plays 108 return -4.41450719511004 value_loss 0.02404296259349933 entropy 0.3496503963858636\n",
      "Valid run -8.0 -5.987324012452403\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-4774d4375887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mbatch_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'act'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mbatch_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mbatch_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prob'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mlogp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_pi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-1be1a29a0d5e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs, normalized)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1621\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1622\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1624\u001b[0m     )\n\u001b[1;32m   1625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_iter = 1000\n",
    "init_collect = 1\n",
    "n_collect = 1\n",
    "n_value = 150\n",
    "n_policy = 150\n",
    "disp_iter = 1\n",
    "val_iter = 1\n",
    "\n",
    "max_len = 1000\n",
    "batch_size = 1000\n",
    "\n",
    "ent_coeff = 0. #0.001\n",
    "discount_factor = .95\n",
    "\n",
    "value_loss = -numpy.Inf\n",
    "ret = -numpy.Inf\n",
    "entropy = -numpy.Inf\n",
    "valid_ret = -numpy.Inf\n",
    "\n",
    "return_history = []\n",
    "\n",
    "for ni in range(n_iter):\n",
    "    player.eval()\n",
    "\n",
    "    if numpy.mod(ni, val_iter) == 0:\n",
    "        _, _, _, _, ret_ = collect_one_episode(env, player, max_len=max_len, deterministic=True)\n",
    "        return_history.append(ret_)\n",
    "        if valid_ret == -numpy.Inf:\n",
    "            valid_ret = ret_\n",
    "        else:\n",
    "            valid_ret = 0.9 * valid_ret + 0.1 * ret_\n",
    "        print('Valid run', ret_, valid_ret)\n",
    "\n",
    "    # collect some episodes using the current policy\n",
    "    # and push (obs,a,r,p(a)) tuples to the replay buffer.\n",
    "    nc = n_collect\n",
    "    if ni == 0:\n",
    "        nc = init_collect\n",
    "    for ci in range(nc):\n",
    "        o_, c_, a_, ap_, ret_ = collect_one_episode(env, player, max_len=max_len, discount_factor=discount_factor)\n",
    "        replay_buffer.add(o_, c_, a_, ap_)\n",
    "        if ret == -numpy.Inf:\n",
    "            ret = ret_\n",
    "        else:\n",
    "            ret = 0.9 * ret + 0.1 * ret_\n",
    "    \n",
    "    player.train()\n",
    "        \n",
    "    # fit a value function\n",
    "    for vi in range(n_value):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        batch_x = torch.from_numpy(numpy.stack([ex['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_y = torch.from_numpy(numpy.stack([ex['crew'] for ex in batch]).astype('float32')).to(device)\n",
    "        pred_y = value(batch_x).squeeze()\n",
    "        loss_ = ((batch_y - pred_y) ** 2)\n",
    "        \n",
    "        batch_a = torch.from_numpy(numpy.stack([ex['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        batch_pi = player(batch_x, normalized=True)\n",
    "        batch_q = torch.from_numpy(numpy.stack([ex['prob'] for ex in batch]).astype('float32')).to(device)\n",
    "        logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "\n",
    "        # (clipped) importance weight: \n",
    "        # because the policy may have changed since the tuple was collected.\n",
    "        iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "    \n",
    "        loss = iw * loss_\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_value.step()\n",
    "        \n",
    "    if value_loss < 0.:\n",
    "        value_loss = loss_.mean().item()\n",
    "    else:\n",
    "        value_loss = 0.9 * value_loss + 0.1 * loss_.mean().item()\n",
    "    \n",
    "    if numpy.mod(ni, disp_iter) == 0:\n",
    "        print('# plays', (ni+1) * n_collect, 'return', ret, 'value_loss', value_loss, 'entropy', -entropy)\n",
    "    \n",
    "    # fit a policy\n",
    "    for pi in range(n_policy):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        batch_x = torch.from_numpy(numpy.stack([ex['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_r = torch.from_numpy(numpy.stack([ex['crew'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        batch_v = value(batch_x)\n",
    "        batch_a = torch.from_numpy(numpy.stack([ex['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        batch_q = torch.from_numpy(numpy.stack([ex['prob'] for ex in batch]).astype('float32')).to(device)\n",
    "\n",
    "        batch_pi = player(batch_x, normalized=True)\n",
    "        \n",
    "        logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "        \n",
    "        # advantage\n",
    "        adv = batch_r - batch_v.clone().detach()\n",
    "        \n",
    "        loss = -(adv * logp)\n",
    "        \n",
    "        # (clipped) importance weight: \n",
    "        # because the policy may have changed since the tuple was collected.\n",
    "        iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "    \n",
    "        loss = iw * loss\n",
    "        \n",
    "        # entropy regularization: though, it doesn't look necessary in this specific case.\n",
    "        ent = (batch_pi * torch.log(batch_pi)).sum(1)\n",
    "        \n",
    "        if entropy == -numpy.Inf:\n",
    "            entropy = ent.mean().item()\n",
    "        else:\n",
    "            entropy = 0.9 * entropy + 0.1 * ent.mean().item()\n",
    "        \n",
    "        loss = (loss + ent_coeff * ent).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_player.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c040424c8645fd894fe04d63e8f62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot.figure()\n",
    "\n",
    "plot.plot(return_history)\n",
    "plot.grid(True)\n",
    "plot.xlabel('# of plays x {}'.format(n_collect))\n",
    "plot.ylabel('Return over the episode of length {}'.format(max_len))\n",
    "\n",
    "plot.show()\n",
    "plot.savefig('return_log.pdf', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m_ctypes/callbacks.c\u001b[0m in \u001b[0;36m'calling callback function'\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/pyglet/libs/darwin/cocoapy/runtime.py\u001b[0m in \u001b[0;36mobjc_method\u001b[0;34m(objc_self, objc_cmd, *args)\u001b[0m\n\u001b[1;32m   1119\u001b[0m                 \u001b[0mpy_self\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjc_cmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjc_cmd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_method_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mObjCClass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/pyglet/window/cocoa/pyglet_window.py\u001b[0m in \u001b[0;36mnextEventMatchingMask_untilDate_inMode_dequeue_\u001b[0;34m(self, mask, date, mode, dequeue)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mPygletWindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'@'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mNSUIntegerEncoding\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34mb'@@B'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnextEventMatchingMask_untilDate_inMode_dequeue_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdequeue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minLiveResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;31m# Call the idle() method while we're stuck in a live resize event.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/pyglet/libs/darwin/cocoapy/runtime.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjc_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_instance_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mObjCBoundMethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m         \u001b[0;31m# Else, search for class method with given name in the class object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;31m# If it exists, return callable object with a pointer to the class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "# let the final policy play the pong longer\n",
    "player.eval()\n",
    "_, _, _, _, ret_ = collect_one_episode(env, player, max_len=1000000, deterministic=True, rendering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
