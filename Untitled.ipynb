{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResLinear(nn.Module):\n",
    "    def __init__(self, n_in, n_out, act=nn.ReLU()):\n",
    "        super(ResLinear, self).__init__()\n",
    "        self.act = act\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "        self.bn = nn.BatchNorm1d(n_out)\n",
    "        \n",
    "        assert(n_in == n_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.act(self.bn(self.linear(x)))\n",
    "        return h + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(nn.Module):\n",
    "    def __init__(self, n_in=128, n_hid=100, n_out=6):\n",
    "        super(Player, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid),\n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, n_out))\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, obs, normalized=False):\n",
    "        if normalized:\n",
    "            return self.softmax(self.layers(obs))\n",
    "        else:\n",
    "            return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(nn.Module):\n",
    "    def __init__(self, n_in=128, n_hid=100):\n",
    "        super(Value, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid),\n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, 1))\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_params(from_, to_):\n",
    "    for f_, t_ in zip(from_.parameters(), to_.parameters()):\n",
    "        t_.data.copy_(f_.data)\n",
    "        \n",
    "def avg_params(from_, to_, coeff=0.95):\n",
    "    for f_, t_ in zip(from_.parameters(), to_.parameters()):\n",
    "        t_.data.copy_(coeff * t_.data + (1.-coeff) * f_.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_obs(obs):\n",
    "    return obs.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data\n",
    "def collect_one_episode(env, player, max_len=50, discount_factor=0.9, deterministic=False, rendering=False, verbose=False):\n",
    "    episode = []\n",
    "\n",
    "    observations = []\n",
    "\n",
    "    rewards = []\n",
    "    crewards = []\n",
    "\n",
    "    actions = []\n",
    "    action_probs = []\n",
    "\n",
    "    obs = env.reset()\n",
    "    \n",
    "    for ml in range(max_len):\n",
    "        if rendering:\n",
    "            env.render()\n",
    "            sleep(0.5)\n",
    "            \n",
    "        obs = normalize_obs(obs)\n",
    "\n",
    "        out_probs = player(torch.from_numpy(obs[None,:]), normalized=True).squeeze()\n",
    "        \n",
    "        if deterministic:\n",
    "            action = numpy.argmax(out_probs.data.numpy())\n",
    "            if verbose:\n",
    "                print(out_probs, action)\n",
    "        else:\n",
    "            act_dist = Categorical(out_probs)\n",
    "            action = act_dist.sample().item()\n",
    "        action_prob = out_probs[action].item()\n",
    "\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        action_probs.append(action_prob)\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if deterministic and verbose:\n",
    "            print(reward, done)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "\n",
    "    rewards = numpy.array(rewards)\n",
    "\n",
    "    for ri in range(len(rewards)):\n",
    "        factors = (discount_factor ** numpy.arange(len(rewards)-ri))\n",
    "        crewards.append(numpy.sum(rewards[ri:] * factors))\n",
    "        \n",
    "    # discard the final 10%, because it really doesn't give me a good signal due to the unbounded horizon\n",
    "    # this is only for training, not for computing the total return of the episode of the given length\n",
    "    discard = max_len // 10\n",
    "        \n",
    "    return observations[:-discard], crewards[:-discard], actions[:-discard], action_probs[:-discard], rewards.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, max_items=10000):\n",
    "        self.max_items = max_items\n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, observations, crewards, actions, action_probs):\n",
    "        new_n = len(observations)\n",
    "        old_n = len(self.buffer)\n",
    "        if new_n + old_n > self.max_items:\n",
    "            del self.buffer[:new_n]\n",
    "        for o, c, a, p in zip(observations, crewards, actions, action_probs):\n",
    "            self.buffer.append((o, c, a, p))\n",
    "            \n",
    "    def sample(self, n=100):\n",
    "        idxs = numpy.random.choice(len(self.buffer),n)\n",
    "        return [self.buffer[ii] for ii in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two models\n",
    "player = Player(n_in=128, n_hid=128, n_out=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a value estimator\n",
    "value = Value(n_in=128, n_hid=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player = Adam(player.parameters(), lr=0.0001)\n",
    "opt_value = Adam(value.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize replay buffer\n",
    "replay_buffer = Buffer(max_items=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyunghyuncho/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid run -20.0\n",
      "# plays 1 return -16.047217095000004 value_loss 0.04384222254157066 entropy inf\n",
      "Valid run -20.0\n",
      "# plays 2 return -15.842495385500005 value_loss 0.04204561561346054 entropy 1.6979414134276425\n",
      "Valid run -20.0\n",
      "# plays 3 return -15.758245846950004 value_loss 0.04074374325573444 entropy 1.6643410936027396\n",
      "Valid run -20.0\n",
      "# plays 4 return -15.682421262255003 value_loss 0.039283606857061386 entropy 1.659256919503523\n",
      "Valid run -20.0\n",
      "# plays 5 return -15.714179136029502 value_loss 0.03801600291579962 entropy 1.6297721570669141\n",
      "Valid run -20.0\n",
      "# plays 6 return -15.742761222426552 value_loss 0.03638627238646149 entropy 1.6026742048999554\n",
      "Valid run -20.0\n",
      "# plays 7 return -15.668485100183897 value_loss 0.03437507956542074 entropy 1.5668668910172823\n",
      "Valid run -20.0\n",
      "# plays 8 return -15.90163659016551 value_loss 0.0333298343880549 entropy 1.5081056329581133\n",
      "Valid run -20.0\n",
      "# plays 9 return -16.01147293114896 value_loss 0.0323350439337112 entropy 1.4706924168616262\n",
      "Valid run -20.0\n",
      "# plays 10 return -16.210325638034064 value_loss 0.031330709110249655 entropy 1.4859669625545682\n",
      "Valid run -19.0\n",
      "# plays 11 return -16.48929307423066 value_loss 0.029885084994628646 entropy 1.488219272712347\n",
      "Valid run -20.0\n",
      "# plays 12 return -16.240363766807594 value_loss 0.028840094344015077 entropy 1.4855934846191199\n",
      "Valid run -19.0\n",
      "# plays 13 return -16.616327390126834 value_loss 0.027676873611982306 entropy 1.52370655879349\n",
      "Valid run -20.0\n",
      "# plays 14 return -16.25469465111415 value_loss 0.026578789836980976 entropy 1.5211754357454992\n",
      "Valid run -20.0\n",
      "# plays 15 return -16.329225186002734 value_loss 0.025819138112799594 entropy 1.5244272761384436\n",
      "Valid run -18.0\n",
      "# plays 16 return -16.396302667402463 value_loss 0.02482200321388507 entropy 1.5299966377903071\n",
      "Valid run -18.0\n",
      "# plays 17 return -16.256672400662218 value_loss 0.02353913989438651 entropy 1.5500594086804562\n",
      "Valid run -7.0\n",
      "# plays 18 return -16.531005160595996 value_loss 0.022401924622273785 entropy 1.5371611686220548\n",
      "Valid run -10.0\n",
      "# plays 19 return -16.577904644536396 value_loss 0.021801529884829828 entropy 1.5067652103682265\n",
      "Valid run -15.0\n",
      "# plays 20 return -16.320114180082758 value_loss 0.021009244786645214 entropy 1.4771236467389275\n",
      "Valid run -14.0\n",
      "# plays 21 return -16.08810276207448 value_loss 0.020609180942426836 entropy 1.4268965646131724\n",
      "Valid run -17.0\n",
      "# plays 22 return -15.479292485867033 value_loss 0.02033703549079661 entropy 1.3791228574952459\n",
      "Valid run -9.0\n",
      "# plays 23 return -15.23136323728033 value_loss 0.020385899603717606 entropy 1.3444042729367356\n",
      "Valid run -17.0\n",
      "# plays 24 return -15.108226913552299 value_loss 0.02004972912551738 entropy 1.309447539486668\n",
      "Valid run -14.0\n",
      "# plays 25 return -14.89740422219707 value_loss 0.0204023115800182 entropy 1.3488660481219488\n",
      "Valid run -20.0\n",
      "# plays 26 return -14.307663799977362 value_loss 0.020997778628739118 entropy 1.3478991981474615\n",
      "Valid run -13.0\n",
      "# plays 27 return -14.176897419979626 value_loss 0.021884144481544854 entropy 1.3356702158372677\n",
      "Valid run -16.0\n",
      "# plays 28 return -14.159207677981664 value_loss 0.022320384617023313 entropy 1.312693091221609\n",
      "Valid run -10.0\n",
      "# plays 29 return -13.843286910183497 value_loss 0.022641922572588305 entropy 1.2929965164167705\n",
      "Valid run -17.0\n",
      "# plays 30 return -13.858958219165148 value_loss 0.023382485953869266 entropy 1.2627138341068758\n",
      "Valid run -16.0\n",
      "# plays 31 return -13.473062397248633 value_loss 0.02389645316264198 entropy 1.2065493038358117\n",
      "Valid run -9.0\n",
      "# plays 32 return -13.62575615752377 value_loss 0.024321277146588258 entropy 1.2073021125438967\n",
      "Valid run -12.0\n",
      "# plays 33 return -13.263180541771392 value_loss 0.024884556081936203 entropy 1.179931691194279\n",
      "Valid run -8.0\n",
      "# plays 34 return -12.736862487594253 value_loss 0.025195759509842493 entropy 1.1716287332001376\n",
      "Valid run -6.0\n",
      "# plays 35 return -12.763176238834829 value_loss 0.02514502061144194 entropy 1.1674121487716143\n",
      "Valid run -13.0\n",
      "# plays 36 return -12.286858614951347 value_loss 0.02523397418322278 entropy 1.1408344313624408\n",
      "Valid run -10.0\n",
      "# plays 37 return -12.058172753456212 value_loss 0.02599664974923831 entropy 1.1201403521064626\n",
      "Valid run -10.0\n",
      "# plays 38 return -11.55235547811059 value_loss 0.026881217498027403 entropy 1.1235301299622906\n",
      "Valid run -14.0\n",
      "# plays 39 return -11.597119930299531 value_loss 0.027863363508978296 entropy 1.0684598523982582\n",
      "Valid run -6.0\n",
      "# plays 40 return -11.037407937269577 value_loss 0.02815310068541659 entropy 1.077716538492195\n",
      "Valid run -11.0\n",
      "# plays 41 return -11.13366714354262 value_loss 0.028316041932528253 entropy 1.052599407566671\n",
      "Valid run -20.0\n",
      "# plays 42 return -10.820300429188359 value_loss 0.028760215883873435 entropy 1.0066554306613937\n",
      "Valid run -14.0\n",
      "# plays 43 return -10.838270386269523 value_loss 0.02903816997595656 entropy 0.9719609396844477\n",
      "Valid run -13.0\n",
      "# plays 44 return -10.554443347642572 value_loss 0.029389300789726035 entropy 0.9897369834174323\n",
      "Valid run -6.0\n",
      "# plays 45 return -10.698999012878314 value_loss 0.02922170412953996 entropy 0.9817075446400663\n",
      "Valid run -5.0\n",
      "# plays 46 return -10.029099111590483 value_loss 0.029027732557800143 entropy 1.004286528818361\n",
      "Valid run -3.0\n",
      "# plays 47 return -9.826189200431436 value_loss 0.029430030149293004 entropy 1.020217634414902\n",
      "Valid run -9.0\n",
      "# plays 48 return -9.943570280388293 value_loss 0.029719965558116488 entropy 1.0186663861724063\n",
      "Valid run -7.0\n",
      "# plays 49 return -10.049213252349464 value_loss 0.029721716069458295 entropy 1.0137691969497002\n",
      "Valid run -8.0\n",
      "# plays 50 return -9.644291927114518 value_loss 0.02923021098817441 entropy 1.004925476227993\n"
     ]
    }
   ],
   "source": [
    "n_iter = 10000 #1000\n",
    "init_collect = 10\n",
    "n_collect = 1 #100\n",
    "n_value = 100 #100\n",
    "n_policy = 100 #10\n",
    "disp_iter = 1\n",
    "val_iter = 1\n",
    "\n",
    "max_len = 1000\n",
    "batch_size = 500\n",
    "\n",
    "ent_coeff = 0. #0.001\n",
    "discount_factor = .95\n",
    "\n",
    "value_loss = -numpy.Inf\n",
    "ret = -numpy.Inf\n",
    "entropy = -numpy.Inf\n",
    "\n",
    "return_history = []\n",
    "\n",
    "for ni in range(n_iter):\n",
    "    player.eval()\n",
    "\n",
    "    if numpy.mod(ni, val_iter) == 0:\n",
    "        _, _, _, _, ret_ = collect_one_episode(env, player, max_len=max_len, deterministic=True)\n",
    "        print('Valid run', ret_)\n",
    "        return_history.append(ret_)\n",
    "\n",
    "    nc = n_collect\n",
    "    if ni == 0:\n",
    "        nc = init_collect\n",
    "    for ci in range(nc):\n",
    "        o_, c_, a_, ap_, ret_ = collect_one_episode(env, player, max_len=max_len, discount_factor=discount_factor)\n",
    "        replay_buffer.add(o_, c_, a_, ap_)\n",
    "        if ret == -numpy.Inf:\n",
    "            ret = ret_\n",
    "        else:\n",
    "            ret = 0.9 * ret + 0.1 * ret_\n",
    "    \n",
    "    player.train()\n",
    "        \n",
    "    # fit a value function\n",
    "    for vi in range(n_value):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        batch_x = torch.from_numpy(numpy.stack([ex[0] for ex in batch]).astype('float32'))\n",
    "        batch_y = torch.from_numpy(numpy.stack([ex[1] for ex in batch]).astype('float32'))\n",
    "        pred_y = value(batch_x).squeeze()\n",
    "        loss_ = ((batch_y - pred_y) ** 2)\n",
    "        \n",
    "        batch_a = torch.from_numpy(numpy.stack([ex[2] for ex in batch]).astype('float32')[:,None])\n",
    "        batch_pi = player(batch_x, normalized=True)\n",
    "        batch_q = torch.from_numpy(numpy.stack([ex[3] for ex in batch]).astype('float32'))\n",
    "        logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "\n",
    "        iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "    \n",
    "        loss = iw * loss_\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_value.step()\n",
    "        \n",
    "    if value_loss < 0.:\n",
    "        value_loss = loss_.mean().item()\n",
    "    else:\n",
    "        value_loss = 0.9 * value_loss + 0.1 * loss_.mean().item()\n",
    "    \n",
    "    if numpy.mod(ni, disp_iter) == 0:\n",
    "        print('# plays', (ni+1) * n_collect, 'return', ret, 'value_loss', value_loss, 'entropy', -entropy)\n",
    "    \n",
    "    # fit a policy\n",
    "    for pi in range(n_policy):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        batch_x = torch.from_numpy(numpy.stack([ex[0] for ex in batch]).astype('float32'))\n",
    "        batch_r = torch.from_numpy(numpy.stack([ex[1] for ex in batch]).astype('float32')[:,None])\n",
    "        batch_v = value(batch_x)\n",
    "        batch_a = torch.from_numpy(numpy.stack([ex[2] for ex in batch]).astype('float32')[:,None])\n",
    "        batch_q = torch.from_numpy(numpy.stack([ex[3] for ex in batch]).astype('float32'))\n",
    "\n",
    "        batch_pi = player(batch_x, normalized=True)\n",
    "        \n",
    "        logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "        \n",
    "        adv = batch_r - batch_v.clone().detach()\n",
    "        \n",
    "        loss = -(adv * logp)\n",
    "        \n",
    "        iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "    \n",
    "        loss = iw * loss\n",
    "        \n",
    "        ent = (batch_pi * torch.log(batch_pi)).sum(1)\n",
    "        \n",
    "        if entropy == -numpy.Inf:\n",
    "            entropy = ent.mean().item()\n",
    "        else:\n",
    "            entropy = 0.9 * entropy + 0.1 * ent.mean().item()\n",
    "        \n",
    "        loss = (loss + ent_coeff * ent).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_player.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.3796)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
