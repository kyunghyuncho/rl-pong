{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResLinear(nn.Module):\n",
    "    def __init__(self, n_in, n_out, act=nn.ReLU()):\n",
    "        super(ResLinear, self).__init__()\n",
    "        self.act = act\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "        self.bn = nn.BatchNorm1d(n_out)\n",
    "        \n",
    "        assert(n_in == n_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.act(self.bn(self.linear(x)))\n",
    "        return h + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(nn.Module):\n",
    "    def __init__(self, n_in=128, n_hid=100, n_out=6):\n",
    "        super(Player, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid),\n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, n_out))\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, obs, normalized=False):\n",
    "        if normalized:\n",
    "            return self.softmax(self.layers(obs))\n",
    "        else:\n",
    "            return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(nn.Module):\n",
    "    def __init__(self, n_in=128, n_hid=100):\n",
    "        super(Value, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid),\n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, 1))\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_params(from_, to_):\n",
    "    for f_, t_ in zip(from_.parameters(), to_.parameters()):\n",
    "        t_.data.copy_(f_.data)\n",
    "        \n",
    "def avg_params(from_, to_, coeff=0.95):\n",
    "    for f_, t_ in zip(from_.parameters(), to_.parameters()):\n",
    "        t_.data.copy_(coeff * t_.data + (1.-coeff) * f_.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_obs(obs):\n",
    "    return obs.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data\n",
    "def collect_one_episode(env, player, max_len=50, discount_factor=0.9, deterministic=False, rendering=False, verbose=False):\n",
    "    episode = []\n",
    "\n",
    "    observations = []\n",
    "\n",
    "    rewards = []\n",
    "    crewards = []\n",
    "\n",
    "    actions = []\n",
    "    action_probs = []\n",
    "\n",
    "    obs = env.reset()\n",
    "    \n",
    "    for ml in range(max_len):\n",
    "        if rendering:\n",
    "            env.render()\n",
    "            sleep(0.05)\n",
    "            \n",
    "        obs = normalize_obs(obs)\n",
    "\n",
    "        out_probs = player(torch.from_numpy(obs[None,:]), normalized=True).squeeze()\n",
    "        \n",
    "        if deterministic:\n",
    "            action = numpy.argmax(out_probs.data.numpy())\n",
    "            if verbose:\n",
    "                print(out_probs, action)\n",
    "        else:\n",
    "            act_dist = Categorical(out_probs)\n",
    "            action = act_dist.sample().item()\n",
    "        action_prob = out_probs[action].item()\n",
    "\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        action_probs.append(action_prob)\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if deterministic and verbose:\n",
    "            print(reward, done)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "\n",
    "    rewards = numpy.array(rewards)\n",
    "\n",
    "    # it's probably not the best idea to compute the discounted cumulative returns here, but well..\n",
    "    for ri in range(len(rewards)):\n",
    "        factors = (discount_factor ** numpy.arange(len(rewards)-ri))\n",
    "        crewards.append(numpy.sum(rewards[ri:] * factors))\n",
    "        \n",
    "    # discard the final 10%, because it really doesn't give me a good signal due to the unbounded horizon\n",
    "    # this is only for training, not for computing the total return of the episode of the given length\n",
    "    discard = max_len // 10\n",
    "        \n",
    "    return observations[:-discard], crewards[:-discard], actions[:-discard], action_probs[:-discard], rewards.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple implementation of FIFO-based replay buffer\n",
    "class Buffer:\n",
    "    def __init__(self, max_items=10000):\n",
    "        self.max_items = max_items\n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, observations, crewards, actions, action_probs):\n",
    "        new_n = len(observations)\n",
    "        old_n = len(self.buffer)\n",
    "        if new_n + old_n > self.max_items:\n",
    "            del self.buffer[:new_n]\n",
    "        for o, c, a, p in zip(observations, crewards, actions, action_probs):\n",
    "            self.buffer.append((o, c, a, p))\n",
    "            \n",
    "    def sample(self, n=100):\n",
    "        idxs = numpy.random.choice(len(self.buffer),n)\n",
    "        return [self.buffer[ii] for ii in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a policy\n",
    "player = Player(n_in=128, n_hid=32, n_out=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a value estimator\n",
    "value = Value(n_in=128, n_hid=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimizers\n",
    "opt_player = Adam(player.parameters(), lr=0.0001)\n",
    "opt_value = Adam(value.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize replay buffer\n",
    "replay_buffer = Buffer(max_items=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyunghyuncho/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid run -20.0\n",
      "# plays 1 return -16.542701650999998 value_loss 0.26948466897010803 entropy inf\n",
      "Valid run -20.0\n",
      "# plays 2 return -16.8884314859 value_loss 0.26769939363002776 entropy 1.7384350076243655\n",
      "Valid run -20.0\n",
      "# plays 3 return -16.59958833731 value_loss 0.26605464190244676 entropy 1.7384942054500834\n",
      "Valid run -20.0\n",
      "# plays 4 return -16.539629503579 value_loss 0.2576955691277981 entropy 1.738444260076567\n",
      "Valid run -20.0\n",
      "# plays 5 return -16.785666553221102 value_loss 0.2503564210504294 entropy 1.7377413517327507\n",
      "Valid run -20.0\n",
      "# plays 6 return -16.707099897898992 value_loss 0.24135280267328027 entropy 1.7375544968376355\n",
      "Valid run -20.0\n",
      "# plays 7 return -16.836389908109094 value_loss 0.23014565511879329 entropy 1.7375649876272936\n",
      "Valid run -20.0\n",
      "# plays 8 return -16.852750917298184 value_loss 0.22176235649887027 entropy 1.737181644152176\n",
      "Valid run -20.0\n",
      "# plays 9 return -16.867475825568366 value_loss 0.21341569632801385 entropy 1.7371947061278707\n",
      "Valid run -20.0\n",
      "# plays 10 return -16.38072824301153 value_loss 0.20369676420551217 entropy 1.737248392830969\n",
      "Valid run -20.0\n",
      "# plays 11 return -15.942655418710377 value_loss 0.19469143974299738 entropy 1.737749058250515\n",
      "Valid run -20.0\n",
      "# plays 12 return -16.04838987683934 value_loss 0.18514278928204336 entropy 1.738286349313358\n",
      "Valid run -20.0\n",
      "# plays 13 return -16.143550889155406 value_loss 0.17555756181267462 entropy 1.7387711017186747\n",
      "Valid run -20.0\n",
      "# plays 14 return -16.52919580023987 value_loss 0.16429491543579902 entropy 1.7390076146331142\n",
      "Valid run -20.0\n",
      "# plays 15 return -16.676276220215883 value_loss 0.15583875212498433 entropy 1.7391100623228355\n",
      "Valid run -20.0\n",
      "# plays 16 return -16.408648598194294 value_loss 0.1489533954867237 entropy 1.739626176057665\n",
      "Valid run -20.0\n",
      "# plays 17 return -16.367783738374865 value_loss 0.1411622270055276 entropy 1.7399514680458856\n",
      "Valid run -20.0\n",
      "# plays 18 return -16.431005364537377 value_loss 0.13381033203968362 entropy 1.7403390337753424\n",
      "Valid run -20.0\n",
      "# plays 19 return -16.58790482808364 value_loss 0.12592178415032287 entropy 1.7403838810916767\n",
      "Valid run -20.0\n",
      "# plays 20 return -16.329114345275276 value_loss 0.1199339903662091 entropy 1.7403315697194361\n",
      "Valid run -20.0\n",
      "# plays 21 return -16.19620291074775 value_loss 0.11299814341391494 entropy 1.7404663561052245\n",
      "Valid run -20.0\n",
      "# plays 22 return -16.076582619672976 value_loss 0.10687251627080474 entropy 1.7402515806914833\n",
      "Valid run -20.0\n",
      "# plays 23 return -15.96892435770568 value_loss 0.10128752877980025 entropy 1.7403132706076363\n",
      "Valid run -20.0\n",
      "# plays 24 return -15.97203192193511 value_loss 0.0958715158611794 entropy 1.7404761020963109\n",
      "Valid run -20.0\n",
      "# plays 25 return -16.1748287297416 value_loss 0.09117129210325417 entropy 1.7405492422115714\n",
      "Valid run -21.0\n",
      "# plays 26 return -16.05734585676744 value_loss 0.08648489427887503 entropy 1.740489708143652\n",
      "Valid run -20.0\n",
      "# plays 27 return -16.051611271090696 value_loss 0.08170228313496504 entropy 1.739950819613174\n",
      "Valid run -20.0\n",
      "# plays 28 return -16.346450143981627 value_loss 0.07810096989057178 entropy 1.7396446818224356\n",
      "Valid run -20.0\n",
      "# plays 29 return -16.211805129583468 value_loss 0.07434584282322271 entropy 1.7397902328800157\n",
      "Valid run -20.0\n",
      "# plays 30 return -15.890624616625121 value_loss 0.07124467156067678 entropy 1.7398937112958797\n",
      "Valid run -20.0\n",
      "# plays 31 return -16.00156215496261 value_loss 0.06780435334207285 entropy 1.739608328670334\n",
      "Valid run -20.0\n",
      "# plays 32 return -15.801405939466349 value_loss 0.06505963637436929 entropy 1.7394834186223016\n",
      "Valid run -20.0\n",
      "# plays 33 return -15.921265345519714 value_loss 0.0627572456062332 entropy 1.7400098632547354\n",
      "Valid run -20.0\n",
      "# plays 34 return -15.429138810967743 value_loss 0.0606154648864712 entropy 1.7405092639259123\n",
      "Valid run -20.0\n",
      "# plays 35 return -15.786224929870968 value_loss 0.05829454869667701 entropy 1.7407761123215788\n",
      "Valid run -20.0\n",
      "# plays 36 return -15.307602436883872 value_loss 0.05593913229382049 entropy 1.7405687716463234\n",
      "Valid run -20.0\n",
      "# plays 37 return -15.376842193195484 value_loss 0.05495396158465157 entropy 1.7404889019759335\n",
      "Valid run -20.0\n",
      "# plays 38 return -15.639157973875937 value_loss 0.054604029146150444 entropy 1.7406913704003464\n",
      "Valid run -20.0\n",
      "# plays 39 return -15.575242176488343 value_loss 0.05428655414639326 entropy 1.7409729097678233\n",
      "Valid run -21.0\n",
      "# plays 40 return -15.717717958839508 value_loss 0.053217881639525996 entropy 1.740787503499274\n",
      "Valid run -20.0\n",
      "# plays 41 return -15.045946162955557 value_loss 0.051538719209783417 entropy 1.7406147944390464\n",
      "Valid run -20.0\n",
      "# plays 42 return -15.541351546660001 value_loss 0.0506135878468333 entropy 1.740635487738366\n",
      "Valid run -20.0\n",
      "# plays 43 return -15.987216391994002 value_loss 0.04902786716861531 entropy 1.7406046640047488\n",
      "Valid run -20.0\n",
      "# plays 44 return -15.688494752794602 value_loss 0.04749907587507074 entropy 1.7404270995357545\n",
      "Valid run -20.0\n",
      "# plays 45 return -15.619645277515142 value_loss 0.047571460466432916 entropy 1.7403044244051458\n",
      "Valid run -20.0\n",
      "# plays 46 return -15.657680749763628 value_loss 0.04687071299032719 entropy 1.7400071325617552\n",
      "Valid run -20.0\n",
      "# plays 47 return -15.291912674787266 value_loss 0.04696184803971366 entropy 1.7396659684878673\n",
      "Valid run -20.0\n",
      "# plays 48 return -15.36272140730854 value_loss 0.04599943080480739 entropy 1.7394517690916722\n",
      "Valid run -20.0\n",
      "# plays 49 return -15.726449266577687 value_loss 0.045627264922283695 entropy 1.7393394593707752\n",
      "Valid run -20.0\n",
      "# plays 50 return -15.45380433991992 value_loss 0.04509143307395811 entropy 1.7394544563742778\n",
      "Valid run -20.0\n",
      "# plays 51 return -15.808423905927928 value_loss 0.04506757542122634 entropy 1.7394071364083987\n",
      "Valid run -20.0\n",
      "# plays 52 return -15.627581515335136 value_loss 0.04495895297493844 entropy 1.7394802767982802\n",
      "Valid run -20.0\n",
      "# plays 53 return -15.364823363801623 value_loss 0.044897384713586945 entropy 1.7395225144852908\n",
      "Valid run -20.0\n",
      "# plays 54 return -14.82834102742146 value_loss 0.04528241919323514 entropy 1.739496179106463\n",
      "Valid run -20.0\n",
      "# plays 55 return -14.945506924679314 value_loss 0.04468422740857044 entropy 1.7393187493220297\n",
      "Valid run -20.0\n",
      "# plays 56 return -15.050956232211384 value_loss 0.04358991855526185 entropy 1.739050157767789\n",
      "Valid run -20.0\n",
      "# plays 57 return -14.945860608990246 value_loss 0.043221219621397994 entropy 1.7389013800534507\n",
      "Valid run -20.0\n",
      "# plays 58 return -15.251274548091223 value_loss 0.043020007081894227 entropy 1.738914046076406\n",
      "Valid run -20.0\n",
      "# plays 59 return -15.126147093282102 value_loss 0.042950404794277086 entropy 1.739509768871602\n",
      "Valid run -20.0\n",
      "# plays 60 return -15.313532383953891 value_loss 0.04228496829682112 entropy 1.7399330369110222\n",
      "Valid run -20.0\n",
      "# plays 61 return -15.482179145558504 value_loss 0.04153085340571571 entropy 1.7405185833595271\n",
      "Valid run -20.0\n",
      "# plays 62 return -15.733961231002654 value_loss 0.040323427374769766 entropy 1.7409098101842406\n",
      "Valid run -20.0\n",
      "# plays 63 return -15.360565107902389 value_loss 0.03990779278183834 entropy 1.7413066401455561\n",
      "Valid run -20.0\n",
      "# plays 64 return -15.82450859711215 value_loss 0.03924225419019035 entropy 1.7416225135489842\n",
      "Valid run -20.0\n",
      "# plays 65 return -15.742057737400936 value_loss 0.038234887861499534 entropy 1.7413046934131713\n",
      "Valid run -20.0\n",
      "# plays 66 return -15.267851963660842 value_loss 0.037694331639965786 entropy 1.741369135518239\n",
      "Valid run -20.0\n",
      "# plays 67 return -14.741066767294758 value_loss 0.03740565513130464 entropy 1.7416323315096076\n",
      "Valid run -20.0\n",
      "# plays 68 return -15.166960090565283 value_loss 0.037382875319329836 entropy 1.7417361624915633\n",
      "Valid run -20.0\n",
      "# plays 69 return -14.750264081508755 value_loss 0.03671523300797457 entropy 1.7416084673924341\n",
      "Valid run -20.0\n",
      "# plays 70 return -15.075237673357881 value_loss 0.03643177145413203 entropy 1.741638824256869\n",
      "Valid run -20.0\n",
      "# plays 71 return -15.167713906022092 value_loss 0.03639718914352337 entropy 1.7412499747162171\n",
      "Valid run -20.0\n",
      "# plays 72 return -15.450942515419884 value_loss 0.03594623416056466 entropy 1.7411402624981838\n",
      "Valid run -19.0\n",
      "# plays 73 return -15.305848263877897 value_loss 0.03602033511749117 entropy 1.7410034637731284\n",
      "Valid run -18.0\n",
      "# plays 74 return -15.275263437490107 value_loss 0.03550279168144949 entropy 1.7405192699575587\n",
      "Valid run -20.0\n",
      "# plays 75 return -14.647737093741098 value_loss 0.03558557691743453 entropy 1.7405800949440362\n",
      "Valid run -20.0\n",
      "# plays 76 return -14.98296338436699 value_loss 0.03563017848942113 entropy 1.7404920980828864\n",
      "Valid run -20.0\n",
      "# plays 77 return -14.884667045930291 value_loss 0.03622579905819981 entropy 1.740035064106472\n",
      "Valid run -20.0\n",
      "# plays 78 return -14.596200341337262 value_loss 0.03546974659054358 entropy 1.73966232957366\n",
      "Valid run -20.0\n",
      "# plays 79 return -14.136580307203536 value_loss 0.03602137506087613 entropy 1.7393436367156117\n",
      "Valid run -20.0\n",
      "# plays 80 return -14.522922276483182 value_loss 0.035965704233169485 entropy 1.7388530402389202\n",
      "Valid run -20.0\n",
      "# plays 81 return -14.370630048834865 value_loss 0.036283290738511026 entropy 1.7386516427507093\n",
      "Valid run -20.0\n",
      "# plays 82 return -14.83356704395138 value_loss 0.03619751306293721 entropy 1.7386794843091429\n",
      "Valid run -20.0\n",
      "# plays 83 return -15.250210339556242 value_loss 0.03660277252124234 entropy 1.738579815130019\n",
      "Valid run -20.0\n",
      "# plays 84 return -14.725189305600617 value_loss 0.03633856042975024 entropy 1.738084322307213\n",
      "Valid run -20.0\n",
      "# plays 85 return -15.052670375040556 value_loss 0.036117832122032366 entropy 1.7378943998992695\n",
      "Valid run -20.0\n",
      "# plays 86 return -15.2474033375365 value_loss 0.03622131333451079 entropy 1.7375772422115454\n",
      "Valid run -20.0\n",
      "# plays 87 return -15.422663003782851 value_loss 0.03604638049632854 entropy 1.7376376597572751\n",
      "Valid run -20.0\n",
      "# plays 88 return -14.780396703404566 value_loss 0.03634323115316522 entropy 1.7378101278990556\n",
      "Valid run -20.0\n",
      "# plays 89 return -15.20235703306411 value_loss 0.035953450740881195 entropy 1.7374069248911852\n",
      "Valid run -20.0\n",
      "# plays 90 return -15.1821213297577 value_loss 0.03567971643046209 entropy 1.7371164916552706\n",
      "Valid run -20.0\n",
      "# plays 91 return -15.063909196781932 value_loss 0.03562221957357826 entropy 1.736999369295065\n",
      "Valid run -20.0\n",
      "# plays 92 return -15.257518277103738 value_loss 0.03469868164311739 entropy 1.7369313722187791\n",
      "Valid run -20.0\n",
      "# plays 93 return -15.531766449393366 value_loss 0.034302809784271346 entropy 1.7369904613156757\n",
      "Valid run -20.0\n",
      "# plays 94 return -15.37858980445403 value_loss 0.03389461784761372 entropy 1.7368144888723087\n",
      "Valid run -20.0\n",
      "# plays 95 return -15.340730824008627 value_loss 0.03364582351626059 entropy 1.7365912721221939\n",
      "Valid run -21.0\n",
      "# plays 96 return -15.306657741607765 value_loss 0.033304072470496 entropy 1.7363907997166352\n",
      "Valid run -20.0\n",
      "# plays 97 return -15.17599196744699 value_loss 0.033180582866993456 entropy 1.736359582979181\n",
      "Valid run -20.0\n",
      "# plays 98 return -15.15839277070229 value_loss 0.03279799317204378 entropy 1.736155869938772\n",
      "Valid run -20.0\n",
      "# plays 99 return -15.042553493632061 value_loss 0.03233190189197739 entropy 1.7366654882420907\n",
      "Valid run -20.0\n",
      "# plays 100 return -15.038298144268856 value_loss 0.03184467365945948 entropy 1.736717996576193\n",
      "Valid run -20.0\n",
      "# plays 101 return -14.534468329841971 value_loss 0.03219997527246689 entropy 1.7366641490907826\n",
      "Valid run -20.0\n",
      "# plays 102 return -14.781021496857775 value_loss 0.031964345615014565 entropy 1.7368117457640408\n",
      "Valid run -20.0\n",
      "# plays 103 return -15.002919347172 value_loss 0.031822053238068644 entropy 1.7369739754344644\n",
      "Valid run -20.0\n",
      "# plays 104 return -15.202627412454799 value_loss 0.031435054855926 entropy 1.7370143767662627\n",
      "Valid run -20.0\n",
      "# plays 105 return -15.48236467120932 value_loss 0.031333650299057214 entropy 1.7369365577719633\n",
      "Valid run -20.0\n",
      "# plays 106 return -15.134128204088388 value_loss 0.03187598450316029 entropy 1.737089910405639\n",
      "Valid run -20.0\n",
      "# plays 107 return -15.02071538367955 value_loss 0.031872916191813684 entropy 1.7370636241222408\n",
      "Valid run -20.0\n",
      "# plays 108 return -15.018643845311594 value_loss 0.03177605238854647 entropy 1.737046851866346\n",
      "Valid run -20.0\n",
      "# plays 109 return -15.216779460780437 value_loss 0.03196768798500085 entropy 1.736360153826482\n",
      "Valid run -20.0\n",
      "# plays 110 return -15.295101514702393 value_loss 0.03175875753673718 entropy 1.736277599209064\n",
      "Valid run -20.0\n",
      "# plays 111 return -14.965591363232154 value_loss 0.0322212465302301 entropy 1.7361270489947804\n",
      "Valid run -20.0\n",
      "# plays 112 return -15.269032226908939 value_loss 0.032425069826497364 entropy 1.735917192331198\n",
      "Valid run -21.0\n",
      "# plays 113 return -15.142129004218045 value_loss 0.03266570293991601 entropy 1.7353477850481378\n",
      "Valid run -20.0\n",
      "# plays 114 return -15.327916103796241 value_loss 0.03199491790612491 entropy 1.73551008225811\n",
      "Valid run -20.0\n",
      "# plays 115 return -15.495124493416618 value_loss 0.031208778694390822 entropy 1.7350952537506485\n",
      "Valid run -20.0\n",
      "# plays 116 return -15.145612044074959 value_loss 0.0308789194227334 entropy 1.7350264180765271\n",
      "Valid run -17.0\n",
      "# plays 117 return -14.331050839667462 value_loss 0.030817960889733688 entropy 1.7344608774088106\n",
      "Valid run -20.0\n",
      "# plays 118 return -14.297945755700717 value_loss 0.03142096640908437 entropy 1.7336853235174041\n",
      "Valid run -20.0\n",
      "# plays 119 return -14.068151180130645 value_loss 0.03143415377059922 entropy 1.7335301821155862\n",
      "Valid run -19.0\n",
      "# plays 120 return -13.86133606211758 value_loss 0.03112656515555989 entropy 1.7327630526691655\n",
      "Valid run -20.0\n",
      "# plays 121 return -13.975202455905823 value_loss 0.031430635005689364 entropy 1.7320268387251567\n",
      "Valid run -20.0\n",
      "# plays 122 return -13.87768221031524 value_loss 0.0315545832965881 entropy 1.7317804284875784\n",
      "Valid run -20.0\n",
      "# plays 123 return -13.789913989283718 value_loss 0.0323719421792725 entropy 1.7312532376774459\n",
      "Valid run -20.0\n",
      "# plays 124 return -13.510922590355346 value_loss 0.032768654653611715 entropy 1.7308209462416255\n",
      "Valid run -20.0\n",
      "# plays 125 return -13.159830331319812 value_loss 0.033514439608916285 entropy 1.7304028229129949\n",
      "Valid run -17.0\n",
      "# plays 126 return -13.44384729818783 value_loss 0.033898469772575476 entropy 1.7296746538404646\n",
      "Valid run -19.0\n",
      "# plays 127 return -13.699462568369047 value_loss 0.034386853396867076 entropy 1.7292975083221693\n",
      "Valid run -20.0\n",
      "# plays 128 return -13.729516311532143 value_loss 0.03519711943266483 entropy 1.7288895737860444\n",
      "Valid run -20.0\n",
      "# plays 129 return -13.456564680378929 value_loss 0.035789085270145765 entropy 1.7287273414585829\n",
      "Valid run -18.0\n",
      "# plays 130 return -13.610908212341036 value_loss 0.03610358864770463 entropy 1.728320397653387\n",
      "Valid run -19.0\n",
      "# plays 131 return -13.349817391106932 value_loss 0.036583177200312594 entropy 1.7284071428598675\n",
      "Valid run -20.0\n",
      "# plays 132 return -13.214835651996239 value_loss 0.0370800751012759 entropy 1.728433217076673\n",
      "Valid run -16.0\n",
      "# plays 133 return -13.593352086796617 value_loss 0.03671567209621041 entropy 1.7280326034917148\n",
      "Valid run -19.0\n",
      "# plays 134 return -13.434016878116957 value_loss 0.03624205426627191 entropy 1.7274900451375412\n",
      "Valid run -19.0\n",
      "# plays 135 return -13.29061519030526 value_loss 0.03699566806483679 entropy 1.7268963424206514\n",
      "Valid run -20.0\n",
      "# plays 136 return -13.161553671274735 value_loss 0.03746906900520694 entropy 1.7266316055351874\n",
      "Valid run -17.0\n",
      "# plays 137 return -13.345398304147261 value_loss 0.03936696261866902 entropy 1.7261780570833114\n",
      "Valid run -20.0\n",
      "# plays 138 return -13.910858473732535 value_loss 0.03974929752085222 entropy 1.7262211714437863\n",
      "Valid run -20.0\n",
      "# plays 139 return -13.919772626359283 value_loss 0.03926305668473494 entropy 1.726106588917713\n",
      "Valid run -20.0\n",
      "# plays 140 return -13.527795363723355 value_loss 0.039845933290603035 entropy 1.7261660259696086\n",
      "Valid run -20.0\n",
      "# plays 141 return -13.57501582735102 value_loss 0.03962623829151595 entropy 1.7259529285792923\n",
      "Valid run -20.0\n",
      "# plays 142 return -13.41751424461592 value_loss 0.039142433966744584 entropy 1.7258175089430772\n",
      "Valid run -20.0\n",
      "# plays 143 return -13.975762820154328 value_loss 0.039343767612172946 entropy 1.725742387532362\n",
      "Valid run -20.0\n",
      "# plays 144 return -14.078186538138896 value_loss 0.03872646383851211 entropy 1.725821288104063\n",
      "Valid run -20.0\n",
      "# plays 145 return -13.970367884325007 value_loss 0.03812851414748526 entropy 1.7262927105498849\n",
      "Valid run -20.0\n",
      "# plays 146 return -13.773331095892509 value_loss 0.037033778889493614 entropy 1.7263500475546147\n",
      "Valid run -20.0\n",
      "# plays 147 return -13.79599798630326 value_loss 0.03672342893199605 entropy 1.726285744039782\n",
      "Valid run -20.0\n",
      "# plays 148 return -13.816398187672934 value_loss 0.03604410626783802 entropy 1.7258959549652368\n",
      "Valid run -20.0\n",
      "# plays 149 return -13.53475836890564 value_loss 0.036172665625466414 entropy 1.7257983530549856\n",
      "Valid run -20.0\n",
      "# plays 150 return -13.381282532015078 value_loss 0.03665005077688709 entropy 1.7248127896735306\n",
      "Valid run -20.0\n",
      "# plays 151 return -13.24315427881357 value_loss 0.03592536312880053 entropy 1.7245730803678285\n",
      "Valid run -20.0\n",
      "# plays 152 return -13.518838850932212 value_loss 0.03609687354453147 entropy 1.7244360563979357\n",
      "Valid run -20.0\n",
      "# plays 153 return -13.066954965838992 value_loss 0.03630293137986428 entropy 1.7243008279635499\n",
      "Valid run -20.0\n",
      "# plays 154 return -12.660259469255093 value_loss 0.03775965364717514 entropy 1.7238865032732686\n",
      "Valid run -20.0\n",
      "# plays 155 return -12.994233522329584 value_loss 0.037978166940299304 entropy 1.7238058294498362\n",
      "Valid run -20.0\n",
      "# plays 156 return -12.694810170096625 value_loss 0.03831148782594887 entropy 1.7233090765829961\n",
      "Valid run -20.0\n",
      "# plays 157 return -12.425329153086963 value_loss 0.038566662870859095 entropy 1.7227193007248607\n",
      "Valid run -20.0\n",
      "# plays 158 return -12.582796237778268 value_loss 0.03882343866200595 entropy 1.722900427508967\n",
      "Valid run -20.0\n",
      "# plays 159 return -12.324516614000443 value_loss 0.03848402058575764 entropy 1.7226858210785116\n",
      "Valid run -20.0\n",
      "# plays 160 return -12.5920649526004 value_loss 0.038804656092770826 entropy 1.7223013874626967\n",
      "Valid run -20.0\n",
      "# plays 161 return -12.53285845734036 value_loss 0.03853825292686933 entropy 1.721905559776232\n",
      "Valid run -20.0\n",
      "# plays 162 return -12.979572611606326 value_loss 0.03880343343347591 entropy 1.7217044594926754\n",
      "Valid run -20.0\n",
      "# plays 163 return -12.681615350445693 value_loss 0.039114917864176174 entropy 1.721354282078063\n",
      "Valid run -20.0\n",
      "# plays 164 return -12.813453815401123 value_loss 0.03908022877553631 entropy 1.7203908120788345\n",
      "Valid run -20.0\n",
      "# plays 165 return -12.932108433861012 value_loss 0.03933425966184911 entropy 1.71944232692142\n",
      "Valid run -20.0\n",
      "# plays 166 return -12.838897590474911 value_loss 0.03886421671463469 entropy 1.718298874185083\n",
      "Valid run -20.0\n",
      "# plays 167 return -12.955007831427421 value_loss 0.03879000679510909 entropy 1.7179019339729629\n",
      "Valid run -20.0\n",
      "# plays 168 return -12.85950704828468 value_loss 0.03850090632609795 entropy 1.7179048787754616\n",
      "Valid run -20.0\n",
      "# plays 169 return -12.873556343456213 value_loss 0.03806108575855736 entropy 1.7175855917339324\n",
      "Valid run -20.0\n",
      "# plays 170 return -12.886200709110593 value_loss 0.037940969021276034 entropy 1.7170808924265688\n",
      "Valid run -20.0\n",
      "# plays 171 return -13.297580638199534 value_loss 0.037424736426313575 entropy 1.7168912016952942\n",
      "Valid run -18.0\n",
      "# plays 172 return -12.86782257437958 value_loss 0.036885212248003355 entropy 1.71631668476922\n",
      "Valid run -20.0\n",
      "# plays 173 return -13.481040316941623 value_loss 0.03618691225537885 entropy 1.716093307318647\n",
      "Valid run -20.0\n",
      "# plays 174 return -13.432936285247463 value_loss 0.0359801466139188 entropy 1.7157157532258456\n",
      "Valid run -20.0\n",
      "# plays 175 return -13.789642656722716 value_loss 0.03578098921823775 entropy 1.7150820325933522\n",
      "Valid run -20.0\n",
      "# plays 176 return -13.610678391050445 value_loss 0.03530508760823258 entropy 1.715099468398847\n",
      "Valid run -19.0\n",
      "# plays 177 return -13.849610551945402 value_loss 0.034882768288673 entropy 1.714436987027892\n",
      "Valid run -20.0\n",
      "# plays 178 return -13.964649496750862 value_loss 0.034424265589196416 entropy 1.7141970409004172\n",
      "Valid run -20.0\n",
      "# plays 179 return -13.868184547075778 value_loss 0.033682979597937104 entropy 1.7143176799191098\n",
      "Valid run -20.0\n",
      "# plays 180 return -13.6813660923682 value_loss 0.033375348854921195 entropy 1.7145539801041598\n",
      "Valid run -20.0\n",
      "# plays 181 return -13.613229483131382 value_loss 0.032959775589879094 entropy 1.7146465426112332\n",
      "Valid run -20.0\n",
      "# plays 182 return -13.851906534818244 value_loss 0.03219859116194463 entropy 1.7150221477227596\n",
      "Valid run -20.0\n",
      "# plays 183 return -13.76671588133642 value_loss 0.03160306662694647 entropy 1.7155270369226367\n",
      "Valid run -20.0\n",
      "# plays 184 return -13.490044293202779 value_loss 0.030844106677556347 entropy 1.716002913016258\n",
      "Valid run -20.0\n",
      "# plays 185 return -13.441039863882501 value_loss 0.030218911941395084 entropy 1.7159510754925391\n",
      "Valid run -20.0\n",
      "# plays 186 return -13.496935877494252 value_loss 0.03015243491579224 entropy 1.7157700145732329\n",
      "Valid run -20.0\n",
      "# plays 187 return -13.647242289744828 value_loss 0.030062361637197103 entropy 1.7159685636989548\n",
      "Valid run -20.0\n",
      "# plays 188 return -13.682518060770345 value_loss 0.029673163328566104 entropy 1.7161270520712797\n",
      "Valid run -20.0\n",
      "# plays 189 return -13.814266254693312 value_loss 0.029706464768050268 entropy 1.7162593072609194\n",
      "Valid run -20.0\n",
      "# plays 190 return -14.232839629223982 value_loss 0.030017609426913998 entropy 1.7151199265314745\n",
      "Valid run -20.0\n",
      "# plays 191 return -13.709555666301585 value_loss 0.030442591037933536 entropy 1.7153001823181884\n",
      "Valid run -20.0\n",
      "# plays 192 return -13.638600099671427 value_loss 0.030316964076385955 entropy 1.7147255961508832\n",
      "Valid run -20.0\n",
      "# plays 193 return -13.474740089704284 value_loss 0.030267743836128173 entropy 1.7141211087217063\n",
      "Valid run -20.0\n",
      "# plays 194 return -13.127266080733856 value_loss 0.030867332059398404 entropy 1.7139000510041336\n",
      "Valid run -20.0\n",
      "# plays 195 return -13.01453947266047 value_loss 0.03157738827608983 entropy 1.7135744373691406\n",
      "Valid run -20.0\n",
      "# plays 196 return -13.213085525394423 value_loss 0.0319114351982739 entropy 1.7129523837779752\n",
      "Valid run -20.0\n",
      "# plays 197 return -13.39177697285498 value_loss 0.031575699125069356 entropy 1.7126011712207394\n",
      "Valid run -20.0\n",
      "# plays 198 return -13.652599275569482 value_loss 0.0331396500536841 entropy 1.7118946830381332\n",
      "Valid run -20.0\n",
      "# plays 199 return -13.787339348012534 value_loss 0.03347801814398543 entropy 1.7109921797970118\n",
      "Valid run -20.0\n",
      "# plays 200 return -13.10860541321128 value_loss 0.03351737320352831 entropy 1.7105801788526185\n",
      "Valid run -20.0\n",
      "# plays 201 return -13.197744871890151 value_loss 0.034605773627125365 entropy 1.7103061915905209\n",
      "Valid run -20.0\n",
      "# plays 202 return -13.477970384701136 value_loss 0.03462320589068217 entropy 1.7094895229820188\n",
      "Valid run -20.0\n",
      "# plays 203 return -13.130173346231022 value_loss 0.03413867803403154 entropy 1.70926539274736\n",
      "Valid run -20.0\n",
      "# plays 204 return -13.01715601160792 value_loss 0.033892921525136364 entropy 1.708540040068408\n",
      "Valid run -20.0\n",
      "# plays 205 return -12.815440410447128 value_loss 0.03429683394839978 entropy 1.7084309679819512\n",
      "Valid run -20.0\n",
      "# plays 206 return -12.933896369402415 value_loss 0.034073508285975 entropy 1.708704006455255\n",
      "Valid run -20.0\n",
      "# plays 207 return -12.640506732462175 value_loss 0.03480350248828132 entropy 1.7083524903850336\n",
      "Valid run -20.0\n",
      "# plays 208 return -12.676456059215958 value_loss 0.0353568447930926 entropy 1.708211985495824\n",
      "Valid run -20.0\n",
      "# plays 209 return -12.608810453294364 value_loss 0.03576365813344543 entropy 1.7079772502877264\n",
      "Valid run -20.0\n",
      "# plays 210 return -12.647929407964929 value_loss 0.03703168925401412 entropy 1.7078224300828264\n",
      "Valid run -20.0\n",
      "# plays 211 return -12.583136467168437 value_loss 0.03719672282120743 entropy 1.7075049577263763\n",
      "Valid run -20.0\n",
      "# plays 212 return -12.324822820451594 value_loss 0.0375096247605785 entropy 1.7069785773452057\n",
      "Valid run -21.0\n",
      "# plays 213 return -12.192340538406434 value_loss 0.03748012633731702 entropy 1.7070477025841297\n",
      "Valid run -20.0\n",
      "# plays 214 return -12.073106484565791 value_loss 0.0379544981713063 entropy 1.7065705531539446\n",
      "Valid run -20.0\n",
      "# plays 215 return -11.565795836109212 value_loss 0.037786958819177735 entropy 1.706317220129929\n",
      "Valid run -20.0\n",
      "# plays 216 return -11.209216252498292 value_loss 0.037680301328492455 entropy 1.7062917978962435\n",
      "Valid run -20.0\n",
      "# plays 217 return -11.388294627248463 value_loss 0.037587010049813054 entropy 1.7060126959008253\n",
      "Valid run -20.0\n",
      "# plays 218 return -11.649465164523617 value_loss 0.036894565314644336 entropy 1.7058445797194248\n",
      "Valid run -20.0\n",
      "# plays 219 return -11.484518648071255 value_loss 0.037404318592483186 entropy 1.7052695354903358\n",
      "Valid run -20.0\n",
      "# plays 220 return -11.63606678326413 value_loss 0.0378707933648238 entropy 1.7049380227246969\n",
      "Valid run -20.0\n",
      "# plays 221 return -11.872460104937717 value_loss 0.03870531289790357 entropy 1.7046475241936152\n",
      "Valid run -20.0\n",
      "# plays 222 return -11.585214094443947 value_loss 0.03885955060137169 entropy 1.7040174406671464\n",
      "Valid run -19.0\n",
      "# plays 223 return -11.726692684999552 value_loss 0.03895589249980023 entropy 1.7031587531325694\n",
      "Valid run -20.0\n",
      "# plays 224 return -11.654023416499596 value_loss 0.03996485505388591 entropy 1.7026960786513758\n",
      "Valid run -20.0\n",
      "# plays 225 return -11.188621074849635 value_loss 0.040841359948096394 entropy 1.701863939638945\n",
      "Valid run -20.0\n",
      "# plays 226 return -11.469758967364672 value_loss 0.041654347528497324 entropy 1.701245093021535\n",
      "Valid run -20.0\n",
      "# plays 227 return -11.922783070628205 value_loss 0.04165560191823239 entropy 1.7019838780761103\n",
      "Valid run -20.0\n",
      "# plays 228 return -12.030504763565386 value_loss 0.04227131324368592 entropy 1.7010378681031078\n",
      "Valid run -20.0\n",
      "# plays 229 return -12.627454287208849 value_loss 0.04154298686109075 entropy 1.7008900247482628\n",
      "Valid run -20.0\n",
      "# plays 230 return -11.964708858487963 value_loss 0.04051384538223227 entropy 1.7014106025077576\n",
      "Valid run -20.0\n",
      "# plays 231 return -11.368237972639166 value_loss 0.040834373621432905 entropy 1.701113774228773\n",
      "Valid run -20.0\n",
      "# plays 232 return -11.53141417537525 value_loss 0.04108054352249705 entropy 1.7012424348771664\n",
      "Valid run -15.0\n",
      "# plays 233 return -11.678272757837727 value_loss 0.040291419960075646 entropy 1.7011785574820184\n",
      "Valid run -15.0\n",
      "# plays 234 return -12.010445482053955 value_loss 0.040027082044196756 entropy 1.7011647617691568\n",
      "Valid run -19.0\n",
      "# plays 235 return -11.70940093384856 value_loss 0.0398484126433545 entropy 1.7011671950631828\n",
      "Valid run -15.0\n",
      "# plays 236 return -11.738460840463706 value_loss 0.03977277478916767 entropy 1.7008721855717543\n",
      "Valid run -17.0\n",
      "# plays 237 return -11.364614756417335 value_loss 0.03973496907660356 entropy 1.7017624159274165\n",
      "Valid run -20.0\n",
      "# plays 238 return -11.828153280775602 value_loss 0.03931335728206328 entropy 1.7011999115482286\n",
      "Valid run -14.0\n",
      "# plays 239 return -11.945337952698043 value_loss 0.03872742723677836 entropy 1.7011483514009091\n"
     ]
    }
   ],
   "source": [
    "n_iter = 10000\n",
    "init_collect = 10\n",
    "n_collect = 1\n",
    "n_value = 10\n",
    "n_policy = 10\n",
    "disp_iter = 1\n",
    "val_iter = 1\n",
    "\n",
    "max_len = 1000\n",
    "batch_size = 500\n",
    "\n",
    "ent_coeff = 0. #0.001\n",
    "discount_factor = .95\n",
    "\n",
    "value_loss = -numpy.Inf\n",
    "ret = -numpy.Inf\n",
    "entropy = -numpy.Inf\n",
    "\n",
    "return_history = []\n",
    "\n",
    "for ni in range(n_iter):\n",
    "    player.eval()\n",
    "\n",
    "    if numpy.mod(ni, val_iter) == 0:\n",
    "        _, _, _, _, ret_ = collect_one_episode(env, player, max_len=max_len, deterministic=True)\n",
    "        print('Valid run', ret_)\n",
    "        return_history.append(ret_)\n",
    "\n",
    "    # collect some episodes using the current policy\n",
    "    # and push (obs,a,r,p(a)) tuples to the replay buffer.\n",
    "    nc = n_collect\n",
    "    if ni == 0:\n",
    "        nc = init_collect\n",
    "    for ci in range(nc):\n",
    "        o_, c_, a_, ap_, ret_ = collect_one_episode(env, player, max_len=max_len, discount_factor=discount_factor)\n",
    "        replay_buffer.add(o_, c_, a_, ap_)\n",
    "        if ret == -numpy.Inf:\n",
    "            ret = ret_\n",
    "        else:\n",
    "            ret = 0.9 * ret + 0.1 * ret_\n",
    "    \n",
    "    player.train()\n",
    "        \n",
    "    # fit a value function\n",
    "    for vi in range(n_value):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        batch_x = torch.from_numpy(numpy.stack([ex[0] for ex in batch]).astype('float32'))\n",
    "        batch_y = torch.from_numpy(numpy.stack([ex[1] for ex in batch]).astype('float32'))\n",
    "        pred_y = value(batch_x).squeeze()\n",
    "        loss_ = ((batch_y - pred_y) ** 2)\n",
    "        \n",
    "        batch_a = torch.from_numpy(numpy.stack([ex[2] for ex in batch]).astype('float32')[:,None])\n",
    "        batch_pi = player(batch_x, normalized=True)\n",
    "        batch_q = torch.from_numpy(numpy.stack([ex[3] for ex in batch]).astype('float32'))\n",
    "        logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "\n",
    "        # (clipped) importance weight: \n",
    "        # because the policy may have changed since the tuple was collected.\n",
    "        iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "    \n",
    "        loss = iw * loss_\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_value.step()\n",
    "        \n",
    "    if value_loss < 0.:\n",
    "        value_loss = loss_.mean().item()\n",
    "    else:\n",
    "        value_loss = 0.9 * value_loss + 0.1 * loss_.mean().item()\n",
    "    \n",
    "    if numpy.mod(ni, disp_iter) == 0:\n",
    "        print('# plays', (ni+1) * n_collect, 'return', ret, 'value_loss', value_loss, 'entropy', -entropy)\n",
    "    \n",
    "    # fit a policy\n",
    "    for pi in range(n_policy):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        batch_x = torch.from_numpy(numpy.stack([ex[0] for ex in batch]).astype('float32'))\n",
    "        batch_r = torch.from_numpy(numpy.stack([ex[1] for ex in batch]).astype('float32')[:,None])\n",
    "        batch_v = value(batch_x)\n",
    "        batch_a = torch.from_numpy(numpy.stack([ex[2] for ex in batch]).astype('float32')[:,None])\n",
    "        batch_q = torch.from_numpy(numpy.stack([ex[3] for ex in batch]).astype('float32'))\n",
    "\n",
    "        batch_pi = player(batch_x, normalized=True)\n",
    "        \n",
    "        logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "        \n",
    "        # advantage\n",
    "        adv = batch_r - batch_v.clone().detach()\n",
    "        \n",
    "        loss = -(adv * logp)\n",
    "        \n",
    "        # (clipped) importance weight: \n",
    "        # because the policy may have changed since the tuple was collected.\n",
    "        iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "    \n",
    "        loss = iw * loss\n",
    "        \n",
    "        # entropy regularization: though, it doesn't look necessary in this specific case.\n",
    "        ent = (batch_pi * torch.log(batch_pi)).sum(1)\n",
    "        \n",
    "        if entropy == -numpy.Inf:\n",
    "            entropy = ent.mean().item()\n",
    "        else:\n",
    "            entropy = 0.9 * entropy + 0.1 * ent.mean().item()\n",
    "        \n",
    "        loss = (loss + ent_coeff * ent).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_player.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.figure()\n",
    "\n",
    "plot.plot(return_history)\n",
    "\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'player' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8b543fa33c12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# let the final policy play the pong longer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_one_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'player' is not defined"
     ]
    }
   ],
   "source": [
    "# let the final policy play the pong longer\n",
    "player.eval()\n",
    "_, _, _, _, ret_ = collect_one_episode(env, player, max_len=1000000, deterministic=True, rendering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
