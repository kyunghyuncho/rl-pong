{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResLinear(nn.Module):\n",
    "    def __init__(self, n_in, n_out, act=nn.ReLU()):\n",
    "        super(ResLinear, self).__init__()\n",
    "        self.act = act\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "        self.bn = nn.BatchNorm1d(n_out)\n",
    "        \n",
    "        assert(n_in == n_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.act(self.bn(self.linear(x)))\n",
    "        return h + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(nn.Module):\n",
    "    def __init__(self, n_in=128, n_hid=100, n_out=6):\n",
    "        super(Player, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid),\n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, n_out))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, obs, normalized=False):\n",
    "        if normalized:\n",
    "            return self.softmax(self.layers(obs))\n",
    "        else:\n",
    "            return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(nn.Module):\n",
    "    def __init__(self, n_in=128, n_act=6, n_hid=100):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid), \n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, n_act))\n",
    "    \n",
    "    def forward(self, obs, act):\n",
    "#         set_trace()\n",
    "        return self.layers(obs).gather(1, act.long())\n",
    "    \n",
    "    def value(self, obs):\n",
    "        return self.layers(obs).max(dim=1)\n",
    "    \n",
    "    def q(self, obs):\n",
    "        return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(nn.Module):\n",
    "    def __init__(self, n_in=128, n_hid=100):\n",
    "        super(Value, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid),\n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, 1))\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_params(from_, to_):\n",
    "    for f_, t_ in zip(from_.parameters(), to_.parameters()):\n",
    "        t_.data.copy_(f_.data)\n",
    "        \n",
    "def avg_params(from_, to_, coeff=0.95):\n",
    "    for f_, t_ in zip(from_.parameters(), to_.parameters()):\n",
    "        t_.data.copy_(coeff * t_.data + (1.-coeff) * f_.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_obs(obs):\n",
    "    return obs.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data\n",
    "def collect_one_episode(env, player, max_len=50, discount_factor=0.9, deterministic=False, rendering=False, verbose=False):\n",
    "    episode = []\n",
    "\n",
    "    observations = []\n",
    "\n",
    "    rewards = []\n",
    "    crewards = []\n",
    "\n",
    "    actions = []\n",
    "    action_probs = []\n",
    "\n",
    "    obs = env.reset()\n",
    "    \n",
    "    for ml in range(max_len):\n",
    "        if rendering:\n",
    "            env.render()\n",
    "            sleep(0.05)\n",
    "            \n",
    "        obs = normalize_obs(obs)\n",
    "\n",
    "        out_probs = player(torch.from_numpy(obs[None,:]).to(device), normalized=True).squeeze()\n",
    "        \n",
    "        if deterministic:\n",
    "            action = numpy.argmax(out_probs.to('cpu').data.numpy())\n",
    "            if verbose:\n",
    "                print(out_probs, action)\n",
    "        else:\n",
    "            act_dist = Categorical(out_probs)\n",
    "            action = act_dist.sample().item()\n",
    "        action_prob = out_probs[action].item()\n",
    "\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        action_probs.append(action_prob)\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if deterministic and verbose:\n",
    "            print(reward, done)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "\n",
    "    rewards = numpy.array(rewards)\n",
    "\n",
    "    # it's probably not the best idea to compute the discounted cumulative returns here, but well..\n",
    "    for ri in range(len(rewards)):\n",
    "        factors = (discount_factor ** numpy.arange(len(rewards)-ri))\n",
    "        crewards.append(numpy.sum(rewards[ri:] * factors))\n",
    "        \n",
    "    # discard the final 10%, because it really doesn't give me a good signal due to the unbounded horizon\n",
    "    # this is only for training, not for computing the total return of the episode of the given length\n",
    "    discard = max_len // 10\n",
    "        \n",
    "    return observations[:-discard], rewards[:-discard], crewards[:-discard], actions[:-discard], action_probs[:-discard], rewards.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple implementation of FIFO-based replay buffer\n",
    "class Buffer:\n",
    "    def __init__(self, max_items=10000):\n",
    "        self.max_items = max_items\n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, observations, rewards, crewards, actions, action_probs):\n",
    "        new_n = len(observations)\n",
    "        old_n = len(self.buffer)\n",
    "        if new_n + old_n > self.max_items:\n",
    "            del self.buffer[:new_n]\n",
    "        for o, r, c, a, p, on, rn, cn, an, pn in zip(observations[:-1], rewards[:-1], crewards[:-1], actions[:-1], action_probs[:-1],\n",
    "                                             observations[1:], rewards[1:], crewards[1:], actions[1:], action_probs[1:]):\n",
    "            self.buffer.append({'current': {'obs': o, \n",
    "                                            'rew': r, \n",
    "                                            'crew': c, \n",
    "                                            'act': a, \n",
    "                                            'prob': p},\n",
    "                                'next': {'obs': on, \n",
    "                                         'rew': rn, \n",
    "                                         'crew': cn, \n",
    "                                         'act': an, \n",
    "                                         'prob': pn}})\n",
    "            \n",
    "    def sample(self, n=100):\n",
    "        idxs = numpy.random.choice(len(self.buffer),n)\n",
    "        return [self.buffer[ii] for ii in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a policy\n",
    "player = Player(n_in=128, n_hid=128, n_out=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a q estimator\n",
    "qnet = Qnet(n_in=128, n_hid=128, n_act=6).to(device)\n",
    "qold = Qnet(n_in=128, n_hid=128, n_act=6).to(device)\n",
    "copy_params(qnet, qold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a value estimator\n",
    "value = Value(n_in=128, n_hid=128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimizers\n",
    "opt_player = Adam(player.parameters(), lr=0.0001)\n",
    "opt_q = Adam(qnet.parameters(), lr=0.0001)\n",
    "opt_value = Adam(value.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize replay buffer\n",
    "replay_buffer = Buffer(max_items=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid run -20.0 -20.0\n",
      "# plays 1 return -18.0 value_loss 0.03813857212662697 q_loss 0.03813857212662697 entropy inf\n",
      "Valid run -19.0 -19.9\n",
      "# plays 2 return -18.099999999999998 value_loss 0.03685493413358927 q_loss 0.03685493413358927 entropy 1.6074097934430505\n",
      "Valid run -20.0 -19.91\n",
      "# plays 3 return -17.99 value_loss 0.03511260205879808 q_loss 0.03511260205879808 entropy 1.6169676973578233\n",
      "Valid run -19.0 -19.819\n",
      "# plays 4 return -18.090999999999998 value_loss 0.03335014883615077 q_loss 0.03335014883615077 entropy 1.5723784684094466\n",
      "Valid run -17.0 -19.5371\n",
      "# plays 5 return -17.281899999999997 value_loss 0.03174497566912324 q_loss 0.03174497566912324 entropy 1.5280875589563776\n",
      "Valid run -20.0 -19.583389999999998\n",
      "# plays 6 return -17.253709999999998 value_loss 0.02990970534274355 q_loss 0.02990970534274355 entropy 1.443850965979701\n",
      "Valid run -20.0 -19.625051\n",
      "# plays 7 return -17.328339 value_loss 0.02876761091222801 q_loss 0.02876761091222801 entropy 1.3796493953665958\n",
      "Valid run -16.0 -19.262545900000003\n",
      "# plays 8 return -17.3955051 value_loss 0.02703930755305682 q_loss 0.02703930755305682 entropy 1.3262698015759027\n",
      "Valid run -20.0 -19.336291310000004\n",
      "# plays 9 return -17.65595459 value_loss 0.026078045433894345 q_loss 0.026078045433894345 entropy 1.283588933661236\n",
      "Valid run -17.0 -19.102662179000003\n",
      "# plays 10 return -17.790359131 value_loss 0.02462132085439761 q_loss 0.02462132085439761 entropy 1.244690070016068\n",
      "Valid run -17.0 -18.892395961100004\n",
      "# plays 11 return -17.911323217899998 value_loss 0.02367224317709088 q_loss 0.02367224317709088 entropy 1.182826232488119\n",
      "Valid run -17.0 -18.703156364990004\n",
      "# plays 12 return -18.020190896109998 value_loss 0.022683488121653914 q_loss 0.022683488121653914 entropy 1.1353483920291234\n",
      "Valid run -20.0 -18.832840728491004\n",
      "# plays 13 return -17.918171806498997 value_loss 0.0214587203407514 q_loss 0.0214587203407514 entropy 1.0847925086309647\n",
      "Valid run -20.0 -18.949556655641903\n",
      "# plays 14 return -17.826354625849095 value_loss 0.020239846098652 q_loss 0.020239846098652 entropy 1.043646666884222\n",
      "Valid run -17.0 -18.754600990077712\n",
      "# plays 15 return -17.943719163264184 value_loss 0.019329386056122642 q_loss 0.019329386056122642 entropy 1.0020297301468863\n",
      "Valid run -17.0 -18.57914089106994\n",
      "# plays 16 return -17.849347246937764 value_loss 0.018349191841325874 q_loss 0.018349191841325874 entropy 0.962112919700222\n",
      "Valid run -19.0 -18.621226801962944\n",
      "# plays 17 return -17.764412522243987 value_loss 0.017419760995514733 q_loss 0.017419760995514733 entropy 0.9339439143974834\n",
      "Valid run -19.0 -18.65910412176665\n",
      "# plays 18 return -17.687971270019588 value_loss 0.01635978412858326 q_loss 0.01635978412858326 entropy 0.8993333076880742\n",
      "Valid run -20.0 -18.793193709589985\n",
      "# plays 19 return -17.61917414301763 value_loss 0.01536366283467423 q_loss 0.01536366283467423 entropy 0.8797727779309601\n",
      "Valid run -20.0 -18.913874338630986\n",
      "# plays 20 return -17.857256728715868 value_loss 0.014563753893488745 q_loss 0.014563753893488745 entropy 0.8483705611466811\n"
     ]
    }
   ],
   "source": [
    "n_iter = 1000\n",
    "init_collect = 1\n",
    "n_collect = 1\n",
    "n_q = 100\n",
    "n_value = 0\n",
    "n_policy = 100\n",
    "disp_iter = 1\n",
    "val_iter = 1\n",
    "\n",
    "max_len = 1000\n",
    "batch_size = 1000\n",
    "\n",
    "ent_coeff = 0. #0.001\n",
    "discount_factor = .95\n",
    "\n",
    "q_loss = -numpy.Inf\n",
    "value_loss = -numpy.Inf\n",
    "ret = -numpy.Inf\n",
    "entropy = -numpy.Inf\n",
    "valid_ret = -numpy.Inf\n",
    "\n",
    "return_history = []\n",
    "\n",
    "for ni in range(n_iter):\n",
    "    player.eval()\n",
    "\n",
    "    if numpy.mod(ni, val_iter) == 0:\n",
    "        _, _, _, _, _, ret_ = collect_one_episode(env, player, max_len=max_len, deterministic=True)\n",
    "        return_history.append(ret_)\n",
    "        if valid_ret == -numpy.Inf:\n",
    "            valid_ret = ret_\n",
    "        else:\n",
    "            valid_ret = 0.9 * valid_ret + 0.1 * ret_\n",
    "        print('Valid run', ret_, valid_ret)\n",
    "\n",
    "    # collect some episodes using the current policy\n",
    "    # and push (obs,a,r,p(a)) tuples to the replay buffer.\n",
    "    nc = n_collect\n",
    "    if ni == 0:\n",
    "        nc = init_collect\n",
    "    for ci in range(nc):\n",
    "        o_, r_, c_, a_, ap_, ret_ = collect_one_episode(env, player, max_len=max_len, discount_factor=discount_factor)\n",
    "        replay_buffer.add(o_, r_, c_, a_, ap_)\n",
    "        if ret == -numpy.Inf:\n",
    "            ret = ret_\n",
    "        else:\n",
    "            ret = 0.9 * ret + 0.1 * ret_\n",
    "    \n",
    "    player.train()\n",
    "    \n",
    "    # fit a q function\n",
    "    # TD learning: min_Q (Q(s,a) - (r + \\gamma \\max_a' Q(s',a')))^2\n",
    "    for qi in range(n_q):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        opt_q.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        batch_x = torch.from_numpy(numpy.stack([ex['current']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_xn = torch.from_numpy(numpy.stack([ex['next']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        \n",
    "        batch_y = torch.from_numpy(numpy.stack([ex['current']['rew'] for ex in batch]).astype('float32')).to(device)\n",
    "        \n",
    "        batch_a = torch.from_numpy(numpy.stack([ex['current']['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        \n",
    "        q_pred = qnet(batch_x, batch_a).squeeze()\n",
    "        q_next = qold.value(batch_xn)[0].squeeze()\n",
    "        \n",
    "        loss_ = ((q_pred - (batch_y + discount_factor * q_next)) ** 2)\n",
    "        \n",
    "#         batch_a = torch.from_numpy(numpy.stack([ex['current']['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "#         batch_pi = player(batch_x, normalized=True)\n",
    "#         batch_q = torch.from_numpy(numpy.stack([ex['current']['prob'] for ex in batch]).astype('float32')).to(device)\n",
    "#         logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "\n",
    "#         # (clipped) importance weight: \n",
    "#         # because the policy may have changed since the tuple was collected.\n",
    "#         iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "        iw = 1.\n",
    "        loss = iw * loss_\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_q.step()\n",
    "        \n",
    "    if q_loss == -numpy.Inf:\n",
    "        q_loss = loss_.mean().item()\n",
    "    else:\n",
    "        q_loss = 0.9 * q_loss + 0.1 * loss_.mean().item()\n",
    "        \n",
    "    copy_params(qnet, qold)\n",
    "        \n",
    "    # fit a value function\n",
    "    # a usual value estimator: min_V (V(s) - Q(s,a))^2\n",
    "    for vi in range(n_value):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        opt_q.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        batch_x = torch.from_numpy(numpy.stack([ex['current']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_a = torch.from_numpy(numpy.stack([ex['current']['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        \n",
    "        pred_y = value(batch_x).squeeze()\n",
    "        pred_q = qnet(batch_x, batch_a).squeeze()\n",
    "        \n",
    "        loss_ = ((pred_y - pred_q) ** 2)\n",
    "        \n",
    "        batch_pi = player(batch_x, normalized=True)\n",
    "        batch_q = torch.from_numpy(numpy.stack([ex['current']['prob'] for ex in batch]).astype('float32')).to(device)\n",
    "        logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "\n",
    "        # (clipped) importance weight: \n",
    "        # because the policy may have changed since the tuple was collected.\n",
    "        iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "    \n",
    "        loss = iw * loss_\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_value.step()\n",
    "        \n",
    "    if value_loss < 0.:\n",
    "        value_loss = loss_.mean().item()\n",
    "    else:\n",
    "        value_loss = 0.9 * value_loss + 0.1 * loss_.mean().item()\n",
    "    \n",
    "    if numpy.mod(ni, disp_iter) == 0:\n",
    "        print('# plays', (ni+1) * n_collect, 'return', ret, 'value_loss', value_loss, 'q_loss', q_loss, 'entropy', -entropy)\n",
    "    \n",
    "    # fit a policy\n",
    "    # advantage: (Q(a,s) - V(s))\n",
    "    for pi in range(n_policy):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        opt_q.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        batch_x = torch.from_numpy(numpy.stack([ex['current']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_a = torch.from_numpy(numpy.stack([ex['current']['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "\n",
    "        batch_pi = player(batch_x, normalized=True)        \n",
    "        logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "\n",
    "        batch_q = qnet(batch_x, batch_a)\n",
    "        batch_v = value(batch_x)\n",
    "#         batch_v = qnet(batch_x, batch_pi.max(1)[1].resize_(batch_x.size(0),1))[0]\n",
    "#         batch_v = 0.\n",
    "        \n",
    "        # advantage\n",
    "        adv = batch_q.clone().detach() - batch_v\n",
    "        \n",
    "        loss = -(adv * logp)\n",
    "        \n",
    "        batch_q = torch.from_numpy(numpy.stack([ex['current']['prob'] for ex in batch]).astype('float32')).to(device)\n",
    "\n",
    "        # (clipped) importance weight: \n",
    "        # because the policy may have changed since the tuple was collected.\n",
    "        iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "    \n",
    "        loss = iw * loss\n",
    "        \n",
    "        # entropy regularization: though, it doesn't look necessary in this specific case.\n",
    "        ent = (batch_pi * torch.log(batch_pi)).sum(1)\n",
    "        \n",
    "        if entropy == -numpy.Inf:\n",
    "            entropy = ent.mean().item()\n",
    "        else:\n",
    "            entropy = 0.9 * entropy + 0.1 * ent.mean().item()\n",
    "        \n",
    "        loss = (loss + ent_coeff * ent).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_player.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.9671e-02],\n",
       "        [-2.1483e-02],\n",
       "        [-2.7753e-03],\n",
       "        [ 6.5715e-02],\n",
       "        [ 9.9265e-03],\n",
       "        [-1.4015e-02],\n",
       "        [-4.2587e-02],\n",
       "        [-4.0621e-02],\n",
       "        [-1.0320e-01],\n",
       "        [-2.0867e-02],\n",
       "        [ 9.3001e-04],\n",
       "        [-1.3461e-02],\n",
       "        [ 4.4695e-02],\n",
       "        [ 4.7797e-02],\n",
       "        [-1.6346e-02],\n",
       "        [-7.0842e-02],\n",
       "        [-5.0579e-02],\n",
       "        [-2.4181e-02],\n",
       "        [ 2.7496e-02],\n",
       "        [ 1.3529e-02],\n",
       "        [ 4.3204e-02],\n",
       "        [-5.8075e-03],\n",
       "        [-1.5394e-02],\n",
       "        [-1.1878e-02],\n",
       "        [-4.6884e-02],\n",
       "        [ 2.2777e-02],\n",
       "        [ 2.6827e-02],\n",
       "        [-2.6603e-02],\n",
       "        [ 7.3631e-03],\n",
       "        [ 3.3792e-02],\n",
       "        [ 3.0504e-03],\n",
       "        [-1.1250e-02],\n",
       "        [-1.5639e-02],\n",
       "        [-4.4990e-02],\n",
       "        [-6.5820e-03],\n",
       "        [ 3.8726e-02],\n",
       "        [-2.2083e-02],\n",
       "        [ 4.3923e-02],\n",
       "        [ 2.8438e-02],\n",
       "        [-2.6864e-02],\n",
       "        [-4.4006e-02],\n",
       "        [-6.1517e-02],\n",
       "        [ 4.1326e-02],\n",
       "        [ 7.7520e-02],\n",
       "        [-1.1404e-01],\n",
       "        [-3.2017e-02],\n",
       "        [ 1.8236e-02],\n",
       "        [ 2.0342e-02],\n",
       "        [-7.6739e-02],\n",
       "        [ 2.9744e-02],\n",
       "        [-3.8860e-02],\n",
       "        [-1.6091e-02],\n",
       "        [-4.3402e-02],\n",
       "        [ 4.0644e-02],\n",
       "        [ 1.8387e-02],\n",
       "        [ 5.5964e-02],\n",
       "        [-7.1270e-02],\n",
       "        [-2.7147e-02],\n",
       "        [-1.5512e-02],\n",
       "        [ 6.1615e-02],\n",
       "        [-6.5282e-02],\n",
       "        [-3.3609e-02],\n",
       "        [ 1.4723e-02],\n",
       "        [-4.2969e-02],\n",
       "        [-3.5002e-02],\n",
       "        [-3.1374e-02],\n",
       "        [ 9.4137e-02],\n",
       "        [-5.2116e-02],\n",
       "        [ 3.6144e-03],\n",
       "        [-2.5960e-02],\n",
       "        [-5.6563e-02],\n",
       "        [-5.2404e-02],\n",
       "        [-2.1141e-02],\n",
       "        [ 3.8398e-02],\n",
       "        [-4.4029e-03],\n",
       "        [-5.5906e-03],\n",
       "        [-4.8780e-04],\n",
       "        [ 1.4878e-02],\n",
       "        [-1.6624e-02],\n",
       "        [ 2.7302e-02],\n",
       "        [-1.1364e-01],\n",
       "        [ 9.0186e-02],\n",
       "        [-1.5288e-02],\n",
       "        [-3.3912e-02],\n",
       "        [ 2.4491e-02],\n",
       "        [ 5.1239e-02],\n",
       "        [ 1.2124e-02],\n",
       "        [-2.2625e-02],\n",
       "        [ 2.0741e-02],\n",
       "        [ 9.7084e-02],\n",
       "        [ 4.1779e-02],\n",
       "        [-1.7103e-02],\n",
       "        [ 2.0653e-02],\n",
       "        [ 2.1805e-02],\n",
       "        [ 6.6328e-02],\n",
       "        [ 3.2428e-02],\n",
       "        [-3.0333e-02],\n",
       "        [ 3.6829e-02],\n",
       "        [ 2.7404e-03],\n",
       "        [ 1.6631e-02],\n",
       "        [ 2.6262e-02],\n",
       "        [-1.0197e-01],\n",
       "        [ 2.6515e-02],\n",
       "        [ 9.9298e-03],\n",
       "        [ 1.2016e-02],\n",
       "        [-4.0377e-02],\n",
       "        [-4.7819e-02],\n",
       "        [-6.3312e-02],\n",
       "        [ 1.3995e-03],\n",
       "        [ 6.9859e-03],\n",
       "        [ 4.4695e-02],\n",
       "        [ 1.8236e-02],\n",
       "        [ 1.3940e-01],\n",
       "        [-4.0820e-02],\n",
       "        [-1.3777e-02],\n",
       "        [-5.8809e-02],\n",
       "        [ 2.2624e-02],\n",
       "        [-4.6610e-03],\n",
       "        [ 2.2606e-02],\n",
       "        [-2.9411e-02],\n",
       "        [ 6.1112e-02],\n",
       "        [-1.0669e-01],\n",
       "        [ 2.7019e-03],\n",
       "        [ 1.1421e-02],\n",
       "        [ 2.8050e-02],\n",
       "        [-1.0168e-02],\n",
       "        [ 5.7806e-03],\n",
       "        [-3.7385e-02],\n",
       "        [ 6.2659e-03],\n",
       "        [-2.5825e-02],\n",
       "        [-8.3748e-02],\n",
       "        [ 1.7049e-02],\n",
       "        [ 3.0946e-02],\n",
       "        [ 4.9674e-02],\n",
       "        [ 2.3021e-03],\n",
       "        [ 4.0845e-02],\n",
       "        [ 4.0527e-02],\n",
       "        [ 1.0860e-02],\n",
       "        [ 8.1283e-03],\n",
       "        [ 5.2291e-02],\n",
       "        [ 3.9706e-02],\n",
       "        [ 3.1711e-02],\n",
       "        [-9.0604e-03],\n",
       "        [ 4.1779e-02],\n",
       "        [ 6.1485e-02],\n",
       "        [ 1.5968e-02],\n",
       "        [ 1.8256e-03],\n",
       "        [ 7.8819e-02],\n",
       "        [ 9.5513e-03],\n",
       "        [-5.1038e-02],\n",
       "        [ 6.6376e-02],\n",
       "        [ 2.5407e-02],\n",
       "        [ 1.6609e-02],\n",
       "        [ 2.3446e-02],\n",
       "        [-7.0212e-03],\n",
       "        [ 1.5026e-02],\n",
       "        [-3.4960e-02],\n",
       "        [ 5.8314e-03],\n",
       "        [-1.9422e-02],\n",
       "        [ 1.0187e-01],\n",
       "        [ 2.9512e-03],\n",
       "        [ 2.9038e-02],\n",
       "        [ 5.2919e-02],\n",
       "        [-5.0075e-03],\n",
       "        [ 2.9751e-02],\n",
       "        [ 2.9078e-02],\n",
       "        [ 6.0987e-02],\n",
       "        [ 8.1839e-02],\n",
       "        [-3.1791e-03],\n",
       "        [-1.1972e-03],\n",
       "        [-2.2717e-03],\n",
       "        [-7.4824e-03],\n",
       "        [ 1.2805e-03],\n",
       "        [ 2.1352e-02],\n",
       "        [-3.4271e-02],\n",
       "        [ 4.7111e-02],\n",
       "        [ 3.9950e-02],\n",
       "        [ 2.1231e-02],\n",
       "        [-4.8940e-02],\n",
       "        [ 2.2975e-02],\n",
       "        [ 1.1119e-01],\n",
       "        [-5.4917e-02],\n",
       "        [ 3.9950e-02],\n",
       "        [ 3.9650e-02],\n",
       "        [-1.8617e-02],\n",
       "        [-1.2966e-02],\n",
       "        [ 6.7238e-03],\n",
       "        [ 2.7500e-02],\n",
       "        [ 4.3882e-02],\n",
       "        [-1.8763e-02],\n",
       "        [ 5.4675e-02],\n",
       "        [-3.2220e-02],\n",
       "        [ 1.5330e-02],\n",
       "        [ 3.8734e-02],\n",
       "        [ 6.6503e-02],\n",
       "        [-1.5288e-02],\n",
       "        [ 1.1581e-01],\n",
       "        [ 4.9891e-02],\n",
       "        [-1.3506e-02],\n",
       "        [-6.5550e-02],\n",
       "        [ 4.9718e-02],\n",
       "        [-7.8623e-02],\n",
       "        [ 2.0640e-02],\n",
       "        [ 2.3064e-02],\n",
       "        [ 2.0631e-02],\n",
       "        [ 2.4669e-02],\n",
       "        [-1.4279e-02],\n",
       "        [ 2.1609e-02],\n",
       "        [ 1.1089e-02],\n",
       "        [ 4.7497e-03],\n",
       "        [-2.9893e-02],\n",
       "        [ 2.8832e-03],\n",
       "        [-5.0756e-02],\n",
       "        [ 1.1352e-02],\n",
       "        [-2.7819e-02],\n",
       "        [ 9.2753e-03],\n",
       "        [-1.7095e-02],\n",
       "        [-2.0698e-02],\n",
       "        [-2.3331e-02],\n",
       "        [-3.7385e-02],\n",
       "        [-3.2160e-03],\n",
       "        [ 1.0223e-02],\n",
       "        [ 4.9164e-02],\n",
       "        [ 2.0014e-02],\n",
       "        [ 5.8468e-02],\n",
       "        [-6.5936e-02],\n",
       "        [-1.0241e-01],\n",
       "        [ 6.0470e-02],\n",
       "        [ 2.9801e-03],\n",
       "        [-8.4782e-02],\n",
       "        [-2.5825e-02],\n",
       "        [-7.5189e-02],\n",
       "        [-8.3816e-04],\n",
       "        [ 1.7591e-02],\n",
       "        [ 9.2771e-02],\n",
       "        [ 5.5892e-02],\n",
       "        [-2.1034e-02],\n",
       "        [ 3.6460e-03],\n",
       "        [ 1.8631e-02],\n",
       "        [ 7.3752e-02],\n",
       "        [ 2.4816e-02],\n",
       "        [-5.6471e-03],\n",
       "        [-5.8239e-02],\n",
       "        [-5.0260e-02],\n",
       "        [-2.9915e-02],\n",
       "        [-2.0514e-03],\n",
       "        [ 4.4760e-02],\n",
       "        [-5.7403e-02],\n",
       "        [-3.9044e-02],\n",
       "        [-2.5657e-02],\n",
       "        [-2.0879e-01],\n",
       "        [ 4.1255e-03],\n",
       "        [-4.2217e-03],\n",
       "        [-2.3985e-02],\n",
       "        [ 1.3995e-03],\n",
       "        [-1.1296e-01],\n",
       "        [ 4.6940e-03],\n",
       "        [-3.2631e-02],\n",
       "        [-1.8399e-03],\n",
       "        [ 1.2452e-02],\n",
       "        [ 6.3135e-02],\n",
       "        [ 7.6954e-02],\n",
       "        [ 1.0585e-02],\n",
       "        [ 5.9691e-02],\n",
       "        [-6.9093e-02],\n",
       "        [-4.0682e-02],\n",
       "        [-5.5817e-03],\n",
       "        [-1.6245e-02],\n",
       "        [-2.6641e-02],\n",
       "        [ 1.7042e-03],\n",
       "        [ 8.6391e-04],\n",
       "        [-1.2997e-01],\n",
       "        [ 3.0451e-02],\n",
       "        [-5.2307e-02],\n",
       "        [-6.5698e-03],\n",
       "        [-5.4124e-02],\n",
       "        [-1.0207e-01],\n",
       "        [-3.8465e-02],\n",
       "        [ 3.8986e-02],\n",
       "        [ 6.2563e-02],\n",
       "        [ 7.6756e-02],\n",
       "        [ 1.1754e-02],\n",
       "        [ 1.5525e-01],\n",
       "        [ 4.2522e-02],\n",
       "        [ 2.0486e-04],\n",
       "        [ 2.8478e-03],\n",
       "        [ 4.2572e-03],\n",
       "        [-2.9779e-02],\n",
       "        [-3.4786e-03],\n",
       "        [-9.9413e-02],\n",
       "        [-2.8682e-02],\n",
       "        [ 2.8900e-02],\n",
       "        [-5.0863e-02],\n",
       "        [-1.9678e-02],\n",
       "        [-2.2083e-02],\n",
       "        [-6.7303e-02],\n",
       "        [ 7.1023e-02],\n",
       "        [ 3.3705e-02],\n",
       "        [ 3.3297e-02],\n",
       "        [-3.3609e-02],\n",
       "        [-2.5097e-02],\n",
       "        [-1.8380e-02],\n",
       "        [ 7.0275e-02],\n",
       "        [ 1.6357e-02],\n",
       "        [-2.0290e-02],\n",
       "        [ 8.6771e-03],\n",
       "        [ 4.3634e-02],\n",
       "        [-4.3731e-02],\n",
       "        [ 1.8151e-02],\n",
       "        [ 1.2965e-01],\n",
       "        [-9.6012e-02],\n",
       "        [-7.5761e-02],\n",
       "        [-1.6743e-02],\n",
       "        [ 1.2860e-02],\n",
       "        [ 1.4005e-01],\n",
       "        [ 1.0460e-02],\n",
       "        [-4.3292e-03],\n",
       "        [-7.2467e-03],\n",
       "        [ 9.7146e-02],\n",
       "        [-1.5023e-02],\n",
       "        [ 5.2779e-02],\n",
       "        [ 8.9179e-03],\n",
       "        [ 3.1781e-02],\n",
       "        [-3.5200e-02],\n",
       "        [-4.4079e-02],\n",
       "        [ 2.8045e-02],\n",
       "        [-1.3127e-02],\n",
       "        [-3.8881e-02],\n",
       "        [-3.6146e-02],\n",
       "        [ 3.8445e-02],\n",
       "        [-8.3816e-04],\n",
       "        [-1.7244e-02],\n",
       "        [ 2.2187e-02],\n",
       "        [-5.0735e-03],\n",
       "        [-3.7124e-02],\n",
       "        [-2.8825e-02],\n",
       "        [ 6.7829e-02],\n",
       "        [ 1.9125e-03],\n",
       "        [-7.0438e-02],\n",
       "        [-3.6415e-02],\n",
       "        [ 1.3510e-02],\n",
       "        [-2.9915e-02],\n",
       "        [ 2.1259e-02],\n",
       "        [ 4.5411e-02],\n",
       "        [-9.1467e-03],\n",
       "        [-6.0433e-04],\n",
       "        [ 1.1035e-02],\n",
       "        [ 3.4589e-03],\n",
       "        [ 3.5402e-02],\n",
       "        [ 5.1617e-02],\n",
       "        [ 5.4413e-04],\n",
       "        [-1.4339e-02],\n",
       "        [-7.1636e-03],\n",
       "        [-2.5657e-02],\n",
       "        [-7.0658e-02],\n",
       "        [ 3.1850e-02],\n",
       "        [-1.3719e-02],\n",
       "        [ 6.2450e-03],\n",
       "        [ 2.0570e-02],\n",
       "        [ 7.2089e-03],\n",
       "        [-1.0320e-03],\n",
       "        [ 2.3504e-02],\n",
       "        [ 1.6557e-02],\n",
       "        [ 7.0257e-02],\n",
       "        [ 1.4872e-02],\n",
       "        [ 1.0674e-02],\n",
       "        [ 7.8504e-02],\n",
       "        [ 5.3571e-02],\n",
       "        [ 9.3399e-02],\n",
       "        [-8.3333e-04],\n",
       "        [-1.7740e-02],\n",
       "        [ 5.8622e-03],\n",
       "        [-4.5871e-02],\n",
       "        [ 6.5135e-03],\n",
       "        [-4.1578e-03],\n",
       "        [-4.7622e-02],\n",
       "        [-1.1819e-01],\n",
       "        [ 1.1375e-03],\n",
       "        [-2.1832e-02],\n",
       "        [ 4.6926e-02],\n",
       "        [-7.0044e-02],\n",
       "        [-1.2954e-02],\n",
       "        [-7.8622e-02],\n",
       "        [-1.0723e-02],\n",
       "        [ 5.5091e-02],\n",
       "        [-5.5728e-02],\n",
       "        [-4.8960e-02],\n",
       "        [ 3.3650e-02],\n",
       "        [ 1.2527e-02],\n",
       "        [-1.9136e-02],\n",
       "        [-2.7885e-02],\n",
       "        [ 4.5676e-02],\n",
       "        [-1.7764e-02],\n",
       "        [-8.5754e-03],\n",
       "        [-6.1103e-02],\n",
       "        [ 6.8677e-02],\n",
       "        [-1.9620e-02],\n",
       "        [-8.5446e-03],\n",
       "        [ 9.0780e-03],\n",
       "        [ 2.6032e-02],\n",
       "        [ 3.8848e-02],\n",
       "        [-1.4468e-02],\n",
       "        [ 5.8046e-02],\n",
       "        [ 1.5056e-03],\n",
       "        [-3.1513e-02],\n",
       "        [-5.5346e-02],\n",
       "        [-1.6980e-02],\n",
       "        [-9.2095e-03],\n",
       "        [-7.5902e-02],\n",
       "        [ 3.0138e-02],\n",
       "        [-9.1320e-02],\n",
       "        [ 1.2783e-02],\n",
       "        [-7.0201e-02],\n",
       "        [ 5.0628e-02],\n",
       "        [-5.1221e-02],\n",
       "        [ 4.2319e-02],\n",
       "        [-6.6248e-02],\n",
       "        [-6.8693e-02],\n",
       "        [ 6.5595e-03],\n",
       "        [ 2.5752e-02],\n",
       "        [ 1.0017e-02],\n",
       "        [-4.2288e-02],\n",
       "        [ 4.2712e-02],\n",
       "        [-1.9311e-02],\n",
       "        [ 2.8544e-02],\n",
       "        [ 3.8534e-02],\n",
       "        [-1.4316e-02],\n",
       "        [-2.3179e-02],\n",
       "        [ 2.9307e-02],\n",
       "        [ 2.6928e-02],\n",
       "        [-9.6546e-03],\n",
       "        [-6.0433e-04],\n",
       "        [ 1.9682e-02],\n",
       "        [ 1.6159e-02],\n",
       "        [ 6.5598e-02],\n",
       "        [ 3.9753e-03],\n",
       "        [-1.0073e-03],\n",
       "        [ 7.1389e-02],\n",
       "        [ 1.2492e-02],\n",
       "        [ 6.7238e-03],\n",
       "        [ 2.3763e-02],\n",
       "        [-1.3582e-02],\n",
       "        [-6.1686e-02],\n",
       "        [-2.3118e-02],\n",
       "        [-2.2776e-02],\n",
       "        [ 3.1028e-02],\n",
       "        [-2.0426e-02],\n",
       "        [ 1.2907e-02],\n",
       "        [-5.8478e-02],\n",
       "        [ 6.3605e-02],\n",
       "        [ 2.3249e-02],\n",
       "        [-1.9048e-02],\n",
       "        [ 1.8567e-03],\n",
       "        [-9.2237e-02],\n",
       "        [-3.5761e-02],\n",
       "        [-1.8763e-02],\n",
       "        [-2.0641e-02],\n",
       "        [ 1.1824e-02],\n",
       "        [ 5.4028e-03],\n",
       "        [ 2.0265e-02],\n",
       "        [ 3.1763e-02],\n",
       "        [ 2.6675e-02],\n",
       "        [-2.2475e-02],\n",
       "        [-1.6687e-02],\n",
       "        [ 2.0560e-02],\n",
       "        [-2.9051e-02],\n",
       "        [ 6.1763e-02],\n",
       "        [-9.3280e-02],\n",
       "        [-6.3098e-02],\n",
       "        [-9.5439e-04],\n",
       "        [-7.0268e-03],\n",
       "        [-1.3507e-02],\n",
       "        [-1.0671e-02],\n",
       "        [-4.3372e-02],\n",
       "        [ 1.2343e-02],\n",
       "        [-3.8440e-02],\n",
       "        [-2.1691e-02],\n",
       "        [-1.4785e-02],\n",
       "        [-2.5596e-02],\n",
       "        [ 4.2026e-02],\n",
       "        [-3.8593e-02],\n",
       "        [ 1.5683e-02],\n",
       "        [-2.6582e-02],\n",
       "        [ 2.5359e-02],\n",
       "        [ 5.6561e-02],\n",
       "        [ 1.6609e-02],\n",
       "        [-2.3264e-02],\n",
       "        [-6.4092e-02],\n",
       "        [ 8.6801e-02],\n",
       "        [ 1.7716e-02],\n",
       "        [ 5.5892e-02],\n",
       "        [ 5.6011e-02],\n",
       "        [-1.3154e-02],\n",
       "        [ 1.3578e-01],\n",
       "        [-4.6884e-02],\n",
       "        [ 2.8948e-02],\n",
       "        [-8.3677e-03],\n",
       "        [-2.1483e-02],\n",
       "        [ 1.8584e-03],\n",
       "        [ 2.2664e-02],\n",
       "        [ 2.6735e-02],\n",
       "        [ 2.9801e-03],\n",
       "        [ 1.5743e-02],\n",
       "        [-3.9872e-02],\n",
       "        [ 1.0312e-02],\n",
       "        [ 2.7664e-02],\n",
       "        [ 1.2381e-01],\n",
       "        [ 3.8258e-02],\n",
       "        [ 5.1113e-03],\n",
       "        [ 1.3167e-02],\n",
       "        [-3.1559e-02],\n",
       "        [-2.1351e-02],\n",
       "        [-7.6271e-03],\n",
       "        [ 2.3874e-02],\n",
       "        [ 4.5747e-02],\n",
       "        [ 1.8888e-02],\n",
       "        [-1.4785e-02],\n",
       "        [ 2.2045e-02],\n",
       "        [ 1.0920e-01],\n",
       "        [ 7.5567e-02],\n",
       "        [-8.2417e-03],\n",
       "        [ 2.0001e-03],\n",
       "        [ 5.0437e-02],\n",
       "        [ 1.8888e-02],\n",
       "        [-1.1573e-02],\n",
       "        [ 1.2984e-01],\n",
       "        [ 1.6767e-02],\n",
       "        [-1.2567e-02],\n",
       "        [ 6.8216e-03],\n",
       "        [-6.3095e-02],\n",
       "        [-6.5941e-04],\n",
       "        [-1.2643e-02],\n",
       "        [-1.5458e-02],\n",
       "        [ 1.3406e-02],\n",
       "        [ 3.0311e-03],\n",
       "        [-5.2327e-02],\n",
       "        [ 2.5418e-02],\n",
       "        [-3.8903e-02],\n",
       "        [-3.1448e-02],\n",
       "        [-5.8082e-02],\n",
       "        [ 6.0241e-02],\n",
       "        [-4.3312e-02],\n",
       "        [-6.8344e-02],\n",
       "        [-7.0820e-02],\n",
       "        [-2.2509e-02],\n",
       "        [ 3.3481e-03],\n",
       "        [ 4.1252e-02],\n",
       "        [ 2.7020e-02],\n",
       "        [-4.2081e-02],\n",
       "        [ 2.2127e-02],\n",
       "        [-1.2518e-02],\n",
       "        [-8.4367e-02],\n",
       "        [-9.9627e-02],\n",
       "        [ 3.2974e-03],\n",
       "        [ 7.5567e-02],\n",
       "        [-1.4063e-02],\n",
       "        [ 5.6152e-02],\n",
       "        [ 2.5359e-02],\n",
       "        [-3.8440e-02],\n",
       "        [ 1.6867e-02],\n",
       "        [-1.1910e-03],\n",
       "        [ 1.0130e-02],\n",
       "        [-7.8540e-02],\n",
       "        [-8.9376e-03],\n",
       "        [-1.5210e-02],\n",
       "        [-1.2700e-03],\n",
       "        [ 5.5948e-02],\n",
       "        [-1.5503e-02],\n",
       "        [ 3.1271e-03],\n",
       "        [-8.2417e-03],\n",
       "        [ 3.7700e-02],\n",
       "        [ 2.0265e-02],\n",
       "        [-3.5453e-03],\n",
       "        [-3.7379e-02],\n",
       "        [-1.8105e-02],\n",
       "        [-1.1267e-02],\n",
       "        [ 2.8434e-02],\n",
       "        [ 1.0757e-02],\n",
       "        [ 6.3135e-02],\n",
       "        [-4.6072e-03],\n",
       "        [ 3.5327e-02],\n",
       "        [ 3.5197e-02],\n",
       "        [-4.2725e-02],\n",
       "        [ 9.3324e-03],\n",
       "        [ 4.5252e-02],\n",
       "        [-4.2615e-02],\n",
       "        [-3.0062e-03],\n",
       "        [-2.9802e-05],\n",
       "        [ 2.9564e-02],\n",
       "        [-1.1878e-02],\n",
       "        [ 2.1511e-02],\n",
       "        [ 1.3464e-02],\n",
       "        [ 2.5095e-02],\n",
       "        [ 1.6139e-02],\n",
       "        [ 6.6210e-03],\n",
       "        [ 1.1328e-01],\n",
       "        [-1.6941e-03],\n",
       "        [ 8.7498e-02],\n",
       "        [ 1.1456e-01],\n",
       "        [-1.1972e-03],\n",
       "        [-3.2631e-02],\n",
       "        [-6.0554e-03],\n",
       "        [ 1.0004e-01],\n",
       "        [-8.2431e-02],\n",
       "        [-6.1730e-02],\n",
       "        [ 2.9699e-02],\n",
       "        [ 3.7920e-02],\n",
       "        [ 7.3631e-03],\n",
       "        [ 4.9812e-02],\n",
       "        [-8.8246e-03],\n",
       "        [ 1.1032e-01],\n",
       "        [ 3.2254e-02],\n",
       "        [-2.2761e-02],\n",
       "        [ 2.0653e-02],\n",
       "        [-2.6296e-02],\n",
       "        [ 1.6283e-02],\n",
       "        [ 4.8768e-02],\n",
       "        [-7.1299e-03],\n",
       "        [-1.5723e-01],\n",
       "        [-1.8469e-03],\n",
       "        [-1.2494e-02],\n",
       "        [ 1.3058e-02],\n",
       "        [ 4.0527e-02],\n",
       "        [ 1.2665e-02],\n",
       "        [-5.5199e-03],\n",
       "        [-1.0263e-02],\n",
       "        [ 3.8217e-02],\n",
       "        [-4.1277e-02],\n",
       "        [-2.2135e-02],\n",
       "        [ 3.0632e-02],\n",
       "        [-1.2533e-02],\n",
       "        [-3.3638e-03],\n",
       "        [-1.0568e-02],\n",
       "        [-3.5990e-03],\n",
       "        [-3.9872e-02],\n",
       "        [ 9.6276e-03],\n",
       "        [ 7.2740e-02],\n",
       "        [ 1.3571e-02],\n",
       "        [ 4.6481e-03],\n",
       "        [ 3.0862e-02],\n",
       "        [-6.3059e-02],\n",
       "        [ 4.4875e-02],\n",
       "        [-1.4370e-02],\n",
       "        [-9.1307e-02],\n",
       "        [-2.2687e-02],\n",
       "        [-3.4419e-02],\n",
       "        [ 3.0368e-02],\n",
       "        [-2.4081e-02],\n",
       "        [-1.1945e-02],\n",
       "        [-1.6670e-02],\n",
       "        [ 2.2873e-02],\n",
       "        [ 2.6922e-02],\n",
       "        [-2.6898e-02],\n",
       "        [-7.1069e-02],\n",
       "        [ 6.6130e-03],\n",
       "        [ 1.2337e-01],\n",
       "        [ 1.5583e-02],\n",
       "        [-2.2828e-02],\n",
       "        [-6.0433e-04],\n",
       "        [ 2.1161e-02],\n",
       "        [ 8.2069e-02],\n",
       "        [ 6.9603e-02],\n",
       "        [ 3.5860e-02],\n",
       "        [ 9.8763e-03],\n",
       "        [ 1.4118e-02],\n",
       "        [-3.9446e-02],\n",
       "        [ 4.2319e-02],\n",
       "        [ 6.0966e-03],\n",
       "        [-7.7060e-03],\n",
       "        [-8.4425e-03],\n",
       "        [ 4.1785e-02],\n",
       "        [-2.1512e-02],\n",
       "        [-2.2145e-03],\n",
       "        [ 6.8634e-02],\n",
       "        [-6.6980e-02],\n",
       "        [-2.2198e-03],\n",
       "        [ 1.7272e-02],\n",
       "        [-1.3155e-02],\n",
       "        [-3.0226e-02],\n",
       "        [-5.1508e-02],\n",
       "        [ 6.6503e-02],\n",
       "        [ 3.0490e-02],\n",
       "        [ 6.7755e-02],\n",
       "        [ 2.4581e-02],\n",
       "        [ 5.4071e-02],\n",
       "        [ 1.4309e-02],\n",
       "        [-4.5344e-02],\n",
       "        [ 1.7091e-02],\n",
       "        [-5.0519e-03],\n",
       "        [-1.5723e-01],\n",
       "        [-1.2809e-02],\n",
       "        [-6.0872e-02],\n",
       "        [-1.7928e-03],\n",
       "        [ 3.6494e-02],\n",
       "        [ 7.2482e-02],\n",
       "        [-2.8439e-03],\n",
       "        [-6.3216e-02],\n",
       "        [-2.2796e-02],\n",
       "        [ 5.2366e-02],\n",
       "        [-2.1793e-02],\n",
       "        [-4.7129e-02],\n",
       "        [ 9.0131e-03],\n",
       "        [ 3.4581e-02],\n",
       "        [-1.0661e-02],\n",
       "        [ 6.1485e-02],\n",
       "        [-3.1729e-02],\n",
       "        [ 7.6893e-02],\n",
       "        [-1.0411e-01],\n",
       "        [-6.3517e-03],\n",
       "        [-3.8881e-02],\n",
       "        [-2.9204e-02],\n",
       "        [-2.8558e-02],\n",
       "        [ 6.3834e-03],\n",
       "        [-1.7288e-03],\n",
       "        [ 1.5089e-03],\n",
       "        [-9.0768e-02],\n",
       "        [ 1.2293e-01],\n",
       "        [-2.3515e-02],\n",
       "        [ 1.9442e-02],\n",
       "        [-5.8563e-02],\n",
       "        [ 1.3002e-02],\n",
       "        [ 9.7146e-02],\n",
       "        [ 1.6074e-02],\n",
       "        [ 2.5306e-03],\n",
       "        [-7.4179e-02],\n",
       "        [-1.9122e-02],\n",
       "        [ 4.8768e-02],\n",
       "        [-4.3047e-02],\n",
       "        [-6.5282e-02],\n",
       "        [-3.6612e-02],\n",
       "        [-3.5992e-02],\n",
       "        [-3.0223e-02],\n",
       "        [ 2.5630e-02],\n",
       "        [-1.3369e-02],\n",
       "        [ 6.0642e-02],\n",
       "        [-8.6194e-03],\n",
       "        [ 6.0377e-02],\n",
       "        [ 1.6530e-02],\n",
       "        [-9.9906e-02],\n",
       "        [ 2.2626e-02],\n",
       "        [ 1.5528e-02],\n",
       "        [ 6.2450e-03],\n",
       "        [ 8.9548e-02],\n",
       "        [-3.8440e-02],\n",
       "        [ 9.4479e-03],\n",
       "        [-5.2404e-03],\n",
       "        [ 1.0313e-01],\n",
       "        [-2.3933e-02],\n",
       "        [-3.5913e-02],\n",
       "        [-1.4565e-02],\n",
       "        [-1.7543e-02],\n",
       "        [-2.2102e-03],\n",
       "        [ 3.2488e-02],\n",
       "        [ 2.5826e-03],\n",
       "        [ 1.6637e-02],\n",
       "        [-6.2668e-03],\n",
       "        [-4.4087e-03],\n",
       "        [-1.9048e-02],\n",
       "        [ 3.3556e-02],\n",
       "        [ 6.6455e-03],\n",
       "        [-4.0621e-02],\n",
       "        [-4.5389e-02],\n",
       "        [ 1.2619e-02],\n",
       "        [-3.6447e-03],\n",
       "        [ 2.4255e-02],\n",
       "        [-1.0948e-03],\n",
       "        [-3.4843e-02],\n",
       "        [ 2.8961e-02],\n",
       "        [-2.1187e-02],\n",
       "        [-3.4883e-02],\n",
       "        [ 2.9914e-02],\n",
       "        [-1.0578e-01],\n",
       "        [-4.6161e-03],\n",
       "        [-6.7038e-02],\n",
       "        [-1.9177e-02],\n",
       "        [-1.9753e-02],\n",
       "        [-5.1359e-02],\n",
       "        [ 5.0900e-03],\n",
       "        [ 2.0034e-02],\n",
       "        [-1.0965e-02],\n",
       "        [-1.2214e-02],\n",
       "        [-1.2195e-01],\n",
       "        [ 6.1059e-04],\n",
       "        [-1.4266e-02],\n",
       "        [-1.5395e-01],\n",
       "        [-1.0738e-02],\n",
       "        [-1.3751e-03],\n",
       "        [-6.7121e-02],\n",
       "        [-1.2187e-02],\n",
       "        [-7.6703e-03],\n",
       "        [ 2.4694e-02],\n",
       "        [-2.2021e-02],\n",
       "        [ 3.3792e-02],\n",
       "        [-2.2510e-02],\n",
       "        [-8.9859e-03],\n",
       "        [ 3.0558e-02],\n",
       "        [ 4.4695e-02],\n",
       "        [-5.6187e-02],\n",
       "        [-8.4044e-02],\n",
       "        [ 2.8328e-02],\n",
       "        [-4.0316e-02],\n",
       "        [ 3.7099e-02],\n",
       "        [ 2.7831e-02],\n",
       "        [ 8.7498e-02],\n",
       "        [ 1.1421e-02],\n",
       "        [-2.2373e-02],\n",
       "        [ 2.7019e-03],\n",
       "        [-1.8485e-02],\n",
       "        [ 5.4906e-02],\n",
       "        [ 1.7583e-02],\n",
       "        [ 2.9744e-02],\n",
       "        [ 4.8768e-02],\n",
       "        [-6.7303e-02],\n",
       "        [-8.2740e-03],\n",
       "        [-3.0032e-02],\n",
       "        [ 8.6164e-03],\n",
       "        [-4.7905e-03],\n",
       "        [ 9.9265e-03],\n",
       "        [ 5.9602e-02],\n",
       "        [ 1.9454e-02],\n",
       "        [ 2.6667e-02],\n",
       "        [ 6.3911e-03],\n",
       "        [-3.0318e-02],\n",
       "        [-8.4180e-02],\n",
       "        [ 8.7963e-02],\n",
       "        [ 6.1208e-04],\n",
       "        [ 4.8768e-02],\n",
       "        [-1.5801e-01],\n",
       "        [ 2.9744e-02],\n",
       "        [-1.0013e-02],\n",
       "        [ 5.4361e-02],\n",
       "        [ 4.2764e-02],\n",
       "        [ 3.7830e-02],\n",
       "        [-3.8440e-02],\n",
       "        [ 2.1801e-02],\n",
       "        [-1.5503e-02],\n",
       "        [ 1.7778e-02],\n",
       "        [-4.3270e-02],\n",
       "        [ 5.6584e-02],\n",
       "        [-7.7506e-03],\n",
       "        [-3.8440e-02],\n",
       "        [ 5.8358e-02],\n",
       "        [-4.9944e-02],\n",
       "        [-5.8499e-02],\n",
       "        [ 2.3504e-02],\n",
       "        [-9.5686e-03],\n",
       "        [-8.8390e-03],\n",
       "        [-2.1599e-02],\n",
       "        [ 1.1494e-03],\n",
       "        [-3.3609e-02],\n",
       "        [-2.0380e-02],\n",
       "        [-1.2243e-02],\n",
       "        [-8.7732e-04],\n",
       "        [-6.4092e-02],\n",
       "        [ 5.6233e-03],\n",
       "        [ 2.3446e-02],\n",
       "        [ 5.0918e-02],\n",
       "        [ 2.6901e-03],\n",
       "        [-1.3412e-02],\n",
       "        [ 4.8867e-02],\n",
       "        [-2.4416e-02],\n",
       "        [-1.1581e-03],\n",
       "        [-3.1186e-02],\n",
       "        [ 1.8736e-02],\n",
       "        [-3.6101e-02],\n",
       "        [-3.0113e-02],\n",
       "        [ 2.1035e-02],\n",
       "        [ 1.1198e-02],\n",
       "        [-7.6855e-03],\n",
       "        [-1.3523e-02],\n",
       "        [-6.4151e-03],\n",
       "        [-1.8527e-02],\n",
       "        [ 2.4332e-02],\n",
       "        [ 4.7111e-02],\n",
       "        [ 6.0537e-02],\n",
       "        [ 3.1456e-03],\n",
       "        [-2.9432e-02],\n",
       "        [ 4.2775e-02],\n",
       "        [-1.9410e-02],\n",
       "        [-1.1532e-01],\n",
       "        [ 2.9494e-02],\n",
       "        [ 4.2388e-02],\n",
       "        [ 1.6561e-02],\n",
       "        [ 1.4748e-02],\n",
       "        [ 2.2237e-02],\n",
       "        [ 1.6899e-02],\n",
       "        [ 7.1590e-02],\n",
       "        [ 5.8747e-02],\n",
       "        [ 3.5295e-02],\n",
       "        [ 9.8178e-03],\n",
       "        [ 1.0582e-01],\n",
       "        [-1.7095e-02],\n",
       "        [-7.8850e-03],\n",
       "        [-2.7839e-03],\n",
       "        [ 1.9315e-02],\n",
       "        [-1.0717e-01],\n",
       "        [ 5.0271e-02],\n",
       "        [-1.4136e-02],\n",
       "        [ 1.6139e-02],\n",
       "        [-3.2617e-02],\n",
       "        [ 1.0999e-02],\n",
       "        [-5.2800e-03],\n",
       "        [ 3.5053e-02],\n",
       "        [-6.0316e-03],\n",
       "        [-1.5764e-02],\n",
       "        [ 2.1704e-02],\n",
       "        [ 3.6591e-02],\n",
       "        [-6.6076e-02],\n",
       "        [-1.0305e-03],\n",
       "        [ 8.1108e-03],\n",
       "        [-2.7237e-02],\n",
       "        [ 4.1659e-02],\n",
       "        [-2.2990e-03],\n",
       "        [-1.1790e-02],\n",
       "        [ 6.0733e-02],\n",
       "        [-6.0430e-03],\n",
       "        [-5.8082e-02],\n",
       "        [ 2.9091e-02],\n",
       "        [ 1.1201e-02],\n",
       "        [ 3.2874e-02],\n",
       "        [ 1.8270e-02],\n",
       "        [ 3.3803e-02],\n",
       "        [-9.7373e-02],\n",
       "        [-8.1796e-02],\n",
       "        [-1.3125e-01],\n",
       "        [-3.1320e-02],\n",
       "        [-1.9195e-02],\n",
       "        [ 3.1453e-02],\n",
       "        [-8.0386e-03],\n",
       "        [ 3.0368e-02],\n",
       "        [ 4.5505e-02],\n",
       "        [ 1.4309e-02],\n",
       "        [ 2.2015e-02],\n",
       "        [ 3.2367e-02],\n",
       "        [-2.0411e-02],\n",
       "        [ 2.3766e-02],\n",
       "        [ 2.5188e-02],\n",
       "        [ 5.0507e-03],\n",
       "        [ 3.7072e-02],\n",
       "        [-1.8617e-02],\n",
       "        [-5.0760e-02],\n",
       "        [ 1.5743e-02],\n",
       "        [ 1.3807e-02],\n",
       "        [-7.0438e-02],\n",
       "        [ 5.7305e-02],\n",
       "        [-2.2308e-02],\n",
       "        [ 1.7881e-02],\n",
       "        [-2.1530e-02],\n",
       "        [ 2.1393e-02],\n",
       "        [-7.2084e-02],\n",
       "        [-5.5353e-02],\n",
       "        [ 1.3367e-03],\n",
       "        [ 9.6347e-02],\n",
       "        [-8.6549e-02],\n",
       "        [ 3.3650e-02],\n",
       "        [ 2.9834e-02],\n",
       "        [-6.3605e-02],\n",
       "        [-5.7296e-02],\n",
       "        [ 6.7804e-03],\n",
       "        [-4.4934e-03],\n",
       "        [-1.1875e-02],\n",
       "        [ 4.7929e-03],\n",
       "        [ 1.0744e-01],\n",
       "        [ 2.3270e-02],\n",
       "        [ 3.4882e-02],\n",
       "        [ 3.6676e-03],\n",
       "        [ 2.2184e-03],\n",
       "        [ 9.6310e-03],\n",
       "        [-4.4763e-02],\n",
       "        [ 6.0537e-02],\n",
       "        [-1.4653e-02],\n",
       "        [ 2.2045e-02],\n",
       "        [-3.1116e-02],\n",
       "        [-3.0569e-02],\n",
       "        [-1.2433e-02],\n",
       "        [ 9.0586e-02],\n",
       "        [-3.9337e-02],\n",
       "        [ 3.1456e-03],\n",
       "        [-1.1581e-03],\n",
       "        [ 1.9916e-03],\n",
       "        [ 9.5631e-03],\n",
       "        [-1.7496e-02],\n",
       "        [ 5.0443e-02],\n",
       "        [ 4.0169e-03],\n",
       "        [ 1.5112e-02],\n",
       "        [-2.2687e-02],\n",
       "        [-1.4970e-02],\n",
       "        [ 2.2387e-02],\n",
       "        [ 4.4735e-02],\n",
       "        [-2.5068e-02],\n",
       "        [ 3.1754e-03],\n",
       "        [-3.6101e-02],\n",
       "        [ 2.5602e-02],\n",
       "        [ 2.8354e-02],\n",
       "        [-1.7764e-02],\n",
       "        [ 3.0939e-02],\n",
       "        [-3.8436e-03],\n",
       "        [ 3.1854e-03],\n",
       "        [-4.0012e-02],\n",
       "        [ 1.5255e-01]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.figure()\n",
    "\n",
    "plot.plot(return_history)\n",
    "plot.grid(True)\n",
    "plot.xlabel('# of plays x {}'.format(n_collect))\n",
    "plot.ylabel('Return over the episode of length {}'.format(max_len))\n",
    "\n",
    "plot.show()\n",
    "plot.savefig('return_log.pdf', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8b543fa33c12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# let the final policy play the pong longer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_one_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-654a7fed751b>\u001b[0m in \u001b[0;36mcollect_one_episode\u001b[0;34m(env, player, max_len, discount_factor, deterministic, rendering, verbose)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let the final policy play the pong longer\n",
    "player.eval()\n",
    "_, _, _, _, ret_ = collect_one_episode(env, player, max_len=1000000, deterministic=True, rendering=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
