{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Buffer, collect_one_episode, normalize_obs, copy_params, avg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResLinear(nn.Module):\n",
    "    def __init__(self, n_in, n_out, act=nn.ReLU()):\n",
    "        super(ResLinear, self).__init__()\n",
    "        self.act = act\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "        self.bn = nn.BatchNorm1d(n_out)\n",
    "        \n",
    "        assert(n_in == n_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.act(self.bn(self.linear(x)))\n",
    "        return h + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(nn.Module):\n",
    "    def __init__(self, n_in=128, n_hid=100, n_out=6):\n",
    "        super(Player, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid),\n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, n_out))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, obs, normalized=False):\n",
    "        if normalized:\n",
    "            return self.softmax(self.layers(obs))\n",
    "        else:\n",
    "            return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(nn.Module):\n",
    "    def __init__(self, n_in=128, n_act=6, n_hid=100):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid), \n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, n_act))\n",
    "    \n",
    "    def forward(self, obs, act):\n",
    "#         set_trace()\n",
    "        return self.layers(obs).gather(1, act.long())\n",
    "    \n",
    "    def value(self, obs):\n",
    "        return self.layers(obs).max(dim=1)\n",
    "    \n",
    "    def q(self, obs):\n",
    "        return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(nn.Module):\n",
    "    def __init__(self, n_in=128, n_hid=100):\n",
    "        super(Value, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, 1))\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-ram-v0')\n",
    "\n",
    "n_frames = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a policy\n",
    "player = Player(n_in=128*n_frames, n_hid=32, n_out=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a q estimator\n",
    "qnet = Qnet(n_in=128*n_frames, n_hid=32, n_act=6).to(device)\n",
    "qold = Qnet(n_in=128*n_frames, n_hid=32, n_act=6).to(device)\n",
    "copy_params(qnet, qold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a value estimator\n",
    "value = Value(n_in=128*n_frames, n_hid=32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimizers\n",
    "opt_player = Adam(player.parameters(), lr=0.0001)\n",
    "opt_q = Adam(qnet.parameters(), lr=0.0001)\n",
    "opt_value = Adam(value.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize replay buffer\n",
    "replay_buffer = Buffer(max_items=50000, n_frames=n_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid run -20.0 -20.0\n",
      "# plays 1 return -18.0 value_loss 0.037978701293468475 q_loss 0.06269843131303787 entropy inf\n",
      "Valid run -19.0 -19.9\n",
      "# plays 2 return -18.0 value_loss 0.03602129090577364 q_loss 0.06188742853701115 entropy 1.7147311011658115\n",
      "Valid run -11.0 -19.01\n",
      "# plays 3 return -17.9 value_loss 0.03370278406888247 q_loss 0.05830822192132474 entropy 1.7054967680587994\n",
      "Valid run -12.0 -18.309\n",
      "# plays 4 return -17.41 value_loss 0.03129968974366784 q_loss 0.05501360213756562 entropy 1.6936297229544723\n",
      "Valid run -20.0 -18.4781\n",
      "# plays 5 return -17.669 value_loss 0.029125739969685677 q_loss 0.052239061103761204 entropy 1.6863429728093475\n",
      "Valid run -20.0 -18.630290000000002\n",
      "# plays 6 return -17.9021 value_loss 0.027020549815483394 q_loss 0.04950471881613136 entropy 1.675384808308366\n",
      "Valid run -20.0 -18.767261\n",
      "# plays 7 return -18.111890000000002 value_loss 0.025016660476032647 q_loss 0.047008587578699 entropy 1.6586931915927248\n",
      "Valid run -20.0 -18.890534900000002\n",
      "# plays 8 return -18.300701000000004 value_loss 0.02313685080495812 q_loss 0.04452495958039016 entropy 1.6467575649652735\n",
      "Valid run -20.0 -19.001481410000004\n",
      "# plays 9 return -18.470630900000003 value_loss 0.021326621292567294 q_loss 0.042072395963628745 entropy 1.6156587165102168\n",
      "Valid run -20.0 -19.101333269000005\n",
      "# plays 10 return -18.723567810000006 value_loss 0.01969663838091735 q_loss 0.03988029739245146 entropy 1.5666525815430727\n",
      "Valid run -20.0 -19.191199942100006\n",
      "# plays 11 return -18.851211029000005 value_loss 0.018147302958755884 q_loss 0.03819200390712353 entropy 1.5243451811158066\n",
      "Valid run -20.0 -19.272079947890006\n",
      "# plays 12 return -18.966089926100004 value_loss 0.016768707951208228 q_loss 0.036392395518445364 entropy 1.493118286896963\n",
      "Valid run -20.0 -19.344871953101006\n",
      "# plays 13 return -19.069480933490006 value_loss 0.015545828599006183 q_loss 0.035290198614201 entropy 1.46025215270409\n",
      "Valid run -20.0 -19.410384757790904\n",
      "# plays 14 return -19.162532840141004 value_loss 0.014434038633424264 q_loss 0.033882312482596384 entropy 1.4454011885918139\n",
      "Valid run -20.0 -19.469346282011813\n",
      "# plays 15 return -19.246279556126904 value_loss 0.01341417042812844 q_loss 0.033054768858254806 entropy 1.429545739766492\n",
      "Valid run -20.0 -19.522411653810632\n",
      "# plays 16 return -19.321651600514215 value_loss 0.012496204572404681 q_loss 0.031573029817254114 entropy 1.4101027063544334\n",
      "Valid run -20.0 -19.570170488429568\n",
      "# plays 17 return -19.389486440462793 value_loss 0.011616752987341815 q_loss 0.030884689989189 entropy 1.3557661584834007\n",
      "Valid run -20.0 -19.61315343958661\n",
      "# plays 18 return -19.450537796416516 value_loss 0.01099351941368288 q_loss 0.03067156544551226 entropy 1.3923851092902197\n",
      "Valid run -20.0 -19.65183809562795\n",
      "# plays 19 return -19.505484016774865 value_loss 0.010236265746582802 q_loss 0.02985075448058521 entropy 1.43227076610669\n",
      "Valid run -20.0 -19.686654286065156\n",
      "# plays 20 return -19.554935615097378 value_loss 0.009513997042846015 q_loss 0.029786015495084008 entropy 1.4076526115441619\n",
      "Valid run -20.0 -19.717988857458643\n",
      "# plays 21 return -18.99944205358764 value_loss 0.008893307935897706 q_loss 0.02902701510735358 entropy 1.4262721198735417\n",
      "Valid run -17.0 -19.44618997171278\n",
      "# plays 22 return -19.099497848228875 value_loss 0.008382462468963145 q_loss 0.02893298280609851 entropy 1.4544953825409126\n",
      "Valid run -19.0 -19.4015709745415\n",
      "# plays 23 return -19.189548063405987 value_loss 0.007835434807401549 q_loss 0.028750633596240326 entropy 1.4541723072482657\n",
      "Valid run -19.0 -19.361413877087347\n",
      "# plays 24 return -19.27059325706539 value_loss 0.0072996028053939205 q_loss 0.028377965449266594 entropy 1.4043161593181899\n",
      "Valid run -20.0 -19.425272489378614\n",
      "# plays 25 return -19.143533931358853 value_loss 0.006832101124027061 q_loss 0.028046756186163094 entropy 1.33810824119004\n",
      "Valid run -20.0 -19.482745240440753\n",
      "# plays 26 return -18.92918053822297 value_loss 0.006427762890605946 q_loss 0.027378623588152826 entropy 1.324903447658197\n",
      "Valid run -19.0 -19.434470716396678\n",
      "# plays 27 return -18.836262484400674 value_loss 0.006109368629156488 q_loss 0.027326785314024778 entropy 1.283071729932551\n",
      "Valid run -20.0 -19.49102364475701\n",
      "# plays 28 return -18.952636235960608 value_loss 0.005928067587151174 q_loss 0.027288948443181478 entropy 1.388620212349897\n",
      "Valid run -20.0 -19.54192128028131\n",
      "# plays 29 return -19.057372612364546 value_loss 0.00562429406534944 q_loss 0.027105473493022173 entropy 1.382798859236362\n",
      "Valid run -20.0 -19.587729152253182\n",
      "# plays 30 return -19.151635351128093 value_loss 0.005319332346875793 q_loss 0.026192524185313985 entropy 1.303780799654398\n",
      "Valid run -20.0 -19.628956237027865\n",
      "# plays 31 return -19.236471816015285 value_loss 0.005045558679790192 q_loss 0.027265852526307886 entropy 1.273645945773484\n",
      "Valid run -20.0 -19.66606061332508\n",
      "# plays 32 return -19.312824634413758 value_loss 0.004811110572836602 q_loss 0.026540231147838343 entropy 1.2935933689186252\n",
      "Valid run -20.0 -19.699454551992574\n",
      "# plays 33 return -19.381542170972384 value_loss 0.004717707566683118 q_loss 0.02661955895075146 entropy 1.2436608218285705\n",
      "Valid run -20.0 -19.72950909679332\n",
      "# plays 34 return -19.443387953875146 value_loss 0.004444844797229431 q_loss 0.02606437132104287 entropy 1.3725434064719375\n",
      "Valid run -20.0 -19.75655818711399\n",
      "# plays 35 return -19.099049158487635 value_loss 0.004219187967457469 q_loss 0.025706425163245644 entropy 1.3295234698654843\n",
      "Valid run -20.0 -19.78090236840259\n",
      "# plays 36 return -18.989144242638872 value_loss 0.004022387392426815 q_loss 0.025590698848452966 entropy 1.349681442010647\n",
      "Valid run -20.0 -19.80281213156233\n",
      "# plays 37 return -18.990229818374985 value_loss 0.003871369169113296 q_loss 0.025818993796664597 entropy 1.3360181796409143\n",
      "Valid run -20.0 -19.8225309184061\n",
      "# plays 38 return -19.091206836537488 value_loss 0.0037887060956274783 q_loss 0.026265465602062454 entropy 1.320066272790522\n",
      "Valid run -20.0 -19.84027782656549\n",
      "# plays 39 return -19.08208615288374 value_loss 0.0036656069796039445 q_loss 0.025998638430043138 entropy 1.2617178264511932\n",
      "Valid run -20.0 -19.856250043908943\n",
      "# plays 40 return -19.173877537595367 value_loss 0.0035171824012998148 q_loss 0.02596992543381817 entropy 1.270152889804119\n",
      "Valid run -20.0 -19.87062503951805\n",
      "# plays 41 return -19.256489783835832 value_loss 0.00349535021206469 q_loss 0.026321149611851873 entropy 1.2687941476028275\n",
      "Valid run -20.0 -19.883562535566245\n",
      "# plays 42 return -19.430840805452252 value_loss 0.0033790452321982144 q_loss 0.02583933144305465 entropy 1.2364061274847404\n",
      "Valid run -20.0 -19.89520628200962\n",
      "# plays 43 return -19.487756724907026 value_loss 0.003225579156446376 q_loss 0.02549609093660996 entropy 1.2858953736773147\n",
      "Valid run -20.0 -19.90568565380866\n",
      "# plays 44 return -19.638981052416327 value_loss 0.0031043824015270934 q_loss 0.02613004899437623 entropy 1.1878106711453644\n",
      "Valid run -20.0 -19.915117088427795\n",
      "# plays 45 return -19.675082947174694 value_loss 0.0030067546758691605 q_loss 0.0263433037526792 entropy 1.2759038533742821\n",
      "Valid run -20.0 -19.923605379585016\n",
      "# plays 46 return -19.707574652457225 value_loss 0.0029965758789232493 q_loss 0.025881458185081398 entropy 1.3288073824899218\n",
      "Valid run -20.0 -19.931244841626516\n",
      "# plays 47 return -19.736817187211503 value_loss 0.00296459867618094 q_loss 0.025770560938386367 entropy 1.1657719881813304\n",
      "Valid run -20.0 -19.938120357463866\n",
      "# plays 48 return -19.763135468490354 value_loss 0.002850540465201677 q_loss 0.025634161752404197 entropy 1.2568683295360539\n",
      "Valid run -20.0 -19.94430832171748\n",
      "# plays 49 return -19.78682192164132 value_loss 0.002769204896541602 q_loss 0.025629413162417493 entropy 1.2888574386385692\n",
      "Valid run -20.0 -19.94987748954573\n",
      "# plays 50 return -19.808139729477187 value_loss 0.0026930789509584012 q_loss 0.02574036619938667 entropy 1.2978863530655065\n",
      "Valid run -20.0 -19.95488974059116\n",
      "# plays 51 return -19.82732575652947 value_loss 0.0027114889859038145 q_loss 0.026222180740927952 entropy 1.2549075504553437\n",
      "Valid run -20.0 -19.959400766532045\n",
      "# plays 52 return -19.844593180876522 value_loss 0.002698639143754025 q_loss 0.026227711476590008 entropy 1.1040784180823602\n",
      "Valid run -20.0 -19.96346068987884\n",
      "# plays 53 return -19.86013386278887 value_loss 0.0026171812157882567 q_loss 0.02635055303964535 entropy 0.9934222983692138\n",
      "Valid run -20.0 -19.967114620890957\n",
      "# plays 54 return -19.874120476509983 value_loss 0.0025669753683182583 q_loss 0.026718852106274837 entropy 1.041900913037203\n",
      "Valid run -20.0 -19.97040315880186\n",
      "# plays 55 return -19.886708428858984 value_loss 0.002763148846781819 q_loss 0.026638481110520668 entropy 1.0962522120892166\n",
      "Valid run -20.0 -19.973362842921677\n",
      "# plays 56 return -19.898037585973086 value_loss 0.0028256500926374514 q_loss 0.025853372334111488 entropy 0.9360199351175682\n",
      "Valid run -20.0 -19.97602655862951\n",
      "# plays 57 return -19.908233827375778 value_loss 0.0028833806663834264 q_loss 0.02632781160056778 entropy 0.9922850019437964\n",
      "Valid run -21.0 -20.07842390276656\n",
      "# plays 58 return -19.9174104446382 value_loss 0.002880251779600967 q_loss 0.025857301247240796 entropy 0.9985084180917847\n",
      "Valid run -20.0 -20.070581512489905\n",
      "# plays 59 return -19.92566940017438 value_loss 0.0028330955370023747 q_loss 0.026056144595553006 entropy 0.9791245019521873\n",
      "Valid run -20.0 -20.063523361240915\n",
      "# plays 60 return -19.933102460156945 value_loss 0.0028432716638971778 q_loss 0.02607029359973329 entropy 1.0412139275570085\n",
      "Valid run -20.0 -20.057171025116823\n",
      "# plays 61 return -19.939792214141253 value_loss 0.0028931511168074084 q_loss 0.026093316769035375 entropy 0.9778184184839702\n",
      "Valid run -20.0 -20.05145392260514\n",
      "# plays 62 return -19.945812992727127 value_loss 0.0028462249753452664 q_loss 0.025973705004541624 entropy 0.9401394081139571\n",
      "Valid run -20.0 -20.04630853034463\n",
      "# plays 63 return -19.951231693454414 value_loss 0.0028521590092102284 q_loss 0.025454274051685825 entropy 0.9897809541757785\n",
      "Valid run -20.0 -20.041677677310165\n",
      "# plays 64 return -19.956108524108974 value_loss 0.0029505415588605044 q_loss 0.02523218876120421 entropy 0.9848044953320265\n",
      "Valid run -20.0 -20.03750990957915\n",
      "# plays 65 return -19.960497671698075 value_loss 0.00300592164292821 q_loss 0.025149629028114434 entropy 0.9929226923935055\n",
      "Valid run -20.0 -20.033758918621235\n",
      "# plays 66 return -19.964447904528267 value_loss 0.00317652813057502 q_loss 0.024660563181167324 entropy 0.8883814523212515\n",
      "Valid run -20.0 -20.03038302675911\n",
      "# plays 67 return -19.96800311407544 value_loss 0.0032762652244603626 q_loss 0.024259688408963812 entropy 0.9416904112157627\n",
      "Valid run -20.0 -20.0273447240832\n",
      "# plays 68 return -19.971202802667896 value_loss 0.003571073848957061 q_loss 0.023761748777442814 entropy 0.9329290591955502\n",
      "Valid run -20.0 -20.02461025167488\n",
      "# plays 69 return -19.97408252240111 value_loss 0.0040730460671671385 q_loss 0.023544656230461872 entropy 0.8438975917824636\n",
      "Valid run -20.0 -20.02214922650739\n",
      "# plays 70 return -19.976674270161 value_loss 0.004193779334478512 q_loss 0.02365594069819356 entropy 0.8174377073385158\n",
      "Valid run -20.0 -20.01993430385665\n",
      "# plays 71 return -19.479006843144898 value_loss 0.004210626096325756 q_loss 0.023708408532983408 entropy 0.8243084292000884\n",
      "Valid run -20.0 -20.017940873470987\n",
      "# plays 72 return -19.531106158830408 value_loss 0.0044930001817324525 q_loss 0.022965277768772552 entropy 0.7858237964806175\n",
      "Valid run -20.0 -20.01614678612389\n",
      "# plays 73 return -19.577995542947367 value_loss 0.004695841613532195 q_loss 0.022447809312583545 entropy 0.8097080585279484\n",
      "Valid run -20.0 -20.014532107511503\n",
      "# plays 74 return -19.62019598865263 value_loss 0.004922774793017884 q_loss 0.02194104990408558 entropy 0.7063176137494422\n",
      "Valid run -20.0 -20.01307889676035\n",
      "# plays 75 return -19.658176389787368 value_loss 0.00504118112554459 q_loss 0.021791254961899566 entropy 0.7626037466669984\n",
      "Valid run -20.0 -20.011771007084317\n",
      "# plays 76 return -19.69235875080863 value_loss 0.00504170741436161 q_loss 0.0214226417849168 entropy 0.6915074006018656\n",
      "Valid run -20.0 -20.010593906375885\n",
      "# plays 77 return -19.723122875727768 value_loss 0.004917369553496934 q_loss 0.02097414212164624 entropy 0.752230759084665\n",
      "Valid run -20.0 -20.009534515738295\n",
      "# plays 78 return -19.75081058815499 value_loss 0.004838092921570388 q_loss 0.02064237544003734 entropy 0.8148719718970059\n",
      "Valid run -20.0 -20.008581064164467\n",
      "# plays 79 return -19.775729529339493 value_loss 0.005351140672769053 q_loss 0.020216462641033016 entropy 0.7296487803613549\n",
      "Valid run -20.0 -20.007722957748022\n",
      "# plays 80 return -19.798156576405546 value_loss 0.005406650148158583 q_loss 0.019925829324787096 entropy 0.5401540938838387\n",
      "Valid run -19.0 -19.90695066197322\n",
      "# plays 81 return -19.818340918764992 value_loss 0.005416805858381373 q_loss 0.019520324926375066 entropy 0.47216948612541576\n",
      "Valid run -20.0 -19.916255595775898\n",
      "# plays 82 return -19.336506826888492 value_loss 0.005416449580685302 q_loss 0.019182185979901595 entropy 0.586904277311104\n",
      "Valid run -20.0 -19.92463003619831\n",
      "# plays 83 return -19.402856144199642 value_loss 0.005632107944350843 q_loss 0.019043608406679787 entropy 0.6682970972416424\n",
      "Valid run -20.0 -19.93216703257848\n",
      "# plays 84 return -19.46257052977968 value_loss 0.005564227021689479 q_loss 0.0193471636097106 entropy 0.6444875195721079\n",
      "Valid run -20.0 -19.938950329320633\n",
      "# plays 85 return -19.516313476801713 value_loss 0.005769424647350845 q_loss 0.018775662192297517 entropy 0.5699814521405456\n",
      "Valid run -20.0 -19.94505529638857\n",
      "# plays 86 return -19.564682129121543 value_loss 0.005611495198541645 q_loss 0.018576087981694227 entropy 0.6114726741669946\n",
      "Valid run -20.0 -19.950549766749713\n",
      "# plays 87 return -19.60821391620939 value_loss 0.005594352754637984 q_loss 0.018988278632074553 entropy 0.6127965830452212\n",
      "Valid run -20.0 -19.95549479007474\n",
      "# plays 88 return -19.64739252458845 value_loss 0.005418595813939322 q_loss 0.018224196918049416 entropy 0.6474482566258208\n",
      "Valid run -20.0 -19.95994531106727\n",
      "# plays 89 return -19.682653272129606 value_loss 0.005883636472281233 q_loss 0.01860460172942784 entropy 0.6687014252469462\n",
      "Valid run -20.0 -19.96395077996054\n",
      "# plays 90 return -19.714387944916645 value_loss 0.005909420877462401 q_loss 0.01841049035654819 entropy 0.5381310657463163\n",
      "Valid run -20.0 -19.96755570196449\n",
      "# plays 91 return -19.74294915042498 value_loss 0.005717838151916063 q_loss 0.017773506833759153 entropy 0.5636471863143935\n",
      "Valid run -16.0 -19.570800131768042\n",
      "# plays 92 return -19.76865423538248 value_loss 0.00595716142087954 q_loss 0.017620250286442523 entropy 0.48196346093944364\n",
      "Valid run -20.0 -19.61372011859124\n",
      "# plays 93 return -19.791788811844235 value_loss 0.0067654522442499795 q_loss 0.016935251293434934 entropy 0.573020572157374\n",
      "Valid run -20.0 -19.652348106732116\n",
      "# plays 94 return -19.81260993065981 value_loss 0.006698434501544834 q_loss 0.01750080590354285 entropy 0.45137740903095674\n",
      "Valid run -20.0 -19.687113296058904\n",
      "# plays 95 return -19.83134893759383 value_loss 0.006437397237559737 q_loss 0.017244601274939313 entropy 0.4084789007140528\n",
      "Valid run -20.0 -19.718401966453015\n",
      "# plays 96 return -19.84821404383445 value_loss 0.007191749742723031 q_loss 0.016627095420281828 entropy 0.45350434176821713\n",
      "Valid run -18.0 -19.546561769807717\n",
      "# plays 97 return -19.863392639451003 value_loss 0.00710360984117972 q_loss 0.015824953641701407 entropy 0.5605141948819802\n",
      "Valid run -13.0 -18.891905592826948\n",
      "# plays 98 return -19.877053375505902 value_loss 0.006876442152138237 q_loss 0.015657720369436145 entropy 0.5107703352960916\n",
      "Valid run -20.0 -19.002715033544252\n",
      "# plays 99 return -19.889348037955312 value_loss 0.00689610767658861 q_loss 0.014932547142956673 entropy 0.49739903784719874\n",
      "Valid run -20.0 -19.102443530189827\n",
      "# plays 100 return -20.000413234159783 value_loss 0.0066363019511518915 q_loss 0.014578058911133425 entropy 0.3632531063787539\n",
      "Valid run -20.0 -19.192199177170846\n",
      "# plays 101 return -20.000371910743805 value_loss 0.006789403842277948 q_loss 0.014311231215538338 entropy 0.363327358581236\n",
      "Valid run -20.0 -19.272979259453763\n",
      "# plays 102 return -19.800334719669426 value_loss 0.007056570964802168 q_loss 0.014012800024963993 entropy 0.49363328223915215\n",
      "Valid run -20.0 -19.345681333508388\n",
      "# plays 103 return -19.820301247702485 value_loss 0.007471876443399962 q_loss 0.01353542098061822 entropy 0.47993474846964485\n",
      "Valid run -20.0 -19.41111320015755\n",
      "# plays 104 return -18.638271122932238 value_loss 0.008145851192004065 q_loss 0.01328878891757055 entropy 0.3552270951379678\n",
      "Valid run -20.0 -19.470001880141798\n",
      "# plays 105 return -18.774444010639016 value_loss 0.008513827626818342 q_loss 0.013201817503192341 entropy 0.5295723377340138\n",
      "Valid run -20.0 -19.523001692127618\n",
      "# plays 106 return -18.596999609575114 value_loss 0.00849294590271764 q_loss 0.012945188810484096 entropy 0.3672532305327642\n",
      "Valid run -20.0 -19.570701522914856\n"
     ]
    }
   ],
   "source": [
    "n_iter = 1000\n",
    "init_collect = 1\n",
    "n_collect = 1\n",
    "n_q = 100\n",
    "n_value = 100\n",
    "n_policy = 100\n",
    "disp_iter = 1\n",
    "val_iter = 1\n",
    "\n",
    "max_len = 1000\n",
    "batch_size = 1000\n",
    "\n",
    "ent_coeff = 0. #0.001\n",
    "discount_factor = .95\n",
    "\n",
    "q_loss = -numpy.Inf\n",
    "value_loss = -numpy.Inf\n",
    "ret = -numpy.Inf\n",
    "entropy = -numpy.Inf\n",
    "valid_ret = -numpy.Inf\n",
    "\n",
    "return_history = []\n",
    "\n",
    "for ni in range(n_iter):\n",
    "    player.eval()\n",
    "\n",
    "    if numpy.mod(ni, val_iter) == 0:\n",
    "        _, _, _, _, _, ret_ = collect_one_episode(env, player, max_len=max_len, deterministic=True, n_frames=n_frames)\n",
    "        return_history.append(ret_)\n",
    "        if valid_ret == -numpy.Inf:\n",
    "            valid_ret = ret_\n",
    "        else:\n",
    "            valid_ret = 0.9 * valid_ret + 0.1 * ret_\n",
    "        print('Valid run', ret_, valid_ret)\n",
    "\n",
    "    # collect some episodes using the current policy\n",
    "    # and push (obs,a,r,p(a)) tuples to the replay buffer.\n",
    "    nc = n_collect\n",
    "    if ni == 0:\n",
    "        nc = init_collect\n",
    "    for ci in range(nc):\n",
    "        o_, r_, c_, a_, ap_, ret_ = collect_one_episode(env, player, max_len=max_len, discount_factor=discount_factor, n_frames=n_frames)\n",
    "        replay_buffer.add(o_, r_, c_, a_, ap_)\n",
    "        if ret == -numpy.Inf:\n",
    "            ret = ret_\n",
    "        else:\n",
    "            ret = 0.9 * ret + 0.1 * ret_\n",
    "    \n",
    "    # fit a q function\n",
    "    # TD learning: min_Q (Q(s,a) - (r + \\gamma \\max_a' Q(s',a')))^2\n",
    "    qnet.train()\n",
    "    for qi in range(n_q):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        opt_q.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        batch_x = torch.from_numpy(numpy.stack([ex['current']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_xn = torch.from_numpy(numpy.stack([ex['next']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        \n",
    "        batch_y = torch.from_numpy(numpy.stack([ex['current']['rew'] for ex in batch]).astype('float32')).to(device)\n",
    "        \n",
    "        batch_a = torch.from_numpy(numpy.stack([ex['current']['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        \n",
    "        q_pred = qnet(batch_x, batch_a).squeeze()\n",
    "        q_next = qold.value(batch_xn)[0].squeeze()\n",
    "        \n",
    "        loss_ = ((q_pred - (batch_y + discount_factor * q_next)) ** 2)\n",
    "\n",
    "        # since Q learning is off-policy, no need for importance weighting.\n",
    "        iw = 1.\n",
    "        loss = iw * loss_\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_q.step()\n",
    "        \n",
    "    if q_loss == -numpy.Inf:\n",
    "        q_loss = loss_.mean().item()\n",
    "    else:\n",
    "        q_loss = 0.9 * q_loss + 0.1 * loss_.mean().item()\n",
    "        \n",
    "    copy_params(qnet, qold)\n",
    "        \n",
    "    # fit a value function\n",
    "    # a usual value estimator: min_V (V(s) - Q(s,a))^2\n",
    "    qnet.eval()\n",
    "    value.train()\n",
    "    for vi in range(n_value):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        opt_q.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        batch_x = torch.from_numpy(numpy.stack([ex['current']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_r = torch.from_numpy(numpy.stack([ex['current']['crew'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        batch_a = torch.from_numpy(numpy.stack([ex['current']['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        \n",
    "        pred_y = value(batch_x).squeeze()\n",
    "        pred_q = qnet(batch_x, batch_a).squeeze().clone().detach()\n",
    "        \n",
    "        loss_ = ((pred_y - pred_q) ** 2)\n",
    "        \n",
    "        batch_pi = player(batch_x, normalized=True)\n",
    "        batch_q = torch.from_numpy(numpy.stack([ex['current']['prob'] for ex in batch]).astype('float32')).to(device)\n",
    "        logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "\n",
    "        # (clipped) importance weight: \n",
    "        # because the policy may have changed since the tuple was collected.\n",
    "        iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "    \n",
    "        loss = iw * loss_\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_value.step()\n",
    "        \n",
    "    if value_loss < 0.:\n",
    "        value_loss = loss_.mean().item()\n",
    "    else:\n",
    "        value_loss = 0.9 * value_loss + 0.1 * loss_.mean().item()\n",
    "    \n",
    "    if numpy.mod(ni, disp_iter) == 0:\n",
    "        print('# plays', (ni+1) * n_collect, 'return', ret, 'value_loss', value_loss, 'q_loss', q_loss, 'entropy', -entropy)\n",
    "    \n",
    "    # fit a policy\n",
    "    # advantage: (Q(a,s) - V(s))\n",
    "    qnet.eval()\n",
    "    value.eval()\n",
    "    player.train()\n",
    "    for pi in range(n_policy):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        opt_q.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        batch_x = torch.from_numpy(numpy.stack([ex['current']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_r = torch.from_numpy(numpy.stack([ex['current']['crew'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        batch_a = torch.from_numpy(numpy.stack([ex['current']['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "\n",
    "        batch_pi = player(batch_x, normalized=True)        \n",
    "        logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "        \n",
    "        batch_q = qnet(batch_x, batch_a)\n",
    "        batch_v = value(batch_x)\n",
    "        \n",
    "        adv = batch_q.clone().detach() - batch_v.clone().detach()\n",
    "        adv = adv / adv.abs().max().clamp(min=1.)\n",
    "        \n",
    "        loss = -(adv * logp)\n",
    "        \n",
    "        batch_q = torch.from_numpy(numpy.stack([ex['current']['prob'] for ex in batch]).astype('float32')).to(device)\n",
    "\n",
    "        # (clipped) importance weight: \n",
    "        # because the policy may have changed since the tuple was collected.\n",
    "        iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "    \n",
    "        loss = iw * loss\n",
    "        \n",
    "        # entropy regularization: though, it doesn't look necessary in this specific case.\n",
    "        ent = (batch_pi * torch.log(batch_pi)).sum(1)\n",
    "        \n",
    "        if entropy == -numpy.Inf:\n",
    "            entropy = ent.mean().item()\n",
    "        else:\n",
    "            entropy = 0.9 * entropy + 0.1 * ent.mean().item()\n",
    "        \n",
    "        loss = (loss + ent_coeff * ent).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_player.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.figure()\n",
    "\n",
    "plot.plot(return_history)\n",
    "plot.grid(True)\n",
    "plot.xlabel('# of plays x {}'.format(n_collect))\n",
    "plot.ylabel('Return over the episode of length {}'.format(max_len))\n",
    "\n",
    "plot.show()\n",
    "plot.savefig('return_log.pdf', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8b543fa33c12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# let the final policy play the pong longer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_one_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-654a7fed751b>\u001b[0m in \u001b[0;36mcollect_one_episode\u001b[0;34m(env, player, max_len, discount_factor, deterministic, rendering, verbose)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let the final policy play the pong longer\n",
    "player.eval()\n",
    "_, _, _, _, ret_ = collect_one_episode(env, player, max_len=1000000, deterministic=True, rendering=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
