{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Buffer, collect_one_episode, normalize_obs, copy_params, avg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResLinear(nn.Module):\n",
    "    def __init__(self, n_in, n_out, act=nn.ReLU()):\n",
    "        super(ResLinear, self).__init__()\n",
    "        self.act = act\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "        self.bn = nn.BatchNorm1d(n_out)\n",
    "        \n",
    "        assert(n_in == n_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.act(self.bn(self.linear(x)))\n",
    "        return h + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(nn.Module):\n",
    "    def __init__(self, n_in=128, n_hid=100, n_out=6):\n",
    "        super(Player, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid),\n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, n_out))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, obs, normalized=False):\n",
    "        if normalized:\n",
    "            return self.softmax(self.layers(obs))\n",
    "        else:\n",
    "            return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(nn.Module):\n",
    "    def __init__(self, n_in=128, n_act=6, n_hid=100):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid), \n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, n_act))\n",
    "    \n",
    "    def forward(self, obs, act):\n",
    "#         set_trace()\n",
    "        return self.layers(obs).gather(1, act.long())\n",
    "    \n",
    "    def value(self, obs):\n",
    "        return self.layers(obs).max(dim=1)\n",
    "    \n",
    "    def q(self, obs):\n",
    "        return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(nn.Module):\n",
    "    def __init__(self, n_in=128, n_hid=100):\n",
    "        super(Value, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid),\n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, 1))\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-ram-v0')\n",
    "\n",
    "n_frames = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a policy\n",
    "player = Player(n_in=128*n_frames, n_hid=128, n_out=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a q estimator\n",
    "qnet = Qnet(n_in=128*n_frames, n_hid=128, n_act=6).to(device)\n",
    "qold = Qnet(n_in=128*n_frames, n_hid=128, n_act=6).to(device)\n",
    "copy_params(qnet, qold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a value estimator\n",
    "value = Value(n_in=128*n_frames, n_hid=128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimizers\n",
    "opt_player = Adam(player.parameters(), lr=0.0001)\n",
    "opt_q = Adam(qnet.parameters(), lr=0.0001)\n",
    "opt_value = Adam(value.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize replay buffer\n",
    "replay_buffer = Buffer(max_items=50000, n_frames=n_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid run -20.0 -20.0\n",
      "0 6\n",
      "1 6\n",
      "2 6\n",
      "3 6\n",
      "4 5\n",
      "5 4\n",
      "6 4\n",
      "7 4\n",
      "8 4\n",
      "9 4\n",
      "10 4\n",
      "11 4\n",
      "12 4\n",
      "13 4\n",
      "14 4\n",
      "15 4\n",
      "16 4\n",
      "17 4\n",
      "18 4\n",
      "19 4\n",
      "20 4\n",
      "21 4\n",
      "22 4\n",
      "23 4\n",
      "24 4\n",
      "25 4\n",
      "26 4\n",
      "27 4\n",
      "28 4\n",
      "29 4\n",
      "30 4\n",
      "31 4\n",
      "32 4\n",
      "33 4\n",
      "34 4\n",
      "35 4\n",
      "36 4\n",
      "37 4\n",
      "38 4\n",
      "39 4\n",
      "40 4\n",
      "41 4\n",
      "42 4\n",
      "43 4\n",
      "44 4\n",
      "45 4\n",
      "46 4\n",
      "47 4\n",
      "48 4\n",
      "49 4\n",
      "50 4\n",
      "51 4\n",
      "52 4\n",
      "53 4\n",
      "54 4\n",
      "55 4\n",
      "56 4\n",
      "57 4\n",
      "58 4\n",
      "59 4\n",
      "60 4\n",
      "61 4\n",
      "62 4\n",
      "63 4\n",
      "64 4\n",
      "65 4\n",
      "66 4\n",
      "67 4\n",
      "68 4\n",
      "69 4\n",
      "70 4\n",
      "71 4\n",
      "72 4\n",
      "73 4\n",
      "74 4\n",
      "75 4\n",
      "76 4\n",
      "77 4\n",
      "78 4\n",
      "79 4\n",
      "80 4\n",
      "81 4\n",
      "82 4\n",
      "83 4\n",
      "84 4\n",
      "85 4\n",
      "86 4\n",
      "87 4\n",
      "88 4\n",
      "89 4\n",
      "90 4\n",
      "91 4\n",
      "92 4\n",
      "93 4\n",
      "94 4\n",
      "95 4\n",
      "96 4\n",
      "97 4\n",
      "98 4\n",
      "99 4\n",
      "100 4\n",
      "101 4\n",
      "102 4\n",
      "103 4\n",
      "104 4\n",
      "105 4\n",
      "106 4\n",
      "107 4\n",
      "108 4\n",
      "109 4\n",
      "110 4\n",
      "111 4\n",
      "112 4\n",
      "113 4\n",
      "114 4\n",
      "115 4\n",
      "116 4\n",
      "117 4\n",
      "118 4\n",
      "119 4\n",
      "120 4\n",
      "121 4\n",
      "122 4\n",
      "123 4\n",
      "124 4\n",
      "125 4\n",
      "126 4\n",
      "127 4\n",
      "128 4\n",
      "129 4\n",
      "130 4\n",
      "131 4\n",
      "132 4\n",
      "133 4\n",
      "134 4\n",
      "135 4\n",
      "136 4\n",
      "137 4\n",
      "138 4\n",
      "139 4\n",
      "140 4\n",
      "141 4\n",
      "142 4\n",
      "143 4\n",
      "144 4\n",
      "145 4\n",
      "146 4\n",
      "147 4\n",
      "148 4\n",
      "149 4\n",
      "150 4\n",
      "151 4\n",
      "152 4\n",
      "153 4\n",
      "154 4\n",
      "155 4\n",
      "156 4\n",
      "157 4\n",
      "158 4\n",
      "159 4\n",
      "160 4\n",
      "161 4\n",
      "162 4\n",
      "163 4\n",
      "164 4\n",
      "165 4\n",
      "166 4\n",
      "167 4\n",
      "168 4\n",
      "169 4\n",
      "170 4\n",
      "171 4\n",
      "172 4\n",
      "173 4\n",
      "174 4\n",
      "175 4\n",
      "176 4\n",
      "177 4\n",
      "178 4\n",
      "179 4\n",
      "180 4\n",
      "181 4\n",
      "182 4\n",
      "183 4\n",
      "184 4\n",
      "185 4\n",
      "186 4\n",
      "187 4\n",
      "188 4\n",
      "189 4\n",
      "190 4\n",
      "191 4\n",
      "192 4\n",
      "193 4\n",
      "194 4\n",
      "195 4\n",
      "196 4\n",
      "197 4\n",
      "198 4\n",
      "199 4\n",
      "200 4\n",
      "201 4\n",
      "202 4\n",
      "203 4\n",
      "204 4\n",
      "205 4\n",
      "206 4\n",
      "207 4\n",
      "208 4\n",
      "209 4\n",
      "210 4\n",
      "211 4\n",
      "212 4\n",
      "213 4\n",
      "214 4\n",
      "215 4\n",
      "216 4\n",
      "217 4\n",
      "218 4\n",
      "219 4\n",
      "220 4\n",
      "221 4\n",
      "222 4\n",
      "223 4\n",
      "224 4\n",
      "225 4\n",
      "226 4\n",
      "227 4\n",
      "228 4\n",
      "229 4\n",
      "230 4\n",
      "231 4\n",
      "232 4\n",
      "233 4\n",
      "234 4\n",
      "235 4\n",
      "236 4\n",
      "237 4\n",
      "238 4\n",
      "239 4\n",
      "240 4\n",
      "241 4\n",
      "242 4\n",
      "243 4\n",
      "244 4\n",
      "245 4\n",
      "246 4\n",
      "247 4\n",
      "248 4\n",
      "249 4\n",
      "250 4\n",
      "251 4\n",
      "252 4\n",
      "253 4\n",
      "254 4\n",
      "255 4\n",
      "256 4\n",
      "257 4\n",
      "258 4\n",
      "259 4\n",
      "260 4\n",
      "261 4\n",
      "262 4\n",
      "263 4\n",
      "264 4\n",
      "265 4\n",
      "266 4\n",
      "267 4\n",
      "268 4\n",
      "269 4\n",
      "270 4\n",
      "271 4\n",
      "272 4\n",
      "273 4\n",
      "274 4\n",
      "275 4\n",
      "276 4\n",
      "277 4\n",
      "278 4\n",
      "279 4\n",
      "280 4\n",
      "281 4\n",
      "282 4\n",
      "283 4\n",
      "284 4\n",
      "285 4\n",
      "286 4\n",
      "287 4\n",
      "288 4\n",
      "289 4\n",
      "290 4\n",
      "291 4\n",
      "292 4\n",
      "293 4\n",
      "294 4\n",
      "295 4\n",
      "296 4\n",
      "297 4\n",
      "298 4\n",
      "299 4\n",
      "300 4\n",
      "301 4\n",
      "302 4\n",
      "303 4\n",
      "304 4\n",
      "305 4\n",
      "306 4\n",
      "307 4\n",
      "308 4\n",
      "309 4\n",
      "310 4\n",
      "311 4\n",
      "312 4\n",
      "313 4\n",
      "314 4\n",
      "315 4\n",
      "316 4\n",
      "317 4\n",
      "318 4\n",
      "319 4\n",
      "320 4\n",
      "321 4\n",
      "322 4\n",
      "323 4\n",
      "324 4\n",
      "325 4\n",
      "326 4\n",
      "327 4\n",
      "328 4\n",
      "329 4\n",
      "330 4\n",
      "331 4\n",
      "332 4\n",
      "333 4\n",
      "334 4\n",
      "335 4\n",
      "336 4\n",
      "337 4\n",
      "338 4\n",
      "339 4\n",
      "340 4\n",
      "341 4\n",
      "342 4\n",
      "343 4\n",
      "344 4\n",
      "345 4\n",
      "346 4\n",
      "347 4\n",
      "348 4\n",
      "349 4\n",
      "350 4\n",
      "351 4\n",
      "352 4\n",
      "353 4\n",
      "354 4\n",
      "355 4\n",
      "356 4\n",
      "357 4\n",
      "358 4\n",
      "359 4\n",
      "360 4\n",
      "361 4\n",
      "362 4\n",
      "363 4\n",
      "364 4\n",
      "365 4\n",
      "366 4\n",
      "367 4\n",
      "368 4\n",
      "369 4\n",
      "370 4\n",
      "371 4\n",
      "372 4\n",
      "373 4\n",
      "374 4\n",
      "375 4\n",
      "376 4\n",
      "377 4\n",
      "378 4\n",
      "379 4\n",
      "380 4\n",
      "381 4\n",
      "382 4\n",
      "383 4\n",
      "384 4\n",
      "385 4\n",
      "386 4\n",
      "387 4\n",
      "388 4\n",
      "389 4\n",
      "390 4\n",
      "391 4\n",
      "392 4\n",
      "393 4\n",
      "394 4\n",
      "395 4\n",
      "396 4\n",
      "397 4\n",
      "398 4\n",
      "399 4\n",
      "400 4\n",
      "401 4\n",
      "402 4\n",
      "403 4\n",
      "404 4\n",
      "405 4\n",
      "406 4\n",
      "407 4\n",
      "408 4\n",
      "409 4\n",
      "410 4\n",
      "411 4\n",
      "412 4\n",
      "413 4\n",
      "414 4\n",
      "415 4\n",
      "416 4\n",
      "417 4\n",
      "418 4\n",
      "419 4\n",
      "420 4\n",
      "421 4\n",
      "422 4\n",
      "423 4\n",
      "424 4\n",
      "425 4\n",
      "426 4\n",
      "427 4\n",
      "428 4\n",
      "429 4\n",
      "430 4\n",
      "431 4\n",
      "432 4\n",
      "433 4\n",
      "434 4\n",
      "435 4\n",
      "436 4\n",
      "437 4\n",
      "438 4\n",
      "439 4\n",
      "440 4\n",
      "441 4\n",
      "442 4\n",
      "443 4\n",
      "444 4\n",
      "445 4\n",
      "446 4\n",
      "447 4\n",
      "448 4\n",
      "449 4\n",
      "450 4\n",
      "451 4\n",
      "452 4\n",
      "453 4\n",
      "454 4\n",
      "455 4\n",
      "456 4\n",
      "457 4\n",
      "458 4\n",
      "459 4\n",
      "460 4\n",
      "461 4\n",
      "462 4\n",
      "463 4\n",
      "464 4\n",
      "465 4\n",
      "466 4\n",
      "467 4\n",
      "468 4\n",
      "469 4\n",
      "470 4\n",
      "471 4\n",
      "472 4\n",
      "473 4\n",
      "474 4\n",
      "475 4\n",
      "476 4\n",
      "477 4\n",
      "478 4\n",
      "479 4\n",
      "480 4\n",
      "481 4\n",
      "482 4\n",
      "483 4\n",
      "484 4\n",
      "485 4\n",
      "486 4\n",
      "487 4\n",
      "488 4\n",
      "489 4\n",
      "490 4\n",
      "491 4\n",
      "492 4\n",
      "493 4\n",
      "494 4\n",
      "495 4\n",
      "496 4\n",
      "497 4\n",
      "498 4\n",
      "499 4\n",
      "500 4\n",
      "501 4\n",
      "502 4\n",
      "503 4\n",
      "504 4\n",
      "505 4\n",
      "506 4\n",
      "507 4\n",
      "508 4\n",
      "509 4\n",
      "510 4\n",
      "511 4\n",
      "512 4\n",
      "513 4\n",
      "514 4\n",
      "515 4\n",
      "516 4\n",
      "517 4\n",
      "518 4\n",
      "519 4\n",
      "520 4\n",
      "521 4\n",
      "522 4\n",
      "523 4\n",
      "524 4\n",
      "525 4\n",
      "526 4\n",
      "527 4\n",
      "528 4\n",
      "529 4\n",
      "530 4\n",
      "531 4\n",
      "532 4\n",
      "533 4\n",
      "534 4\n",
      "535 4\n",
      "536 4\n",
      "537 4\n",
      "538 4\n",
      "539 4\n",
      "540 4\n",
      "541 4\n",
      "542 4\n",
      "543 4\n",
      "544 4\n",
      "545 4\n",
      "546 4\n",
      "547 4\n",
      "548 4\n",
      "549 4\n",
      "550 4\n",
      "551 4\n",
      "552 4\n",
      "553 4\n",
      "554 4\n",
      "555 4\n",
      "556 4\n",
      "557 4\n",
      "558 4\n",
      "559 4\n",
      "560 4\n",
      "561 4\n",
      "562 4\n",
      "563 4\n",
      "564 4\n",
      "565 4\n",
      "566 4\n",
      "567 4\n",
      "568 4\n",
      "569 4\n",
      "570 4\n",
      "571 4\n",
      "572 4\n",
      "573 4\n",
      "574 4\n",
      "575 4\n",
      "576 4\n",
      "577 4\n",
      "578 4\n",
      "579 4\n",
      "580 4\n",
      "581 4\n",
      "582 4\n",
      "583 4\n",
      "584 4\n",
      "585 4\n",
      "586 4\n",
      "587 4\n",
      "588 4\n",
      "589 4\n",
      "590 4\n",
      "591 4\n",
      "592 4\n",
      "593 4\n",
      "594 4\n",
      "595 4\n",
      "596 4\n",
      "597 4\n",
      "598 4\n",
      "599 4\n",
      "600 4\n",
      "601 4\n",
      "602 4\n",
      "603 4\n",
      "604 4\n",
      "605 4\n",
      "606 4\n",
      "607 4\n",
      "608 4\n",
      "609 4\n",
      "610 4\n",
      "611 4\n",
      "612 4\n",
      "613 4\n",
      "614 4\n",
      "615 4\n",
      "616 4\n",
      "617 4\n",
      "618 4\n",
      "619 4\n",
      "620 4\n",
      "621 4\n",
      "622 4\n",
      "623 4\n",
      "624 4\n",
      "625 4\n",
      "626 4\n",
      "627 4\n",
      "628 4\n",
      "629 4\n",
      "630 4\n",
      "631 4\n",
      "632 4\n",
      "633 4\n",
      "634 4\n",
      "635 4\n",
      "636 4\n",
      "637 4\n",
      "638 4\n",
      "639 4\n",
      "640 4\n",
      "641 4\n",
      "642 4\n",
      "643 4\n",
      "644 4\n",
      "645 4\n",
      "646 4\n",
      "647 4\n",
      "648 4\n",
      "649 4\n",
      "650 4\n",
      "651 4\n",
      "652 4\n",
      "653 4\n",
      "654 4\n",
      "655 4\n",
      "656 4\n",
      "657 4\n",
      "658 4\n",
      "659 4\n",
      "660 4\n",
      "661 4\n",
      "662 4\n",
      "663 4\n",
      "664 4\n",
      "665 4\n",
      "666 4\n",
      "667 4\n",
      "668 4\n",
      "669 4\n",
      "670 4\n",
      "671 4\n",
      "672 4\n",
      "673 4\n",
      "674 4\n",
      "675 4\n",
      "676 4\n",
      "677 4\n",
      "678 4\n",
      "679 4\n",
      "680 4\n",
      "681 4\n",
      "682 4\n",
      "683 4\n",
      "684 4\n",
      "685 4\n",
      "686 4\n",
      "687 4\n",
      "688 4\n",
      "689 4\n",
      "690 4\n",
      "691 4\n",
      "692 4\n",
      "693 4\n",
      "694 4\n",
      "695 4\n",
      "696 4\n",
      "697 4\n",
      "698 4\n",
      "699 4\n",
      "700 4\n",
      "701 4\n",
      "702 4\n",
      "703 4\n",
      "704 4\n",
      "705 4\n",
      "706 4\n",
      "707 4\n",
      "708 4\n",
      "709 4\n",
      "710 4\n",
      "711 4\n",
      "712 4\n",
      "713 4\n",
      "714 4\n",
      "715 4\n",
      "716 4\n",
      "717 4\n",
      "718 4\n",
      "719 4\n",
      "720 4\n",
      "721 4\n",
      "722 4\n",
      "723 4\n",
      "724 4\n",
      "725 4\n",
      "726 4\n",
      "727 4\n",
      "728 4\n",
      "729 4\n",
      "730 4\n",
      "731 4\n",
      "732 4\n",
      "733 4\n",
      "734 4\n",
      "735 4\n",
      "736 4\n",
      "737 4\n",
      "738 4\n",
      "739 4\n",
      "740 4\n",
      "741 4\n",
      "742 4\n",
      "743 4\n",
      "744 4\n",
      "745 4\n",
      "746 4\n",
      "747 4\n",
      "748 4\n",
      "749 4\n",
      "750 4\n",
      "751 4\n",
      "752 4\n",
      "753 4\n",
      "754 4\n",
      "755 4\n",
      "756 4\n",
      "757 4\n",
      "758 4\n",
      "759 4\n",
      "760 4\n",
      "761 4\n",
      "762 4\n",
      "763 4\n",
      "764 4\n",
      "765 4\n",
      "766 4\n",
      "767 4\n",
      "768 4\n",
      "769 4\n",
      "770 4\n",
      "771 4\n",
      "772 4\n",
      "773 4\n",
      "774 4\n",
      "775 4\n",
      "776 4\n",
      "777 4\n",
      "778 4\n",
      "779 4\n",
      "780 4\n",
      "781 4\n",
      "782 4\n",
      "783 4\n",
      "784 4\n",
      "785 4\n",
      "786 4\n",
      "787 4\n",
      "788 4\n",
      "789 4\n",
      "790 4\n",
      "791 4\n",
      "792 4\n",
      "793 4\n",
      "794 4\n",
      "795 4\n",
      "796 4\n",
      "797 4\n",
      "798 4\n",
      "799 4\n",
      "800 4\n",
      "801 4\n",
      "802 4\n",
      "803 4\n",
      "804 4\n",
      "805 4\n",
      "806 4\n",
      "807 4\n",
      "808 4\n",
      "809 4\n",
      "810 4\n",
      "811 4\n",
      "812 4\n",
      "813 4\n",
      "814 4\n",
      "815 4\n",
      "816 4\n",
      "817 4\n",
      "818 4\n",
      "819 4\n",
      "820 4\n",
      "821 4\n",
      "822 4\n",
      "823 4\n",
      "824 4\n",
      "825 4\n",
      "826 4\n",
      "827 4\n",
      "828 4\n",
      "829 4\n",
      "830 4\n",
      "831 4\n",
      "832 4\n",
      "833 4\n",
      "834 4\n",
      "835 4\n",
      "836 4\n",
      "837 4\n",
      "838 4\n",
      "839 4\n",
      "840 4\n",
      "841 4\n",
      "842 4\n",
      "843 4\n",
      "844 4\n",
      "845 4\n",
      "846 4\n",
      "847 4\n",
      "848 4\n",
      "849 4\n",
      "850 4\n",
      "851 4\n",
      "852 4\n",
      "853 4\n",
      "854 4\n",
      "855 4\n",
      "856 4\n",
      "857 4\n",
      "858 4\n",
      "859 4\n",
      "860 4\n",
      "861 4\n",
      "862 4\n",
      "863 4\n",
      "864 4\n",
      "865 4\n",
      "866 4\n",
      "867 4\n",
      "868 4\n",
      "869 4\n",
      "870 4\n",
      "871 4\n",
      "872 4\n",
      "873 4\n",
      "874 4\n",
      "875 4\n",
      "876 4\n",
      "877 4\n",
      "878 4\n",
      "879 4\n",
      "880 4\n",
      "881 4\n",
      "882 4\n",
      "883 4\n",
      "884 4\n",
      "885 4\n",
      "886 4\n",
      "887 4\n",
      "888 4\n",
      "889 4\n",
      "890 4\n",
      "891 4\n",
      "892 4\n",
      "893 4\n",
      "894 4\n",
      "895 4\n",
      "896 4\n",
      "897 4\n",
      "898 4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4b80251e21f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'current'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'obs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mbatch_xn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'next'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'obs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'current'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rew'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all input arrays must have the same shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "n_iter = 1000\n",
    "init_collect = 1\n",
    "n_collect = 1\n",
    "n_q = 100\n",
    "n_value = 0\n",
    "n_policy = 100\n",
    "disp_iter = 1\n",
    "val_iter = 1\n",
    "\n",
    "max_len = 1000\n",
    "batch_size = 1000\n",
    "\n",
    "ent_coeff = 0. #0.001\n",
    "discount_factor = .95\n",
    "\n",
    "q_loss = -numpy.Inf\n",
    "value_loss = -numpy.Inf\n",
    "ret = -numpy.Inf\n",
    "entropy = -numpy.Inf\n",
    "valid_ret = -numpy.Inf\n",
    "\n",
    "return_history = []\n",
    "\n",
    "for ni in range(n_iter):\n",
    "    player.eval()\n",
    "\n",
    "    if numpy.mod(ni, val_iter) == 0:\n",
    "        _, _, _, _, _, ret_ = collect_one_episode(env, player, max_len=max_len, deterministic=True, n_frames=n_frames)\n",
    "        return_history.append(ret_)\n",
    "        if valid_ret == -numpy.Inf:\n",
    "            valid_ret = ret_\n",
    "        else:\n",
    "            valid_ret = 0.9 * valid_ret + 0.1 * ret_\n",
    "        print('Valid run', ret_, valid_ret)\n",
    "\n",
    "    # collect some episodes using the current policy\n",
    "    # and push (obs,a,r,p(a)) tuples to the replay buffer.\n",
    "    nc = n_collect\n",
    "    if ni == 0:\n",
    "        nc = init_collect\n",
    "    for ci in range(nc):\n",
    "        o_, r_, c_, a_, ap_, ret_ = collect_one_episode(env, player, max_len=max_len, discount_factor=discount_factor, n_frames=n_frames)\n",
    "        replay_buffer.add(o_, r_, c_, a_, ap_)\n",
    "        if ret == -numpy.Inf:\n",
    "            ret = ret_\n",
    "        else:\n",
    "            ret = 0.9 * ret + 0.1 * ret_\n",
    "    \n",
    "    player.train()\n",
    "    \n",
    "    # fit a q function\n",
    "    # TD learning: min_Q (Q(s,a) - (r + \\gamma \\max_a' Q(s',a')))^2\n",
    "    for qi in range(n_q):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        opt_q.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        batch_x = torch.from_numpy(numpy.stack([ex['current']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_xn = torch.from_numpy(numpy.stack([ex['next']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        \n",
    "        batch_y = torch.from_numpy(numpy.stack([ex['current']['rew'] for ex in batch]).astype('float32')).to(device)\n",
    "        \n",
    "        batch_a = torch.from_numpy(numpy.stack([ex['current']['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        \n",
    "        q_pred = qnet(batch_x, batch_a).squeeze()\n",
    "        q_next = qold.value(batch_xn)[0].squeeze()\n",
    "        \n",
    "        loss_ = ((q_pred - (batch_y + discount_factor * q_next)) ** 2)\n",
    "\n",
    "        # since Q learning is off-policy, no need for importance weighting.\n",
    "        iw = 1.\n",
    "        loss = iw * loss_\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_q.step()\n",
    "        \n",
    "    if q_loss == -numpy.Inf:\n",
    "        q_loss = loss_.mean().item()\n",
    "    else:\n",
    "        q_loss = 0.9 * q_loss + 0.1 * loss_.mean().item()\n",
    "        \n",
    "    copy_params(qnet, qold)\n",
    "        \n",
    "    # fit a value function\n",
    "    # a usual value estimator: min_V (V(s) - Q(s,a))^2\n",
    "    for vi in range(n_value):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        opt_q.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        batch_x = torch.from_numpy(numpy.stack([ex['current']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_a = torch.from_numpy(numpy.stack([ex['current']['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        \n",
    "        pred_y = value(batch_x).squeeze()\n",
    "        pred_q = qnet(batch_x, batch_a).squeeze()\n",
    "        \n",
    "        loss_ = ((pred_y - pred_q) ** 2)\n",
    "        \n",
    "        batch_pi = player(batch_x, normalized=True)\n",
    "        batch_q = torch.from_numpy(numpy.stack([ex['current']['prob'] for ex in batch]).astype('float32')).to(device)\n",
    "        logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "\n",
    "        # (clipped) importance weight: \n",
    "        # because the policy may have changed since the tuple was collected.\n",
    "        iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "    \n",
    "        loss = iw * loss_\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_value.step()\n",
    "        \n",
    "    if value_loss < 0.:\n",
    "        value_loss = loss_.mean().item()\n",
    "    else:\n",
    "        value_loss = 0.9 * value_loss + 0.1 * loss_.mean().item()\n",
    "    \n",
    "    if numpy.mod(ni, disp_iter) == 0:\n",
    "        print('# plays', (ni+1) * n_collect, 'return', ret, 'value_loss', value_loss, 'q_loss', q_loss, 'entropy', -entropy)\n",
    "    \n",
    "    # fit a policy\n",
    "    # advantage: (Q(a,s) - V(s))\n",
    "    for pi in range(n_policy):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        opt_q.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        batch_x = torch.from_numpy(numpy.stack([ex['current']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_a = torch.from_numpy(numpy.stack([ex['current']['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "\n",
    "        batch_pi = player(batch_x, normalized=True)        \n",
    "        logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "\n",
    "        batch_q = qnet(batch_x, batch_a)\n",
    "        batch_v = value(batch_x)\n",
    "        \n",
    "        # advantage\n",
    "        adv = batch_q.clone().detach() - batch_v\n",
    "        adv = adv / adv.abs().max()\n",
    "        \n",
    "        loss = -(adv * logp)\n",
    "        \n",
    "        batch_q = torch.from_numpy(numpy.stack([ex['current']['prob'] for ex in batch]).astype('float32')).to(device)\n",
    "\n",
    "        # (clipped) importance weight: \n",
    "        # because the policy may have changed since the tuple was collected.\n",
    "        iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "    \n",
    "        loss = iw * loss\n",
    "        \n",
    "        # entropy regularization: though, it doesn't look necessary in this specific case.\n",
    "        ent = (batch_pi * torch.log(batch_pi)).sum(1)\n",
    "        \n",
    "        if entropy == -numpy.Inf:\n",
    "            entropy = ent.mean().item()\n",
    "        else:\n",
    "            entropy = 0.9 * entropy + 0.1 * ent.mean().item()\n",
    "        \n",
    "        loss = (loss + ent_coeff * ent).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_player.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.figure()\n",
    "\n",
    "plot.plot(return_history)\n",
    "plot.grid(True)\n",
    "plot.xlabel('# of plays x {}'.format(n_collect))\n",
    "plot.ylabel('Return over the episode of length {}'.format(max_len))\n",
    "\n",
    "plot.show()\n",
    "plot.savefig('return_log.pdf', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8b543fa33c12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# let the final policy play the pong longer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_one_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-654a7fed751b>\u001b[0m in \u001b[0;36mcollect_one_episode\u001b[0;34m(env, player, max_len, discount_factor, deterministic, rendering, verbose)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let the final policy play the pong longer\n",
    "player.eval()\n",
    "_, _, _, _, ret_ = collect_one_episode(env, player, max_len=1000000, deterministic=True, rendering=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
