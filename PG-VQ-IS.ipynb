{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResLinear(nn.Module):\n",
    "    def __init__(self, n_in, n_out, act=nn.ReLU()):\n",
    "        super(ResLinear, self).__init__()\n",
    "        self.act = act\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "        self.bn = nn.BatchNorm1d(n_out)\n",
    "        \n",
    "        assert(n_in == n_out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.act(self.bn(self.linear(x)))\n",
    "        return h + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(nn.Module):\n",
    "    def __init__(self, n_in=128, n_hid=100, n_out=6):\n",
    "        super(Player, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid),\n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, n_out))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, obs, normalized=False):\n",
    "        if normalized:\n",
    "            return self.softmax(self.layers(obs))\n",
    "        else:\n",
    "            return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(nn.Module):\n",
    "    def __init__(self, n_in=128, n_act=6, n_hid=100):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid), \n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, n_act))\n",
    "    \n",
    "    def forward(self, obs, act):\n",
    "#         set_trace()\n",
    "        return self.layers(obs).gather(1, act.long())\n",
    "    \n",
    "    def value(self, obs):\n",
    "        return self.layers(obs).max(dim=1)\n",
    "    \n",
    "    def q(self, obs):\n",
    "        return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(nn.Module):\n",
    "    def __init__(self, n_in=128, n_hid=100):\n",
    "        super(Value, self).__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_in, n_hid),\n",
    "                                    nn.BatchNorm1d(n_hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    ResLinear(n_hid, n_hid, nn.ReLU()),\n",
    "                                    nn.Linear(n_hid, 1))\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.layers(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_params(from_, to_):\n",
    "    for f_, t_ in zip(from_.parameters(), to_.parameters()):\n",
    "        t_.data.copy_(f_.data)\n",
    "        \n",
    "def avg_params(from_, to_, coeff=0.95):\n",
    "    for f_, t_ in zip(from_.parameters(), to_.parameters()):\n",
    "        t_.data.copy_(coeff * t_.data + (1.-coeff) * f_.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_obs(obs):\n",
    "    return obs.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data\n",
    "def collect_one_episode(env, player, max_len=50, discount_factor=0.9, deterministic=False, rendering=False, verbose=False):\n",
    "    episode = []\n",
    "\n",
    "    observations = []\n",
    "\n",
    "    rewards = []\n",
    "    crewards = []\n",
    "\n",
    "    actions = []\n",
    "    action_probs = []\n",
    "\n",
    "    obs = env.reset()\n",
    "    \n",
    "    for ml in range(max_len):\n",
    "        if rendering:\n",
    "            env.render()\n",
    "            sleep(0.05)\n",
    "            \n",
    "        obs = normalize_obs(obs)\n",
    "\n",
    "        out_probs = player(torch.from_numpy(obs[None,:]).to(device), normalized=True).squeeze()\n",
    "        \n",
    "        if deterministic:\n",
    "            action = numpy.argmax(out_probs.to('cpu').data.numpy())\n",
    "            if verbose:\n",
    "                print(out_probs, action)\n",
    "        else:\n",
    "            act_dist = Categorical(out_probs)\n",
    "            action = act_dist.sample().item()\n",
    "        action_prob = out_probs[action].item()\n",
    "\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        action_probs.append(action_prob)\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if deterministic and verbose:\n",
    "            print(reward, done)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "\n",
    "    rewards = numpy.array(rewards)\n",
    "\n",
    "    # it's probably not the best idea to compute the discounted cumulative returns here, but well..\n",
    "    for ri in range(len(rewards)):\n",
    "        factors = (discount_factor ** numpy.arange(len(rewards)-ri))\n",
    "        crewards.append(numpy.sum(rewards[ri:] * factors))\n",
    "        \n",
    "    # discard the final 10%, because it really doesn't give me a good signal due to the unbounded horizon\n",
    "    # this is only for training, not for computing the total return of the episode of the given length\n",
    "    discard = max_len // 10\n",
    "        \n",
    "    return observations[:-discard], rewards[:-discard], crewards[:-discard], actions[:-discard], action_probs[:-discard], rewards.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple implementation of FIFO-based replay buffer\n",
    "class Buffer:\n",
    "    def __init__(self, max_items=10000):\n",
    "        self.max_items = max_items\n",
    "        self.buffer = []\n",
    "        \n",
    "    def add(self, observations, rewards, crewards, actions, action_probs):\n",
    "        new_n = len(observations)\n",
    "        old_n = len(self.buffer)\n",
    "        if new_n + old_n > self.max_items:\n",
    "            del self.buffer[:new_n]\n",
    "        for o, r, c, a, p, on, rn, cn, an, pn in zip(observations[:-1], rewards[:-1], crewards[:-1], actions[:-1], action_probs[:-1],\n",
    "                                             observations[1:], rewards[1:], crewards[1:], actions[1:], action_probs[1:]):\n",
    "            self.buffer.append({'current': {'obs': o, \n",
    "                                            'rew': r, \n",
    "                                            'crew': c, \n",
    "                                            'act': a, \n",
    "                                            'prob': p},\n",
    "                                'next': {'obs': on, \n",
    "                                         'rew': rn, \n",
    "                                         'crew': cn, \n",
    "                                         'act': an, \n",
    "                                         'prob': pn}})\n",
    "            \n",
    "    def sample(self, n=100):\n",
    "        idxs = numpy.random.choice(len(self.buffer),n)\n",
    "        return [self.buffer[ii] for ii in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a policy\n",
    "player = Player(n_in=128, n_hid=128, n_out=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a q estimator\n",
    "qnet = Qnet(n_in=128, n_hid=128, n_act=6).to(device)\n",
    "qold = Qnet(n_in=128, n_hid=128, n_act=6).to(device)\n",
    "copy_params(qnet, qold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a value estimator\n",
    "value = Value(n_in=128, n_hid=128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimizers\n",
    "opt_player = Adam(player.parameters(), lr=0.0001)\n",
    "opt_q = Adam(qnet.parameters(), lr=0.0001)\n",
    "opt_value = Adam(value.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize replay buffer\n",
    "replay_buffer = Buffer(max_items=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid run -20.0 -20.0\n",
      "# plays 1 return -16.0 value_loss 0.02605981007218361 q_loss 0.013791528530418873 entropy inf\n",
      "Valid run -15.0 -19.5\n",
      "# plays 2 return -16.3 value_loss 0.024653300270438192 q_loss 0.013632574398070575 entropy 1.7467347790864332\n",
      "Valid run -9.0 -18.45\n",
      "# plays 3 return -16.270000000000003 value_loss 0.033247423581779 q_loss 0.013662318764254453 entropy 1.7430345535599252\n",
      "Valid run -20.0 -18.605\n",
      "# plays 4 return -16.243000000000002 value_loss 0.03648584626242518 q_loss 0.013870261016301812 entropy 1.7341013453573104\n",
      "Valid run -20.0 -18.744500000000002\n",
      "# plays 5 return -16.618700000000004 value_loss 0.03465242442153394 q_loss 0.014718127461802216 entropy 1.7230722022331884\n",
      "Valid run -20.0 -18.870050000000003\n",
      "# plays 6 return -16.956830000000004 value_loss 0.03329117602106184 q_loss 0.015449385460494087 entropy 1.7092774051620379\n",
      "Valid run -20.0 -18.983045000000004\n",
      "# plays 7 return -16.861147000000003 value_loss 0.03009503423940763 q_loss 0.015261248025695794 entropy 1.6953020655102162\n",
      "Valid run -20.0 -19.084740500000006\n",
      "# plays 8 return -16.975032300000002 value_loss 0.02733372800220512 q_loss 0.015547546057462974 entropy 1.6826102615455387\n",
      "Valid run -21.0 -19.27626645000001\n",
      "# plays 9 return -17.27752907 value_loss 0.038846498602653705 q_loss 0.015332821864837663 entropy 1.6690208416450267\n",
      "Valid run -20.0 -19.34863980500001\n",
      "# plays 10 return -16.849776163 value_loss 0.03821278106334075 q_loss 0.01586593250240766 entropy 1.6536566616776798\n",
      "Valid run -20.0 -19.413775824500007\n",
      "# plays 11 return -17.164798546700002 value_loss 0.03490920287105976 q_loss 0.01629915756558671 entropy 1.6378917284835208\n",
      "Valid run -20.0 -19.47239824205001\n",
      "# plays 12 return -17.44831869203 value_loss 0.03298449781596844 q_loss 0.016193896305489192 entropy 1.6192157393700215\n",
      "Valid run -20.0 -19.52515841784501\n",
      "# plays 13 return -17.703486822827003 value_loss 0.044396069053353526 q_loss 0.016255218331749127 entropy 1.5954966964318071\n",
      "Valid run -20.0 -19.57264257606051\n",
      "# plays 14 return -17.9331381405443 value_loss 0.061486208203621204 q_loss 0.016296803022684055 entropy 1.5704512627313032\n",
      "Valid run -20.0 -19.61537831845446\n",
      "# plays 15 return -18.139824326489872 value_loss 0.05749071521000936 q_loss 0.01616058023893362 entropy 1.5426270483354676\n",
      "Valid run -20.0 -19.653840486609017\n",
      "# plays 16 return -18.325841893840884 value_loss 0.06944777264389582 q_loss 0.01683942397347265 entropy 1.517361159562277\n",
      "Valid run -20.0 -19.688456437948116\n",
      "# plays 17 return -18.493257704456795 value_loss 0.0626497600826012 q_loss 0.017251178464989424 entropy 1.493909271189933\n",
      "Valid run -20.0 -19.719610794153304\n",
      "# plays 18 return -18.643931934011118 value_loss 0.06213771411946895 q_loss 0.017423293784291055 entropy 1.4729954320292815\n",
      "Valid run -21.0 -19.847649714737976\n",
      "# plays 19 return -18.779538740610008 value_loss 0.056715627618177084 q_loss 0.016857526100454188 entropy 1.4547787877079064\n",
      "Valid run -20.0 -19.86288474326418\n",
      "# plays 20 return -18.90158486654901 value_loss 0.06204514364082517 q_loss 0.016441216916689982 entropy 1.4397324431336367\n"
     ]
    }
   ],
   "source": [
    "n_iter = 1000\n",
    "init_collect = 1\n",
    "n_collect = 1\n",
    "n_q = 50\n",
    "n_value = 50\n",
    "n_policy = 50\n",
    "disp_iter = 1\n",
    "val_iter = 1\n",
    "\n",
    "max_len = 1000\n",
    "batch_size = 1000\n",
    "\n",
    "ent_coeff = 0. #0.001\n",
    "discount_factor = .95\n",
    "\n",
    "q_loss = -numpy.Inf\n",
    "value_loss = -numpy.Inf\n",
    "ret = -numpy.Inf\n",
    "entropy = -numpy.Inf\n",
    "valid_ret = -numpy.Inf\n",
    "\n",
    "return_history = []\n",
    "\n",
    "for ni in range(n_iter):\n",
    "    player.eval()\n",
    "\n",
    "    if numpy.mod(ni, val_iter) == 0:\n",
    "        _, _, _, _, _, ret_ = collect_one_episode(env, player, max_len=max_len, deterministic=True)\n",
    "        return_history.append(ret_)\n",
    "        if valid_ret == -numpy.Inf:\n",
    "            valid_ret = ret_\n",
    "        else:\n",
    "            valid_ret = 0.9 * valid_ret + 0.1 * ret_\n",
    "        print('Valid run', ret_, valid_ret)\n",
    "\n",
    "    # collect some episodes using the current policy\n",
    "    # and push (obs,a,r,p(a)) tuples to the replay buffer.\n",
    "    nc = n_collect\n",
    "    if ni == 0:\n",
    "        nc = init_collect\n",
    "    for ci in range(nc):\n",
    "        o_, r_, c_, a_, ap_, ret_ = collect_one_episode(env, player, max_len=max_len, discount_factor=discount_factor)\n",
    "        replay_buffer.add(o_, r_, c_, a_, ap_)\n",
    "        if ret == -numpy.Inf:\n",
    "            ret = ret_\n",
    "        else:\n",
    "            ret = 0.9 * ret + 0.1 * ret_\n",
    "    \n",
    "    player.train()\n",
    "    \n",
    "    # fit a q function\n",
    "    for qi in range(n_q):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        opt_q.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        batch_x = torch.from_numpy(numpy.stack([ex['current']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_xn = torch.from_numpy(numpy.stack([ex['next']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        \n",
    "        batch_y = torch.from_numpy(numpy.stack([ex['current']['rew'] for ex in batch]).astype('float32')).to(device)\n",
    "        \n",
    "        batch_a = torch.from_numpy(numpy.stack([ex['current']['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        \n",
    "        q_pred = qnet(batch_x, batch_a).squeeze()\n",
    "        q_next = qold.value(batch_xn)[0].squeeze()\n",
    "        \n",
    "        loss_ = ((q_pred - (batch_y + discount_factor * q_pred)) ** 2)\n",
    "        \n",
    "        batch_a = torch.from_numpy(numpy.stack([ex['current']['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        batch_pi = player(batch_x, normalized=True)\n",
    "        batch_q = torch.from_numpy(numpy.stack([ex['current']['prob'] for ex in batch]).astype('float32')).to(device)\n",
    "        logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "\n",
    "        # (clipped) importance weight: \n",
    "        # because the policy may have changed since the tuple was collected.\n",
    "        iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "    \n",
    "        loss = iw * loss_\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_q.step()\n",
    "        \n",
    "    if q_loss == -numpy.Inf:\n",
    "        q_loss = loss_.mean().item()\n",
    "    else:\n",
    "        q_loss = 0.9 * q_loss + 0.1 * loss_.mean().item()\n",
    "        \n",
    "    copy_params(qnet, qold)\n",
    "        \n",
    "    # fit a value function\n",
    "    for vi in range(n_value):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        opt_q.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        batch_x = torch.from_numpy(numpy.stack([ex['current']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_a = torch.from_numpy(numpy.stack([ex['current']['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "        \n",
    "        pred_y = value(batch_x).squeeze()\n",
    "        pred_q = qnet(batch_x, batch_a)[0].squeeze()\n",
    "        \n",
    "        loss_ = ((pred_y - pred_q) ** 2)\n",
    "        \n",
    "        batch_pi = player(batch_x, normalized=True)\n",
    "        batch_q = torch.from_numpy(numpy.stack([ex['current']['prob'] for ex in batch]).astype('float32')).to(device)\n",
    "        logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "\n",
    "        # (clipped) importance weight: \n",
    "        # because the policy may have changed since the tuple was collected.\n",
    "        iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "    \n",
    "        loss = iw * loss_\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_value.step()\n",
    "        \n",
    "    if value_loss < 0.:\n",
    "        value_loss = loss_.mean().item()\n",
    "    else:\n",
    "        value_loss = 0.9 * value_loss + 0.1 * loss_.mean().item()\n",
    "    \n",
    "    if numpy.mod(ni, disp_iter) == 0:\n",
    "        print('# plays', (ni+1) * n_collect, 'return', ret, 'value_loss', value_loss, 'q_loss', q_loss, 'entropy', -entropy)\n",
    "    \n",
    "    # fit a policy\n",
    "    for pi in range(n_policy):\n",
    "        opt_player.zero_grad()\n",
    "        opt_value.zero_grad()\n",
    "        opt_q.zero_grad()\n",
    "        \n",
    "        batch = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        batch_x = torch.from_numpy(numpy.stack([ex['current']['obs'] for ex in batch]).astype('float32')).to(device)\n",
    "        batch_a = torch.from_numpy(numpy.stack([ex['current']['act'] for ex in batch]).astype('float32')[:,None]).to(device)\n",
    "\n",
    "#         batch_v = value(batch_x)\n",
    "        batch_v = qnet.value(batch_x)[0]\n",
    "        batch_q = qnet(batch_x, batch_a)\n",
    "        \n",
    "        batch_pi = player(batch_x, normalized=True)\n",
    "        \n",
    "        logp = torch.log(batch_pi.gather(1, batch_a.long()))\n",
    "        \n",
    "        # advantage\n",
    "        adv = batch_q.clone().detach() - batch_v.clone().detach()\n",
    "        \n",
    "        loss = -(adv * logp)\n",
    "        \n",
    "        batch_q = torch.from_numpy(numpy.stack([ex['current']['prob'] for ex in batch]).astype('float32')).to(device)\n",
    "\n",
    "        # (clipped) importance weight: \n",
    "        # because the policy may have changed since the tuple was collected.\n",
    "        iw = torch.exp((logp.clone().detach() - torch.log(batch_q)).clamp(max=0.))\n",
    "    \n",
    "        loss = iw * loss\n",
    "        \n",
    "        # entropy regularization: though, it doesn't look necessary in this specific case.\n",
    "        ent = (batch_pi * torch.log(batch_pi)).sum(1)\n",
    "        \n",
    "        if entropy == -numpy.Inf:\n",
    "            entropy = ent.mean().item()\n",
    "        else:\n",
    "            entropy = 0.9 * entropy + 0.1 * ent.mean().item()\n",
    "        \n",
    "        loss = (loss + ent_coeff * ent).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        opt_player.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d591b02996e6461b9c4b94ed7359ea99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureCanvasNbAgg()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot.figure()\n",
    "\n",
    "plot.plot(return_history)\n",
    "plot.grid(True)\n",
    "plot.xlabel('# of plays x {}'.format(n_collect))\n",
    "plot.ylabel('Return over the episode of length {}'.format(max_len))\n",
    "\n",
    "plot.show()\n",
    "plot.savefig('return_log.pdf', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8b543fa33c12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# let the final policy play the pong longer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_one_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-654a7fed751b>\u001b[0m in \u001b[0;36mcollect_one_episode\u001b[0;34m(env, player, max_len, discount_factor, deterministic, rendering, verbose)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let the final policy play the pong longer\n",
    "player.eval()\n",
    "_, _, _, _, ret_ = collect_one_episode(env, player, max_len=1000000, deterministic=True, rendering=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
